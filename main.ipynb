{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-02-26 11:42:12.233066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740570132.255266   72624 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740570132.262072   72624 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \n",
    "    \"cuda\" if torch.cuda.is_available() else \n",
    "    \"cpu\"\n",
    ")\n",
    "print('Got device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook to work, please add \n",
    "\n",
    "```python\n",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "```\n",
    "\n",
    "to the `OFFICIAL_MODEL_NAMES` list in the `loading_from_pretrained.py` file under the `TransformerLens` library after you've downloaded it locally.\n",
    "\n",
    "I also increased the `n_ctx` for architectures `\"QWenLMHeadModel\"` and `\"QWen2ForCausalLM\"` from 2048 to 4096 because the documentation mentions that they are capped due to memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Model context length: 2048\n",
      "ðŸ§  Model layers: 28\n",
      "ðŸ”¤ Vocabulary size: 151936\n",
      "ðŸ“Š Hidden dimension: 1536\n",
      "ðŸ§© Attention heads: 12\n",
      "ðŸ·ï¸ Model name: DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ“ Model context length: {model.cfg.n_ctx}\")\n",
    "print(f\"ðŸ§  Model layers: {model.cfg.n_layers}\")\n",
    "print(f\"ðŸ”¤ Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"ðŸ“Š Hidden dimension: {model.cfg.d_model}\")\n",
    "print(f\"ðŸ§© Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"ðŸ·ï¸ Model name: {model.cfg.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_prompt = lambda problem: f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem} Solution: \\n<think>\\n\"\"\"\n",
    "result = model.generate([lambda_prompt(x) for x in [\"2x+3=11\", \"What is the meaning of life?\", \"4x^2 + 3y^2 + 2x + 3y = 11\", \"Riemann Hypothesis\"]], \n",
    "                        temperature=0,\n",
    "                        max_new_tokens=1400,\n",
    "                        top_p=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, activations = model.run_with_cache(\"Hello World\")\n",
    "\n",
    "print(logits)\n",
    "print(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(prompt, return_type=\"loss\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_str_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(prompt, return_type=\"logits\")\n",
    "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tokens = model.to_tokens(prompt).squeeze()[1:]\n",
    "is_correct = prediction == true_tokens\n",
    "\n",
    "print(f\"Model accuracy: {is_correct.sum()}/{len(true_tokens)}\")\n",
    "print(f\"Correct tokens: {model.to_str_tokens(prediction[is_correct])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[0].attn.W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "\n",
    "print(type(logits), type(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_patterns_from_shorthand = cache[\"pattern\", 0]\n",
    "attn_patterns_from_full_name = cache[\"blocks.0.attn.hook_pattern\"]\n",
    "\n",
    "torch.testing.assert_close(attn_patterns_from_shorthand, attn_patterns_from_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This part fails, is it because the attention mechanisms for Qwen (1.5B) and GPT-2 (small) are different?\n",
    "layer0_pattern_from_cache = cache[\"pattern\", 0]\n",
    "\n",
    "q, k = cache[\"q\", 0], cache[\"k\", 0]\n",
    "seq, nhead, headsize = q.shape\n",
    "layer0_attn_scores = einops.einsum(q, k, \"seqQ n h, seqK n h -> n seqQ seqK\")\n",
    "mask = torch.triu(torch.ones((seq, seq), dtype=torch.bool), diagonal=1).to(device)\n",
    "layer0_attn_scores.masked_fill_(mask, -1e9)\n",
    "layer0_pattern_from_q_and_k = (layer0_attn_scores / headsize**0.5).softmax(-1)\n",
    "\n",
    "torch.testing.assert_close(layer0_pattern_from_cache, layer0_pattern_from_q_and_k)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cache))\n",
    "attention_pattern = cache[\"pattern\", 0]\n",
    "print(attention_pattern.shape)\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(\n",
    "    cv.attention.attention_patterns(\n",
    "        tokens=str_tokens,\n",
    "        attention=attention_pattern,\n",
    "        #attention_head_names=[f\"L0H{i}\" for i in range(12)],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_activations_for_all_layers = torch.stack([cache[\"post\", layer] for layer in range(model.cfg.n_layers)], dim=1)\n",
    "# shape = (seq_pos, layers, neurons)\n",
    "\n",
    "cv.activations.text_neuron_activations(\n",
    "    tokens=str_tokens,\n",
    "    activations=neuron_activations_for_all_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_activations_for_all_layers_rearranged = to_numpy(einops.rearrange(neuron_activations_for_all_layers, \"seq layers neurons -> 1 layers seq neurons\"))\n",
    "\n",
    "cv.topk_tokens.topk_tokens(\n",
    "    # Some weird indexing required here Â¯\\_(ãƒ„)_/Â¯\n",
    "    tokens=[str_tokens],\n",
    "    activations=neuron_activations_for_all_layers_rearranged,\n",
    "    max_k=7,\n",
    "    first_dimension_name=\"Layer\",\n",
    "    third_dimension_name=\"Neuron\",\n",
    "    first_dimension_labels=list(range(12))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\"What is 5+5?\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(\"What is 5+5 divided 16?\", remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Let's solve this step by step:\n",
    "\n",
    "Question: What is Einstein's law of general relativity?\n",
    "\n",
    "Let's break this down:\n",
    "\"\"\"\n",
    "\n",
    "result = model.generate(prompt, \n",
    "                        temperature=0.6, \n",
    "                        max_new_tokens=500,\n",
    "                        top_p=0.95)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['problem', 'level', 'type', 'solution'],\n",
      "        num_rows: 7500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['problem', 'level', 'type', 'solution'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "Available splits: dict_keys(['train', 'test'])\n",
      "Number of examples in train: 7500\n",
      "Number of examples in test: 5000\n",
      "\n",
      "Example from the dataset:\n",
      "Problem: Let \\[f(x) = \\left\\{\n",
      "\\begin{array}{cl} ax+3, &\\text{ if }x>2, \\\\\n",
      "x-5 &\\text{ if } -2 \\le x \\le 2, \\\\\n",
      "2x-b &\\text{ if } x <-2.\n",
      "\\end{array}\n",
      "\\right.\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).\n",
      "Level: Level 5\n",
      "Type: Algebra\n",
      "Solution: For the piecewise function to be continuous, the cases must \"meet\" at $2$ and $-2$. For example, $ax+3$ and $x-5$ must be equal when $x=2$. This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\Rightarrow a=-3$. Similarly, $x-5$ and $2x-b$ must be equal when $x=-2$. Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$. So $a+b=-3+3=\\boxed{0}$.\n"
     ]
    }
   ],
   "source": [
    "# Load the MATH dataset\n",
    "math_dataset = load_dataset(\"fdyrd/math\")\n",
    "print(f\"Dataset structure: {math_dataset}\")\n",
    "\n",
    "# Examine the dataset structure\n",
    "print(f\"Available splits: {math_dataset.keys()}\")\n",
    "print(f\"Number of examples in train: {len(math_dataset['train'])}\")\n",
    "print(f\"Number of examples in test: {len(math_dataset['test'])}\")\n",
    "\n",
    "# Look at the first example to understand the format\n",
    "print(\"\\nExample from the dataset:\")\n",
    "example = math_dataset['train'][0]\n",
    "print(f\"Problem: {example['problem']}\")\n",
    "print(f\"Level: {example['level']}\")\n",
    "print(f\"Type: {example['type']}\")\n",
    "print(f\"Solution: {example['solution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample problems from the dataset\n",
    "def sample_math_problems(dataset, n=5, level=None, problem_type=None):\n",
    "    \"\"\"\n",
    "    Sample n problems from the dataset, optionally filtering by level or type.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The MATH dataset\n",
    "        n: Number of problems to sample\n",
    "        level: Optional filter for problem difficulty (e.g., \"Level 1\")\n",
    "        problem_type: Optional filter for problem type (e.g., \"Algebra\")\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled problems\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset['train']\n",
    "    \n",
    "    if level:\n",
    "        filtered_dataset = [ex for ex in filtered_dataset if ex['level'] == level]\n",
    "    \n",
    "    if problem_type:\n",
    "        filtered_dataset = [ex for ex in filtered_dataset if ex['type'] == problem_type]\n",
    "    \n",
    "    filtered_dataset = list(filtered_dataset)  # Convert to list to ensure it's a sequence\n",
    "    return random.sample(filtered_dataset, min(n, len(filtered_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled problems for testing:\n",
      "\n",
      "Problem 1:\n",
      "Type: Geometry, Level: Level 3\n",
      "Problem statement: Two triangles are similar. The ratio of their areas is 1:4. If the height of the smaller triangle is 3 cm, how long is the corresponding height of the larger triangle, in centimeters?\n",
      "\n",
      "Problem 2:\n",
      "Type: Counting & Probability, Level: Level 3\n",
      "Problem statement: I have 6 shirts, 4 pairs of pants, and 6 hats. The pants come in tan, black, blue, and gray. The shirts and hats come in those colors, and also white and yellow. I refuse to wear an outfit in which all 3 items are the same color. How many choices for outfits, consisting of one shirt, one hat, and one pair of pants, do I have?\n",
      "\n",
      "Problem 3:\n",
      "Type: Algebra, Level: Level 3\n",
      "Problem statement: Lulu has a quadratic of the form $x^2+bx+44$, where $b$ is a specific positive number. Using her knowledge of how to complete the square, Lulu is able to rewrite this quadratic in the form $(x+m)^2+8$. What is $b$?\n"
     ]
    }
   ],
   "source": [
    "sampled_problems = sample_math_problems(math_dataset, n=3, level=\"Level 3\")\n",
    "print(\"\\nSampled problems for testing:\")\n",
    "for i, problem in enumerate(sampled_problems):\n",
    "    print(f\"\\nProblem {i+1}:\")\n",
    "    print(f\"Type: {problem['type']}, Level: {problem['level']}\")\n",
    "    print(f\"Problem statement: {problem['problem']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate CoT using the model\n",
    "def generate_cot_for_problem(\n",
    "    model: HookedTransformer, \n",
    "    problem: str, \n",
    "    temperature: float = 0.4, \n",
    "    max_new_tokens: int = 1500, \n",
    "    top_p: float = 0.92\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a chain-of-thought solution for a given math problem.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        problem: The math problem text\n",
    "        temperature: The temperature for the model\n",
    "        max_new_tokens: The maximum number of tokens to generate\n",
    "        top_p: The top-p value for the model\n",
    "    Returns:\n",
    "        The generated chain-of-thought solution\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem} Solution: \\n<think>\\n\"\"\"\n",
    "    result = model.generate(prompt, \n",
    "                            temperature=temperature,\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            top_p=top_p)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4491fd4f5e19435a97cd79a9b32db061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Chain-of-Thought solution:\n",
      "Solve this math problem step by step. Put your final answer in \\boxed{}. Problem: Two triangles are similar. The ratio of their areas is 1:4. If the height of the smaller triangle is 3 cm, how long is the corresponding height of the larger triangle, in centimeters? Solution: \n",
      "<think>\n",
      "First, I recognize that the ratio of the areas of similar triangles is 1:4. This means that the square of the ratio of their corresponding heights is also 1:4.\n",
      "\n",
      "Let h be the height of the larger triangle. According to the ratio, (h / 3)^2 = 1/4.\n",
      "\n",
      "Taking the square root of both sides gives h / 3 = 1/2.\n",
      "\n",
      "Finally, solving for h gives h = 3 * (1/2) = 1.5 cm.\n",
      "</think>\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "We are given two similar triangles with the following information:\n",
      "- The ratio of their areas is 1:4.\n",
      "- The height of the smaller triangle is 3 cm.\n",
      "\n",
      "We need to find the corresponding height of the larger triangle.\n",
      "\n",
      "**Step 1: Understand the relationship between the areas and heights of similar triangles.**\n",
      "\n",
      "For similar triangles, the ratio of their areas is equal to the square of the ratio of their corresponding linear measurements (e.g., heights, bases, etc.).\n",
      "\n",
      "Mathematically, this can be expressed as:\n",
      "\\[\n",
      "\\left( \\frac{\\text{Height of larger triangle}}{\\text{Height of smaller triangle}} \\right)^2 = \\frac{\\text{Area of larger triangle}}{\\text{Area of smaller triangle}}\n",
      "\\]\n",
      "\n",
      "**Step 2: Plug in the known values.**\n",
      "\n",
      "Given:\n",
      "\\[\n",
      "\\frac{\\text{Area of larger triangle}}{\\text{Area of smaller triangle}} = \\frac{1}{4}\n",
      "\\]\n",
      "\\[\n",
      "\\text{Height of smaller triangle} = 3 \\, \\text{cm}\n",
      "\\]\n",
      "\n",
      "Let \\( h \\) be the height of the larger triangle. Substituting into the equation:\n",
      "\\[\n",
      "\\left( \\frac{h}{3} \\right)^2 = \\frac{1}{4}\n",
      "\\]\n",
      "\n",
      "**Step 3: Solve for \\( h \\).**\n",
      "\n",
      "Take the square root of both sides:\n",
      "\\[\n",
      "\\frac{h}{3} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}\n",
      "\\]\n",
      "\n",
      "Multiply both sides by 3:\n",
      "\\[\n",
      "h = 3 \\times \\frac{1}{2} = 1.5 \\, \\text{cm}\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{1.5}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "# Select a problem\n",
    "problem_text = sampled_problems[0]['problem']\n",
    "\n",
    "# Generate CoT\n",
    "cot_solution = generate_cot_for_problem(\n",
    "    model, \n",
    "    problem_text, \n",
    "    temperature=0.6, \n",
    "    max_new_tokens=1500, \n",
    "    top_p=0.92\n",
    ")\n",
    "print(\"\\nGenerated Chain-of-Thought solution:\")\n",
    "print(cot_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to batch process multiple problems\n",
    "def batch_generate_cot(\n",
    "    model, \n",
    "    problems, \n",
    "    batch_size=4,  # Process this many problems in parallel\n",
    "    temperature=0.6, \n",
    "    max_new_tokens=1500, \n",
    "    top_p=0.92, \n",
    "    save_every=5,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate CoT solutions for multiple problems in parallel batches.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        problems: List of problem dictionaries\n",
    "        batch_size: Number of problems to process in parallel\n",
    "        temperature: The temperature for the model\n",
    "        max_new_tokens: The maximum number of tokens to generate\n",
    "        top_p: The top-p value for the model\n",
    "        save_every: How often to save intermediate results\n",
    "        save_path: Optional path to save results\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing problems and their CoT solutions\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process problems in batches\n",
    "    for i in tqdm(range(0, len(problems), batch_size), desc=\"Processing batches\"):\n",
    "        batch_problems = problems[i:i+batch_size]\n",
    "\n",
    "        # Prepare prompts for the batch\n",
    "        prompts = [\n",
    "            f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem['problem']} Solution: \\n<think>\\n\"\"\"\n",
    "            for problem in batch_problems\n",
    "        ]\n",
    "        \n",
    "        # Generate solutions for the batch in parallel\n",
    "        try:\n",
    "            batch_solutions = model.generate(\n",
    "                prompts,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating solutions for batch {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Process and store results\n",
    "        for j, (problem, solution) in enumerate(zip(batch_problems, batch_solutions if batch_size > 1 else [batch_solutions])):\n",
    "            results.append({\n",
    "                \"problem_id\": i + j,\n",
    "                \"problem_text\": problem['problem'],\n",
    "                \"problem_type\": problem['type'],\n",
    "                \"problem_level\": problem['level'],\n",
    "                \"ground_truth_solution\": problem['solution'],\n",
    "                \"generated_cot\": solution\n",
    "            })\n",
    "        \n",
    "        # Save intermediate results\n",
    "        if (i // batch_size) % save_every == 0 and save_path:\n",
    "            print(f\"Saving results to {save_path}...\")\n",
    "            import json\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save final results\n",
    "    if save_path:\n",
    "        import json\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a small set of problems for testing\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "test_problems = sample_math_problems(math_dataset, n=1000)\n",
    "\n",
    "temperature = 0.6\n",
    "max_new_tokens = 1500\n",
    "top_p = 0.92\n",
    "batch_size = 1\n",
    "\n",
    "# Generate CoT solutions for the test problems\n",
    "cot_results = batch_generate_cot(\n",
    "    model, \n",
    "    test_problems, \n",
    "    batch_size=batch_size,\n",
    "    temperature=temperature, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    top_p=top_p, \n",
    "    save_path=f\"math_cot_results_t={temperature}_mnt={max_new_tokens}_tp={top_p}.json\",\n",
    "    save_every=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
