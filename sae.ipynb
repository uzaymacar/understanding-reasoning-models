{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Training\n",
    "\n",
    "This notebook is used to train a Sparse Autoencoder (SAE) on the OpenR1-Math-220k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-03-01 05:43:47.839368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740807827.857863  382456 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740807827.863417  382456 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import Image, display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sae_lens.config import LanguageModelSAERunnerConfig\n",
    "from sae_lens import SAETrainingRunner, SAE, HookedSAETransformer, TrainingSAE, ActivationsStore\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from datasets import load_dataset, Dataset, Features, Value\n",
    "from utils import *\n",
    "import json\n",
    "import os\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7611ed0d60b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)\n",
    "model.cfg.n_ctx = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_balanced_backtracking_dataset(\n",
    "    json_file_paths=[\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\", \"math_cot_results_t=0.7_mnt=1800_tp=0.92.json\"],\n",
    "    output_path=\"backtracking_dataset_n=1000.json\",\n",
    "    n=1000,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"openr1_math_backtracking_dataset.json\"\n",
    "stats = create_openr1_backtracking_dataset(output_path, backtracking_phrases)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I uploaded this dataset to `uzaymacar/openr1_math_backtracking_dataset` in HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset with 193767 examples\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the JSON file\n",
    "path = \"openr1_math_backtracking_dataset_hf\"\n",
    "\n",
    "if os.path.exists(path):  \n",
    "    dataset = Dataset.load_from_disk(path)\n",
    "else:\n",
    "    with open(\"openr1_math_backtracking_dataset.json\", \"r\") as f: dataset_json = json.load(f)\n",
    "    # Filter to keep only the three specified fields\n",
    "    filtered_dataset = []\n",
    "    for item in dataset_json:\n",
    "        filtered_item = {'uuid': item['uuid'], 'has_backtracking': item['has_backtracking'], 'text': item['text']}\n",
    "        filtered_dataset.append(filtered_item)\n",
    "\n",
    "    dataset = Dataset.from_list(filtered_dataset)\n",
    "    dataset.save_to_disk(path)\n",
    "\n",
    "print(f\"Got dataset with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"backtracking_dataset_n=1000.json\", \"r\") as f: examples = json.load(f)\n",
    "examples = [x for x in examples if x[\"has_backtracking\"] and x[\"is_correct\"] and \"i think i made a mistake\" in x[\"generated_cot\"].lower()]\n",
    "examples.sort(key=lambda x: len(x[\"generated_cot\"]))\n",
    "example = examples[0][\"generated_cot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SAE training\n",
    "# Adjust these parameters based on your computational resources\n",
    "total_training_steps = 10000 # Reduce if needed for faster training\n",
    "batch_size = 2048  # Adjust based on your GPU memory\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "# Learning rate schedule parameters\n",
    "lr_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 10  # 10% of training\n",
    "\n",
    "# Create the SAE configuration\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Model configuration\n",
    "    model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    hook_name=\"blocks.21.hook_mlp_out\", \n",
    "    hook_layer=21,  \n",
    "    d_in=1536, \n",
    "    \n",
    "    # Dataset configuration\n",
    "    dataset_path=\"open-r1/OpenR1-Math-220k\",  # We will override this in the training runner\n",
    "    streaming=True,  # Stream the dataset to save memory\n",
    "    \n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,\n",
    "    expansion_factor=16,\n",
    "    b_dec_init_method=\"zeros\",\n",
    "    apply_b_dec_to_input=False,\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_activations=\"expected_average_only_in\",\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr=3e-5, # NEW\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"cosineannealingwarmrestarts\", # NEW, before it was \"constant\"\n",
    "    lr_warm_up_steps=lr_warm_up_steps,\n",
    "    lr_decay_steps=lr_decay_steps,\n",
    "    l1_coefficient=4.0,  # Controls sparsity\n",
    "    l1_warm_up_steps=l1_warm_up_steps,\n",
    "    lp_norm=1.0,\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512, \n",
    "    activation_fn='relu',\n",
    "    prepend_bos=False,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=32,\n",
    "    training_tokens=total_training_tokens,\n",
    "    store_batch_size_prompts=8,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=1000,\n",
    "    dead_feature_window=1000,\n",
    "    dead_feature_threshold=1e-4,\n",
    "    \n",
    "    # WANDB configuration\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"deepseek_sae\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    \n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=2,  # Save 2 checkpoints during training\n",
    "    checkpoint_path=\"deepseek_sae_checkpoints\",\n",
    "    dtype=\"float32\",  # Use bfloat16 if you have memory issues: \"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SAE\n",
    "sae = SAETrainingRunner(cfg, override_dataset=dataset).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sae_lens/sae.py:146: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# s66sea6t/final_40960000 => layer 7 mlp\n",
    "# 09h5vx03/final_40960000 => layer 21 mlp\n",
    "# g2jkk2t9/14204928 => layer 14 mlp\n",
    "# k2j8l3a8/3366912 => layer 25 attention z\n",
    "# mau5uadu/5121024 => layer 21 resid post\n",
    "# a264vb4e/final_20480000 => layer 25 mlp\n",
    "path = \"deepseek_sae_checkpoints/09h5vx03/final_40960000\"\n",
    "sae = TrainingSAE(cfg).load_from_pretrained(path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2047])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "tokens_list = []\n",
    "\n",
    "# Get total dataset size\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# Generate random indices\n",
    "random_indices = random.sample(range(dataset_size), batch_size)\n",
    "\n",
    "# Sample randomly from the dataset\n",
    "for idx in random_indices:\n",
    "    x = dataset[idx]\n",
    "    # Get tokens for this example\n",
    "    token_tensor = model.to_tokens(x[\"text\"])\n",
    "    tokens_list.append(token_tensor)\n",
    "\n",
    "# Find max length for padding\n",
    "max_length = max(t.shape[1] for t in tokens_list)\n",
    "\n",
    "# Pad and stack\n",
    "padded_tokens = []\n",
    "for t in tokens_list:\n",
    "    # Pad with zeros if needed\n",
    "    if t.shape[1] < max_length:\n",
    "        padding = torch.zeros((1, max_length - t.shape[1]), dtype=t.dtype, device=t.device)\n",
    "        t_padded = torch.cat([t, padding], dim=1)\n",
    "    else:\n",
    "        t_padded = t\n",
    "    padded_tokens.append(t_padded)\n",
    "\n",
    "tokens = torch.stack(padded_tokens, dim=0).to(device).view(batch_size, -1)\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis: 100%|██████████| 64/64 [07:28<00:00,  6.99s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time    </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Forward passes to gather model activations │ 434.58s │ 92.3% │\n",
       "│ (1.5) Clearing memory                          │ 1.07s   │ 0.2%  │\n",
       "│ (2) Getting data for sequences                 │ 16.21s  │ 3.4%  │\n",
       "│ (3) Getting data for non-sequence components   │ 4.95s   │ 1.1%  │\n",
       "│ (?) Unaccounted time                           │ 14.02s  │ 3.0%  │\n",
       "└────────────────────────────────────────────────┴─────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Forward passes to gather model activations │ 434.58s │ 92.3% │\n",
       "│ (1.5) Clearing memory                          │ 1.07s   │ 0.2%  │\n",
       "│ (2) Getting data for sequences                 │ 16.21s  │ 3.4%  │\n",
       "│ (3) Getting data for non-sequence components   │ 4.95s   │ 1.1%  │\n",
       "│ (?) Unaccounted time                           │ 14.02s  │ 3.0%  │\n",
       "└────────────────────────────────────────────────┴─────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis: 100%|██████████| 64/64 [07:50<00:00,  7.36s/it]\n",
      "Extracting vis data from cached data: 100%|██████████| 70/70 [07:50<00:00,  6.73s/it]\n",
      "Saving feature-centric vis: 100%|██████████| 70/70 [00:02<00:00, 25.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sae_vis_data = SaeVisData.create(\n",
    "    sae=sae,\n",
    "    model=model,\n",
    "    tokens=tokens,\n",
    "    cfg=SaeVisConfig(features=list(range(64)) + [22424, 5349, 6685, 6041, 1607, 7976], minibatch_size_tokens=16),\n",
    "    clear_memory_between_batches=True,\n",
    "    verbose=True,\n",
    ")\n",
    "sae_vis_data.save_feature_centric_vis(filename=str(\"deepseek_sae_data_math_layer_21_mlp_features_64.html\"), verbose=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 70/70 [00:08<00:00,  8.32it/s]\n"
     ]
    }
   ],
   "source": [
    "sae_vis_data.save_prompt_centric_vis(\n",
    "    filename=str(\"deepseek_sae_data_math_layer_21_mlp_features_64_prompt_centric.html\"), \n",
    "    # prompt=\"Solve this math problem step by step. Put your final answer in \\\\boxed{}. Problem: Find the derivative of f(x) = x^3 - 4x^2 + 5x - 2. Solution: \\n<think>\\nTo find the derivative, I'll apply the power rule to each term.\\nFor x^3, the derivative is 3x^2.\\nFor -4x^2, the derivative is -4(2x) = -8x.\\nFor 5x, the derivative is 5.\\nFor -2, the derivative is 0.\\nWait, I think I went wrong with the second term. Let me double-check. For -4x^2, applying the power rule gives -4 times 2x, which is -8x. So I was correct.\\nCombining all terms: f'(x) = 3x^2 - 8x + 5.\\n</think>\\n\\nTo find the derivative of f(x) = x^3 - 4x^2 + 5x - 2, I'll apply the power rule to each term.\\n\\nFor x^3: The derivative is 3x^2\\nFor -4x^2: The derivative is -4(2x) = -8x\\nFor 5x: The derivative is 5\\nFor -2: The derivative is 0\\n\\nCombining all terms: f'(x) = 3x^2 - 8x + 5\\n\\nTherefore, \\\\boxed{f'(x) = 3x^2 - 8x + 5}\",\n",
    "    prompt=examples[1][\"generated_cot\"],\n",
    "    num_top_features=100,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sae_lens/training/activations_store.py:301: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "activation_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    store_batch_size_prompts=8,\n",
    "    train_batch_size_tokens=4096,\n",
    "    n_batches_in_buffer=32,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top tokens for random features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24566</th>\n",
       "      <th>24567</th>\n",
       "      <th>24568</th>\n",
       "      <th>24569</th>\n",
       "      <th>24570</th>\n",
       "      <th>24571</th>\n",
       "      <th>24572</th>\n",
       "      <th>24573</th>\n",
       "      <th>24574</th>\n",
       "      <th>24575</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>able</td>\n",
       "      <td>diameter</td>\n",
       "      <td>either</td>\n",
       "      <td>widest</td>\n",
       "      <td>factors</td>\n",
       "      <td>center</td>\n",
       "      <td>zero</td>\n",
       "      <td>get</td>\n",
       "      <td>ille</td>\n",
       "      <td>value</td>\n",
       "      <td>...</td>\n",
       "      <td>even</td>\n",
       "      <td>cases</td>\n",
       "      <td>to</td>\n",
       "      <td>stream</td>\n",
       "      <td>close</td>\n",
       "      <td>half</td>\n",
       "      <td>confused</td>\n",
       "      <td>least</td>\n",
       "      <td>when</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>provided</td>\n",
       "      <td>Diameter</td>\n",
       "      <td>Either</td>\n",
       "      <td>_F</td>\n",
       "      <td>Factors</td>\n",
       "      <td>centers</td>\n",
       "      <td>zero</td>\n",
       "      <td>Get</td>\n",
       "      <td>So</td>\n",
       "      <td>值</td>\n",
       "      <td>...</td>\n",
       "      <td>Even</td>\n",
       "      <td>Case</td>\n",
       "      <td>待</td>\n",
       "      <td>stay</td>\n",
       "      <td>closest</td>\n",
       "      <td>paid</td>\n",
       "      <td>final</td>\n",
       "      <td>TO</td>\n",
       "      <td>When</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diss</td>\n",
       "      <td>直径</td>\n",
       "      <td>Either</td>\n",
       "      <td>F</td>\n",
       "      <td>Factors</td>\n",
       "      <td>Center</td>\n",
       "      <td>0</td>\n",
       "      <td>gets</td>\n",
       "      <td>so</td>\n",
       "      <td>value</td>\n",
       "      <td>...</td>\n",
       "      <td>Even</td>\n",
       "      <td>case</td>\n",
       "      <td>to</td>\n",
       "      <td>di</td>\n",
       "      <td>closer</td>\n",
       "      <td>halfway</td>\n",
       "      <td>conf</td>\n",
       "      <td>Least</td>\n",
       "      <td>when</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>希望</td>\n",
       "      <td>diam</td>\n",
       "      <td>OR</td>\n",
       "      <td>A</td>\n",
       "      <td>factor</td>\n",
       "      <td>_center</td>\n",
       "      <td>义务</td>\n",
       "      <td>shift</td>\n",
       "      <td>so</td>\n",
       "      <td>Value</td>\n",
       "      <td>...</td>\n",
       "      <td>even</td>\n",
       "      <td>Cases</td>\n",
       "      <td>yet</td>\n",
       "      <td>-MM</td>\n",
       "      <td>distance</td>\n",
       "      <td>DECLARE</td>\n",
       "      <td>confusion</td>\n",
       "      <td>someone</td>\n",
       "      <td>When</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>present</td>\n",
       "      <td>λ</td>\n",
       "      <td>either</td>\n",
       "      <td>formed</td>\n",
       "      <td>因子</td>\n",
       "      <td>corners</td>\n",
       "      <td>Cathy</td>\n",
       "      <td>Get</td>\n",
       "      <td>hello</td>\n",
       "      <td>larger</td>\n",
       "      <td>...</td>\n",
       "      <td>—even</td>\n",
       "      <td>两种</td>\n",
       "      <td>To</td>\n",
       "      <td>stream</td>\n",
       "      <td>difference</td>\n",
       "      <td>urt</td>\n",
       "      <td>,</td>\n",
       "      <td>Sterling</td>\n",
       "      <td>WHEN</td>\n",
       "      <td>Hemisphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>supposed</td>\n",
       "      <td>Machine</td>\n",
       "      <td>or</td>\n",
       "      <td>k</td>\n",
       "      <td>isors</td>\n",
       "      <td>center</td>\n",
       "      <td>thereof</td>\n",
       "      <td>GET</td>\n",
       "      <td>So</td>\n",
       "      <td>_value</td>\n",
       "      <td>...</td>\n",
       "      <td>despite</td>\n",
       "      <td>Case</td>\n",
       "      <td>_to</td>\n",
       "      <td>Types</td>\n",
       "      <td>clos</td>\n",
       "      <td>WAYS</td>\n",
       "      <td>blood</td>\n",
       "      <td>auce</td>\n",
       "      <td>当</td>\n",
       "      <td>LO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>included</td>\n",
       "      <td>λ</td>\n",
       "      <td>或者</td>\n",
       "      <td>F</td>\n",
       "      <td>Its</td>\n",
       "      <td>vertices</td>\n",
       "      <td>Som</td>\n",
       "      <td>hurry</td>\n",
       "      <td>heim</td>\n",
       "      <td>-value</td>\n",
       "      <td>...</td>\n",
       "      <td>_even</td>\n",
       "      <td>case</td>\n",
       "      <td>To</td>\n",
       "      <td>-stream</td>\n",
       "      <td>Close</td>\n",
       "      <td>ondo</td>\n",
       "      <td>mixed</td>\n",
       "      <td>buffers</td>\n",
       "      <td>whenever</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>found</td>\n",
       "      <td>lambda</td>\n",
       "      <td>或</td>\n",
       "      <td>两年前</td>\n",
       "      <td>Factor</td>\n",
       "      <td>centered</td>\n",
       "      <td>egot</td>\n",
       "      <td>Promise</td>\n",
       "      <td>audience</td>\n",
       "      <td>Value</td>\n",
       "      <td>...</td>\n",
       "      <td>ext</td>\n",
       "      <td>cases</td>\n",
       "      <td>要</td>\n",
       "      <td>Browse</td>\n",
       "      <td>close</td>\n",
       "      <td>synth</td>\n",
       "      <td>conf</td>\n",
       "      <td>ripe</td>\n",
       "      <td>当他</td>\n",
       "      <td>可行</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>contained</td>\n",
       "      <td>waves</td>\n",
       "      <td>或</td>\n",
       "      <td>formation</td>\n",
       "      <td>受益</td>\n",
       "      <td>centre</td>\n",
       "      <td>anytime</td>\n",
       "      <td>Helping</td>\n",
       "      <td>ateral</td>\n",
       "      <td>largest</td>\n",
       "      <td>...</td>\n",
       "      <td>支出</td>\n",
       "      <td>possibilities</td>\n",
       "      <td>forthcoming</td>\n",
       "      <td>dba</td>\n",
       "      <td>Close</td>\n",
       "      <td>明年</td>\n",
       "      <td>Conf</td>\n",
       "      <td>everyone</td>\n",
       "      <td>\"When</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>allowed</td>\n",
       "      <td>rotates</td>\n",
       "      <td>nor</td>\n",
       "      <td>Aleppo</td>\n",
       "      <td>_factors</td>\n",
       "      <td>Center</td>\n",
       "      <td>@media</td>\n",
       "      <td>实验</td>\n",
       "      <td>asse</td>\n",
       "      <td>values</td>\n",
       "      <td>...</td>\n",
       "      <td>Inf</td>\n",
       "      <td>depending</td>\n",
       "      <td>值得</td>\n",
       "      <td>~/.</td>\n",
       "      <td>差距</td>\n",
       "      <td>satisfying</td>\n",
       "      <td>strongly</td>\n",
       "      <td>Di</td>\n",
       "      <td>在我</td>\n",
       "      <td>Themes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24576 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1        2           3         4          5         6      \\\n",
       "0        able   diameter   either      widest   factors     center      zero   \n",
       "1    provided   Diameter   Either          _F   Factors    centers      zero   \n",
       "2        diss         直径   Either           F   Factors     Center         0   \n",
       "3          希望       diam       OR           A    factor    _center        义务   \n",
       "4     present          λ   either      formed        因子    corners     Cathy   \n",
       "5    supposed    Machine       or           k     isors     center   thereof   \n",
       "6    included          λ       或者           F       Its   vertices       Som   \n",
       "7       found     lambda        或         两年前    Factor   centered      egot   \n",
       "8   contained      waves        或   formation        受益     centre   anytime   \n",
       "9     allowed    rotates      nor      Aleppo  _factors     Center    @media   \n",
       "\n",
       "      7          8         9      ...     24566           24567         24568  \\\n",
       "0       get       ille     value  ...      even           cases            to   \n",
       "1       Get         So         值  ...      Even            Case             待   \n",
       "2      gets         so     value  ...      Even            case            to   \n",
       "3     shift         so     Value  ...      even           Cases           yet   \n",
       "4       Get      hello    larger  ...     —even              两种            To   \n",
       "5       GET         So    _value  ...   despite            Case           _to   \n",
       "6     hurry       heim    -value  ...     _even            case            To   \n",
       "7   Promise   audience     Value  ...       ext           cases             要   \n",
       "8   Helping     ateral   largest  ...        支出   possibilities   forthcoming   \n",
       "9        实验       asse    values  ...       Inf       depending            值得   \n",
       "\n",
       "     24569        24570        24571       24572      24573      24574  \\\n",
       "0   stream        close         half    confused      least       when   \n",
       "1     stay      closest         paid       final         TO       When   \n",
       "2       di       closer      halfway        conf      Least       when   \n",
       "3      -MM     distance      DECLARE   confusion    someone       When   \n",
       "4   stream   difference          urt           ,   Sterling       WHEN   \n",
       "5    Types         clos         WAYS       blood       auce          当   \n",
       "6  -stream        Close         ondo       mixed    buffers   whenever   \n",
       "7   Browse        close        synth        conf       ripe         当他   \n",
       "8      dba        Close           明年        Conf   everyone      \"When   \n",
       "9      ~/.           差距   satisfying    strongly         Di         在我   \n",
       "\n",
       "         24575  \n",
       "0            m  \n",
       "1            t  \n",
       "2            j  \n",
       "3            a  \n",
       "4   Hemisphere  \n",
       "5           LO  \n",
       "6            g  \n",
       "7           可行  \n",
       "8            v  \n",
       "9       Themes  \n",
       "\n",
       "[10 rows x 24576 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After training, analyze the SAE features\n",
    "# Get the projection onto the unembedding matrix\n",
    "projection_onto_unembed = sae.W_dec @ model.W_U\n",
    "\n",
    "# Get the top 10 logits for each feature\n",
    "vals, inds = torch.topk(projection_onto_unembed, 10, dim=1)\n",
    "\n",
    "# Get 10 random features to examine\n",
    "random_indices = torch.arange(projection_onto_unembed.shape[0]) # torch.randint(0, projection_onto_unembed.shape[0], (10,)) # torch.tensor([19])\n",
    "\n",
    "# Show the top 10 logits promoted by those features\n",
    "top_10_logits_df = pd.DataFrame([model.to_str_tokens(i) for i in inds[random_indices]], index=random_indices.tolist()).T\n",
    "print(\"Top tokens for random features:\")\n",
    "display(top_10_logits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14759 ['一处', 'edom', ' widths', 'interfaces', '-pack', 'wait', 'cing', ' nutrition', ' redo', '-xs']\n"
     ]
    }
   ],
   "source": [
    "for top_feature in top_10_logits_df.columns:\n",
    "    top_logits = top_10_logits_df[top_feature].tolist()\n",
    "    if 'wait' in top_logits:\n",
    "        print(top_feature, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most important features and their top tokens:\n",
      "Feature 19: ,,  not, :, d,  moment,  off,  in,  but,  when, ,\\\n",
      "Feature 22424: ,, :,  but, d,  not,  moment, a,  in,  this,  by\n",
      "Feature 5349: ,, 这点, 这一点,  del, hang,  /,  moment,  tongue, d,  hold\n",
      "Feature 6685: ,, 侯,  moment,  null,  those,  hard,  Taylor,  not,  off,  ellipt\n"
     ]
    }
   ],
   "source": [
    "# Visualize feature activations on a sample text\n",
    "sample_text = \"<think> 2 + 4 = 5. Wait, I think I made a mistake. 2 + 4 = 6 </think>\"\n",
    "tokens = model.to_tokens(sample_text)\n",
    "_, cache = model.run_with_cache(sample_text)\n",
    "\n",
    "# Get activations from the hook point we trained on\n",
    "activations = cache[f\"blocks.{sae.cfg.hook_layer}.hook_mlp_out\"]\n",
    "\n",
    "# Encode the activations with our SAE\n",
    "feature_activations = sae.encode(activations.reshape(-1, cfg.d_in))\n",
    "feature_activations = feature_activations.reshape(activations.shape[0], activations.shape[1], -1)\n",
    "\n",
    "# Find the most active features\n",
    "feature_importance = feature_activations.abs().mean(dim=(0, 1))\n",
    "# top_features = torch.topk(feature_importance, 100).indices\n",
    "top_features = [19, 22424, 5349, 6685]\n",
    "\n",
    "print(\"\\nMost important features and their top tokens:\")\n",
    "for i, feature_idx in enumerate(top_features):\n",
    "    top_tokens = model.to_str_tokens(inds[feature_idx][:10])\n",
    "    print(f\"Feature {feature_idx}: {', '.join(top_tokens)}\")\n",
    "\n",
    "del cache \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Examples found: 4096: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# finding max activating examples is a bit harder. To do this we need to calculate feature activations for a large number of tokens\n",
    "feature_list = torch.arange(sae.cfg.d_sae)\n",
    "examples_found = 0\n",
    "all_fired_tokens = []\n",
    "all_feature_acts = []\n",
    "all_reconstructions = []\n",
    "all_token_dfs = []\n",
    "\n",
    "total_batches = 1\n",
    "batch_size_prompts = activation_store.store_batch_size_prompts\n",
    "batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
    "pbar = tqdm(range(total_batches))\n",
    "\n",
    "for i in pbar:\n",
    "    tokens = activation_store.get_batch_tokens()\n",
    "    tokens_df = make_token_df(tokens, len_prefix=5, len_suffix=3, model=model)\n",
    "    tokens_df[\"batch\"] = i\n",
    "\n",
    "    flat_tokens = tokens.flatten()\n",
    "\n",
    "    _, cache = model.run_with_cache(tokens, stop_at_layer=sae.cfg.hook_layer + 1, names_filter=[sae.cfg.hook_name])\n",
    "    sae_in = cache[sae.cfg.hook_name]\n",
    "    feature_acts = sae.encode(sae_in).squeeze()\n",
    "\n",
    "    feature_acts = feature_acts.flatten(0, 1)\n",
    "    fired_mask = (feature_acts[:, feature_list]).sum(dim=-1) > 0\n",
    "    fired_tokens = model.to_str_tokens(flat_tokens[fired_mask])\n",
    "    reconstruction = feature_acts[fired_mask][:, feature_list] @ sae.W_dec[feature_list]\n",
    "\n",
    "    token_df = tokens_df.iloc[fired_mask.cpu().nonzero().flatten().numpy()]\n",
    "    all_token_dfs.append(token_df)\n",
    "    all_feature_acts.append(feature_acts[fired_mask][:, feature_list])\n",
    "    all_fired_tokens.append(fired_tokens)\n",
    "    all_reconstructions.append(reconstruction)\n",
    "\n",
    "    examples_found += len(fired_tokens)\n",
    "    pbar.set_description(f\"Examples found: {examples_found}\")\n",
    "    \n",
    "    del cache \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# flatten the list of lists\n",
    "all_token_dfs = pd.concat(all_token_dfs)\n",
    "all_fired_tokens = list_flatten(all_fired_tokens)\n",
    "all_reconstructions = torch.cat(all_reconstructions)\n",
    "all_feature_acts = torch.cat(all_feature_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 24576)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_acts_df = pd.DataFrame(all_feature_acts.detach().cpu().numpy(), columns=[f\"feature_{i}\" for i in feature_list])\n",
    "feature_acts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = 0\n",
    "all_positive_acts = all_feature_acts[all_feature_acts[:, feature_idx] > 0][:, feature_idx].detach()\n",
    "prop_positive_activations = (100 * len(all_positive_acts) / (total_batches * batch_size_tokens))\n",
    "\n",
    "px.histogram(\n",
    "    all_positive_acts.cpu(),\n",
    "    nbins=50,\n",
    "    title=f\"Histogram of positive activations - {prop_positive_activations:.3f}% of activations were positive\",\n",
    "    labels={\"value\": \"Activation\"},\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>str_tokens</th>\n",
       "      <th>unique_token</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>pos</th>\n",
       "      <th>label</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>weight</td>\n",
       "      <td>weight/79</td>\n",
       "      <td>the bucket, and the| weight| of the bucket</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>2/79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0/0</td>\n",
       "      <td>|0|\\right\\</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0/0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>}}</td>\n",
       "      <td>}}/176</td>\n",
       "      <td>Similar triangles (other)| }}|\\end{</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>5/176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>Cos</td>\n",
       "      <td>Cos/163</td>\n",
       "      <td>}\\text { Law of| Cos|ines } \\\\</td>\n",
       "      <td>5</td>\n",
       "      <td>163</td>\n",
       "      <td>5/163</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>ines</td>\n",
       "      <td>ines/164</td>\n",
       "      <td>text { Law of Cos|ines| } \\\\ {</td>\n",
       "      <td>5</td>\n",
       "      <td>164</td>\n",
       "      <td>5/164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>(C</td>\n",
       "      <td>(C/347</td>\n",
       "      <td>2}&lt;3 t$\\n|(C|) $3</td>\n",
       "      <td>2</td>\n",
       "      <td>347</td>\n",
       "      <td>2/347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>)</td>\n",
       "      <td>)/348</td>\n",
       "      <td>}&lt;3 t$\\n(C|)| $3 t</td>\n",
       "      <td>2</td>\n",
       "      <td>348</td>\n",
       "      <td>2/348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>$</td>\n",
       "      <td>$/349</td>\n",
       "      <td>3 t$\\n(C)| $|3 t&lt;s</td>\n",
       "      <td>2</td>\n",
       "      <td>349</td>\n",
       "      <td>2/349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>3</td>\n",
       "      <td>3/350</td>\n",
       "      <td>t$\\n(C) $|3| t&lt;s^{</td>\n",
       "      <td>2</td>\n",
       "      <td>350</td>\n",
       "      <td>2/350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>that</td>\n",
       "      <td>that/511</td>\n",
       "      <td>real numbers $a$| that|</td>\n",
       "      <td>7</td>\n",
       "      <td>511</td>\n",
       "      <td>7/511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     str_tokens unique_token                                      context  \\\n",
       "1103     weight    weight/79   the bucket, and the| weight| of the bucket   \n",
       "0             0          0/0                                   |0|\\right\\   \n",
       "2736         }}       }}/176          Similar triangles (other)| }}|\\end{   \n",
       "2723        Cos      Cos/163               }\\text { Law of| Cos|ines } \\\\   \n",
       "2724       ines     ines/164               text { Law of Cos|ines| } \\\\ {   \n",
       "...         ...          ...                                          ...   \n",
       "1371         (C       (C/347                            2}<3 t$\\n|(C|) $3   \n",
       "1372          )        )/348                           }<3 t$\\n(C|)| $3 t   \n",
       "1373          $        $/349                           3 t$\\n(C)| $|3 t<s   \n",
       "1374          3        3/350                           t$\\n(C) $|3| t<s^{   \n",
       "4095       that     that/511                      real numbers $a$| that|   \n",
       "\n",
       "      prompt  pos  label  batch  \n",
       "1103       2   79   2/79      0  \n",
       "0          0    0    0/0      0  \n",
       "2736       5  176  5/176      0  \n",
       "2723       5  163  5/163      0  \n",
       "2724       5  164  5/164      0  \n",
       "...      ...  ...    ...    ...  \n",
       "1371       2  347  2/347      0  \n",
       "1372       2  348  2/348      0  \n",
       "1373       2  349  2/349      0  \n",
       "1374       2  350  2/350      0  \n",
       "4095       7  511  7/511      0  \n",
       "\n",
       "[4096 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_activations = feature_acts_df.sort_values(f\"feature_{feature_list[6685]}\", ascending=False)\n",
    "all_token_dfs.iloc[top_10_activations.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2346, 4614, 10611\n",
    "for i in tqdm(range(feature_list.shape[0]), desc=\"Finding features with backtracking phrases\"):\n",
    "    top_10_activations = feature_acts_df.sort_values(f\"feature_{feature_list[i]}\", ascending=False)\n",
    "    tokens_we_want_df = all_token_dfs.iloc[top_10_activations.index]\n",
    "    strings = [x.lower() for x in tokens_we_want_df['str_tokens'].tolist()]\n",
    "    if any(x.lower() in strings[0:10] for x in [\"wait\", \"mistake\", \"incorrect\", \"redo\", \"hold\"]):\n",
    "        print(strings[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
