{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-02-28 00:10:32.157333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740701432.179691  888758 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740701432.186539  888758 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import hypergeom\n",
    "import os\n",
    "import re\n",
    "from matplotlib.patches import Patch\n",
    "from utils import *    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Got device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable gradient computation for all tensors to speed up inference and save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x75d984146110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)\n",
    "model.cfg.n_ctx = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Model context length: 2048\n",
      "üß† Model layers: 28\n",
      "üî§ Vocabulary size: 151936\n",
      "üìä Hidden dimension: 1536\n",
      "üß© Attention heads: 12\n",
      "üè∑Ô∏è Model name: DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìè Model context length: {model.cfg.n_ctx}\")\n",
    "print(f\"üß† Model layers: {model.cfg.n_layers}\")\n",
    "print(f\"üî§ Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"üìä Hidden dimension: {model.cfg.d_model}\")\n",
    "print(f\"üß© Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"üè∑Ô∏è Model name: {model.cfg.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 487 CoT solutions...\n",
      "\n",
      "==================================================\n",
      "CHAIN-OF-THOUGHT ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Total problems analyzed: 487\n",
      "\n",
      "1. CORRECTNESS\n",
      "Correct answers: 253 (51.95%)\n",
      "\n",
      "2. THINK TAGS\n",
      "Solutions with </think> close tags: 365 (74.95%)\n",
      "\n",
      "3. TOKEN LIMITS\n",
      "Problems that ran out of tokens: 150 (30.80%)\n",
      "\n",
      "4. BACKTRACKING\n",
      "Solutions with backtracking: 229 (47.02%)\n",
      "Sample of problems with backtracking:\n",
      "     Level: Level 2, Type: Algebra, Phrases: wait, let me double-check\n",
      "     Level: Level 3, Type: Algebra, Phrases: i made a mistake, let me think again\n",
      "     Level: Level 4, Type: Number Theory, Phrases: i made a mistake, wait, let me double-check\n",
      "     Level: Level 3, Type: Counting & Probability, Phrases: wait, let me double-check\n",
      "     Level: Level 3, Type: Intermediate Algebra, Phrases: i made a mistake\n",
      "     Level: Level 3, Type: Number Theory, Phrases: i made a mistake, that's not right, wait, that's not, wait, let me double-check\n",
      "     Level: Level 3, Type: Prealgebra, Phrases: wait, let me double-check\n",
      "     Level: Level 1, Type: Geometry, Phrases: wait, this doesn't\n",
      "\n",
      "5. PERFORMANCE BY LEVEL\n",
      "  Level 1: 85.37% correct (41 problems)\n",
      "  Level 2: 67.05% correct (88 problems)\n",
      "  Level 3: 58.00% correct (100 problems)\n",
      "  Level 4: 52.54% correct (118 problems)\n",
      "  Level 5: 27.86% correct (140 problems)\n",
      "\n",
      "6. PERFORMANCE BY TYPE\n",
      "  Algebra: 70.25% correct (121 problems)\n",
      "  Counting & Probability: 44.44% correct (63 problems)\n",
      "  Geometry: 31.25% correct (48 problems)\n",
      "  Intermediate Algebra: 36.14% correct (83 problems)\n",
      "  Number Theory: 54.39% correct (57 problems)\n",
      "  Prealgebra: 68.57% correct (70 problems)\n",
      "  Precalculus: 35.56% correct (45 problems)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# analysis = run_analysis(\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\")\n",
    "# analysis = run_analysis(\"math_cot_results_t=0.7_mnt=1800_tp=0.92.json\")\n",
    "analysis = run_analysis(\"math_cot_results_t=0.8_mnt=3600_tp=0.92.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from math_cot_results_t=0.6_mnt=1500_tp=0.92.json...\n",
      "Loading data from math_cot_results_t=0.7_mnt=1800_tp=0.92.json...\n",
      "Loaded 1954 total problems from all files\n",
      "Found 992 completed problems\n",
      "Found 280 problems with backtracking that were solved correctly\n",
      "Found 622 problems without backtracking\n",
      "Saved balanced dataset with 100 samples to backtracking_dataset_n=100.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_samples': 100,\n",
       " 'backtracking_samples': 50,\n",
       " 'no_backtracking_samples': 50,\n",
       " 'unique_backtracking_ids': 50,\n",
       " 'unique_no_backtracking_ids': 50,\n",
       " 'original_backtracking_correct_count': 280,\n",
       " 'original_no_backtracking_count': 622,\n",
       " 'total_problems_processed': 1954}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_balanced_backtracking_dataset(\n",
    "    json_file_paths=[\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\", \"math_cot_results_t=0.7_mnt=1800_tp=0.92.json\"],\n",
    "    output_path=\"backtracking_dataset_n=100.json\",\n",
    "    n=100,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_backtracking_neurons(model, json_file_path, min_sample_size=100, top_k=1000):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events by processing entire CoT solutions\n",
    "    and tracking activations at specific backtracking points.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        min_sample_size: Minimum number of examples to process\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"    \n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    non_backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(min_sample_size, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    for result in tqdm(sampled_results, desc=\"Processing examples\"):\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        if not generated_cot: continue\n",
    "        \n",
    "        tokens = model.to_tokens(generated_cot.lower())\n",
    "        str_tokens = model.to_str_tokens(generated_cot.lower())\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Find positions of backtracking phrases in the token sequence\n",
    "        backtracking_positions = []\n",
    "        for phrase in backtracking_phrases:\n",
    "            phrase_tokens = model.to_str_tokens(phrase.lower())\n",
    "            \n",
    "            # Look for this phrase in the token sequence\n",
    "            for i in range(len(str_tokens) - len(phrase_tokens) + 1):\n",
    "                match = True\n",
    "                for j in range(len(phrase_tokens)):\n",
    "                    if i+j >= len(str_tokens) or str_tokens[i+j].strip().lower() != phrase_tokens[j].strip().lower():\n",
    "                        match = False\n",
    "                        break\n",
    "                \n",
    "                if match:\n",
    "                    backtracking_positions.append((i, i + len(phrase_tokens)))\n",
    "        \n",
    "        # If no backtracking phrases found, sample random positions as non-backtracking\n",
    "        if not backtracking_positions:\n",
    "            # Sample random positions (avoiding the beginning and end)\n",
    "            if len(tokens[0]) > 20:\n",
    "                num_samples = min(5, len(tokens[0]) - 10)\n",
    "                for _ in range(num_samples):\n",
    "                    pos = random.randint(5, len(tokens[0]) - 5)\n",
    "                    # Extract activations for this position from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                        non_backtracking_activations.append((layer, layer_activations))\n",
    "        else:\n",
    "            # For each backtracking position, extract activations\n",
    "            for start_pos, end_pos in backtracking_positions:\n",
    "                # Get the position where backtracking starts\n",
    "                trigger_pos = start_pos\n",
    "                \n",
    "                # Extract activations at the trigger position from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer][0, trigger_pos].detach().cpu().numpy()\n",
    "                    backtracking_activations.append((layer, layer_activations))\n",
    "                \n",
    "                # Also sample non-backtracking positions from the same solution\n",
    "                # (avoiding positions close to backtracking phrases)\n",
    "                safe_positions = []\n",
    "                for pos in range(5, len(tokens[0]) - 5):\n",
    "                    # Check if this position is far from any backtracking phrase\n",
    "                    is_safe = True\n",
    "                    for bt_start, bt_end in backtracking_positions:\n",
    "                        if pos >= bt_start - 10 and pos <= bt_end + 10:\n",
    "                            is_safe = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_safe:\n",
    "                        safe_positions.append(pos)\n",
    "                \n",
    "                # Sample from safe positions\n",
    "                if safe_positions:\n",
    "                    num_samples = min(len(backtracking_positions), len(safe_positions))\n",
    "                    for pos in random.sample(safe_positions, num_samples):\n",
    "                        # Extract activations for this position from all layers\n",
    "                        for layer in range(model.cfg.n_layers):\n",
    "                            layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                            non_backtracking_activations.append((layer, layer_activations))\n",
    "\n",
    "    print(f\"Found {len(backtracking_activations)} backtracking activations and {len(non_backtracking_activations)} non-backtracking activations\")\n",
    "    \n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, analyze neuron activations\n",
    "    for layer in tqdm(range(model.cfg.n_layers), desc=\"Analyzing layers\"):\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0: continue\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(layer_backtracking.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            bt_activations = layer_backtracking[:, neuron_idx]\n",
    "            non_bt_activations = layer_non_backtracking[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(bt_activations)\n",
    "            mean_non_backtracking = np.mean(non_bt_activations)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(bt_activations) + np.var(non_bt_activations)) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Create dataset for AUC calculation\n",
    "            X = np.concatenate([bt_activations, non_bt_activations])\n",
    "            y = np.concatenate([np.ones(len(bt_activations)), np.zeros(len(non_bt_activations))])\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try: auc = roc_auc_score(y, X)\n",
    "            except: auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for n in neurons[:top_k]:\n",
    "            all_neurons.append({'layer': layer, 'neuron': n['neuron'], 'effect_size': n['effect_size'], 'auc': n['auc']})\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {'top_neurons': all_neurons[:top_k], 'layer_scores': neuron_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 examples to identify backtracking neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 79.56 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 385.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_888758/467083861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneuron_analysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentify_backtracking_neurons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"backtracking_dataset_n=100.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_888758/4101921951.py\u001b[0m in \u001b[0;36midentify_backtracking_neurons\u001b[0;34m(model, json_file_path, min_sample_size, top_k)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_cot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mstr_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_cot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Find positions of backtracking phrases in the token sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHookedRootModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \"\"\"\n\u001b[0;32m--> 694\u001b[0;31m         out, cache_dict = super().run_with_cache(\n\u001b[0m\u001b[1;32m    695\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mclear_contexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_contexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         ):\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincl_bwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    610\u001b[0m                     )\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                 residual = block(\n\u001b[0m\u001b[1;32m    613\u001b[0m                     \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/components/transformer_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         attn_scores = self.calculate_attention_scores(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )  # [batch, head_index, query_pos, key_pos]\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/components/grouped_query_attention.py\u001b[0m in \u001b[0;36mcalculate_attention_scores\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mungroup_grouped_query_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_kv_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_attention_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     def calculate_z_scores(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/components/abstract_attention.py\u001b[0m in \u001b[0;36mcalculate_attention_scores\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch key_pos head_index d_head -> batch head_index d_head key_pos\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         )\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mattn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk_\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_scores_soft_cap\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             attn_scores = self.cfg.attn_scores_soft_cap * F.tanh(\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 79.56 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 385.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "neuron_analysis = identify_backtracking_neurons(model=model, json_file_path=\"backtracking_dataset_n=100.json\", top_k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 100 neurons associated with backtracking:\")\n",
    "for i, neuron in enumerate(neuron_analysis['top_neurons'][:100]):\n",
    "    print(f\"{i+1}. Layer {neuron['layer']}, Neuron {neuron['neuron']}: Effect size = {neuron['effect_size']:.4f}, AUC = {neuron['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_neurons_on_new_dataset(model, original_top_neurons, new_json_file_path, top_k=100):\n",
    "    \"\"\"\n",
    "    Validate neurons by running the same analysis on a different dataset and comparing results.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        original_top_neurons: List of top neurons from the first analysis\n",
    "        new_json_file_path: Path to a different JSON file with CoT results\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Run the same analysis on the new dataset\n",
    "    new_analysis = identify_backtracking_neurons(\n",
    "        model=model,\n",
    "        json_file_path=new_json_file_path,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Extract the top neurons from both analyses\n",
    "    original_neurons = [(n['layer'], n['neuron']) for n in original_top_neurons[:top_k]]\n",
    "    new_neurons = [(n['layer'], n['neuron']) for n in new_analysis['top_neurons'][:top_k]]\n",
    "    \n",
    "    # Find overlap between the two sets\n",
    "    overlap = set(original_neurons).intersection(set(new_neurons))\n",
    "    \n",
    "    # Calculate overlap statistics\n",
    "    overlap_count = len(overlap)\n",
    "    overlap_percentage = (overlap_count / top_k) * 100\n",
    "    \n",
    "    # Create a mapping of neuron to rank in each analysis\n",
    "    original_ranks = {(n['layer'], n['neuron']): i for i, n in enumerate(original_top_neurons[:top_k])}\n",
    "    new_ranks = {(n['layer'], n['neuron']): i for i, n in enumerate(new_analysis['top_neurons'][:top_k])}\n",
    "    \n",
    "    # For overlapping neurons, get their ranks in both analyses\n",
    "    overlap_details = []\n",
    "    for layer, neuron in overlap:\n",
    "        overlap_details.append({\n",
    "            'layer': layer,\n",
    "            'neuron': neuron,\n",
    "            'original_rank': original_ranks[(layer, neuron)],\n",
    "            'new_rank': new_ranks[(layer, neuron)],\n",
    "            'original_effect_size': next(n['effect_size'] for n in original_top_neurons if n['layer'] == layer and n['neuron'] == neuron),\n",
    "            'new_effect_size': next(n['effect_size'] for n in new_analysis['top_neurons'] if n['layer'] == layer and n['neuron'] == neuron)\n",
    "        })\n",
    "    \n",
    "    # Sort by average rank\n",
    "    overlap_details.sort(key=lambda x: (x['original_rank'] + x['new_rank'])/2)\n",
    "    \n",
    "    return {\n",
    "        'original_neurons': original_neurons,\n",
    "        'new_neurons': new_neurons,\n",
    "        'overlap': list(overlap),\n",
    "        'overlap_count': overlap_count,\n",
    "        'overlap_percentage': overlap_percentage,\n",
    "        'overlap_details': overlap_details,\n",
    "        'new_analysis': new_analysis\n",
    "    }\n",
    "\n",
    "def print_validation_comparison(validation_results):\n",
    "    \"\"\"\n",
    "    Print a comparison of the two analyses.\n",
    "    \n",
    "    Args:\n",
    "        validation_results: Dictionary with validation results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NEURON VALIDATION ACROSS DATASETS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"\\nOverlap: {validation_results['overlap_count']} neurons ({validation_results['overlap_percentage']:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nTop overlapping neurons:\")\n",
    "    for i, neuron in enumerate(validation_results['overlap_details'][:10]):\n",
    "        print(f\"{i+1}. Layer {neuron['layer']}, Neuron {neuron['neuron']}:\")\n",
    "        print(f\"   Original rank: {neuron['original_rank']+1}, Effect size: {neuron['original_effect_size']:.4f}\")\n",
    "        print(f\"   New rank: {neuron['new_rank']+1}, Effect size: {neuron['new_effect_size']:.4f}\")\n",
    "    \n",
    "    print(\"\\nStatistical significance:\")\n",
    "    \n",
    "    # Calculate p-value for this overlap using hypergeometric distribution\n",
    "    # Parameters for hypergeometric test\n",
    "    M = model.cfg.n_layers * model.cfg.d_model  # Total number of neurons\n",
    "    n = len(validation_results['original_neurons'])  # Number of neurons in first set\n",
    "    N = len(validation_results['new_neurons'])  # Number of neurons in second set\n",
    "    k = validation_results['overlap_count']  # Number of overlapping neurons\n",
    "    \n",
    "    # Calculate p-value (probability of this much overlap or more by chance)\n",
    "    p_value = 1 - hypergeom.cdf(k-1, M, n, N)\n",
    "    \n",
    "    print(f\"p-value for overlap: {p_value:.10f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"The overlap is statistically significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"The overlap is not statistically significant (p >= 0.05)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = validate_neurons_on_new_dataset(\n",
    "    model=model,\n",
    "    original_top_neurons=neuron_analysis['top_neurons'],\n",
    "    new_json_file_path=\"toy_dataset_n=10.json\",\n",
    "    top_k=100\n",
    ")\n",
    "print_validation_comparison(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_neuron_activations(model, neuron_info_list, examples):\n",
    "    \"\"\"\n",
    "    Improved visualization of neuron activations with better readability.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        neuron_info_list: List of dictionaries with neuron information (layer, neuron)\n",
    "        examples: List of text examples to analyze\n",
    "        \n",
    "    Returns:\n",
    "        List of matplotlib figures with visualizations\n",
    "    \"\"\"\n",
    "    all_figs = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Identify backtracking phrases\n",
    "        bt_instances = identify_backtracking(example)\n",
    "        bt_phrases = [phrase for phrase, _ in bt_instances] if isinstance(bt_instances[0], tuple) else bt_instances\n",
    "        \n",
    "        # Get tokens\n",
    "        tokens = model.to_tokens(example)\n",
    "        str_tokens = model.to_str_tokens(example)\n",
    "        \n",
    "        # Find positions of backtracking phrases\n",
    "        bt_positions = []\n",
    "        for phrase in bt_phrases:\n",
    "            phrase_tokens = model.to_str_tokens(phrase)\n",
    "            \n",
    "            # Look for this phrase in the token sequence\n",
    "            for i in range(len(str_tokens) - len(phrase_tokens) + 1):\n",
    "                match = True\n",
    "                for j in range(len(phrase_tokens)):\n",
    "                    if str_tokens[i+j].strip().lower() != phrase_tokens[j].strip().lower():\n",
    "                        match = False\n",
    "                        break\n",
    "                \n",
    "                if match:\n",
    "                    bt_positions.extend(range(i, i + len(phrase_tokens)))\n",
    "        \n",
    "        # Run with cache once for this example\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Now process each neuron using the same cache\n",
    "        for neuron_info in neuron_info_list:\n",
    "            layer = neuron_info['layer']\n",
    "            neuron = neuron_info['neuron']\n",
    "            \n",
    "            # Get activations for this layer and neuron from the cache\n",
    "            layer_activations = cache[\"post\", layer][0, :, neuron].detach().cpu().numpy()\n",
    "            \n",
    "            # Create visualization for this neuron\n",
    "            fig, ax = plt.subplots(figsize=(15, 4))\n",
    "            \n",
    "            # Plot activations\n",
    "            bars = ax.bar(range(len(layer_activations)), layer_activations)\n",
    "            \n",
    "            # Color backtracking phrases\n",
    "            for j, bar in enumerate(bars):\n",
    "                if j in bt_positions:\n",
    "                    bar.set_color('red')\n",
    "            \n",
    "            # IMPROVEMENT 1: Only show a subset of token labels\n",
    "            # Show every nth token label to avoid overcrowding\n",
    "            n = max(1, len(str_tokens) // 30)  # Show at most ~30 labels\n",
    "            \n",
    "            # Set x-ticks at every position but only label some\n",
    "            ax.set_xticks(range(len(str_tokens)))\n",
    "            sparse_labels = [''] * len(str_tokens)\n",
    "            for j in range(0, len(str_tokens), n):\n",
    "                sparse_labels[j] = str_tokens[j]\n",
    "            \n",
    "            # IMPROVEMENT 2: Always show backtracking tokens\n",
    "            for j in bt_positions:\n",
    "                sparse_labels[j] = str_tokens[j]\n",
    "            \n",
    "            # IMPROVEMENT 3: Always show high activation tokens\n",
    "            threshold = np.mean(layer_activations) + np.std(layer_activations)\n",
    "            for j, act in enumerate(layer_activations):\n",
    "                if act > threshold and j % n != 0 and j not in bt_positions:\n",
    "                    sparse_labels[j] = str_tokens[j]\n",
    "            \n",
    "            ax.set_xticklabels(sparse_labels, rotation=45, ha='right')\n",
    "            \n",
    "            # Highlight important tokens\n",
    "            for j in range(len(str_tokens)):\n",
    "                if sparse_labels[j]:  # Only for tokens that have labels\n",
    "                    if j in bt_positions:\n",
    "                        ax.get_xticklabels()[j].set_color('red')\n",
    "                        ax.get_xticklabels()[j].set_weight('bold')\n",
    "                    elif layer_activations[j] > threshold:\n",
    "                        ax.get_xticklabels()[j].set_color('blue')\n",
    "                        ax.get_xticklabels()[j].set_weight('bold')\n",
    "            \n",
    "            # Add a legend\n",
    "            legend_elements = [\n",
    "                Patch(facecolor='red', label='Backtracking phrase'),\n",
    "                Patch(facecolor='blue', label='High activation (non-backtracking)')\n",
    "            ]\n",
    "            ax.legend(handles=legend_elements, loc='upper right')\n",
    "            \n",
    "            # IMPROVEMENT 4: Add vertical lines to separate logical sections\n",
    "            # Add vertical lines every 10 tokens for visual guidance\n",
    "            for j in range(10, len(str_tokens), 10):\n",
    "                ax.axvline(x=j, color='gray', linestyle='--', alpha=0.3)\n",
    "            \n",
    "            # IMPROVEMENT 5: Add a mini-map view of the full sequence\n",
    "            # Create a small subplot for the overview\n",
    "            ax_overview = fig.add_axes([0.15, 0.85, 0.7, 0.1])\n",
    "            ax_overview.bar(range(len(layer_activations)), layer_activations, color='gray', alpha=0.5)\n",
    "            for j in bt_positions:\n",
    "                ax_overview.axvspan(j-0.5, j+0.5, color='red', alpha=0.3)\n",
    "            ax_overview.set_xticks([])\n",
    "            ax_overview.set_yticks([])\n",
    "            ax_overview.set_title(\"Full sequence overview\")\n",
    "            \n",
    "            ax.set_title(f\"Neuron {neuron} in Layer {layer}\")\n",
    "            ax.set_ylabel(\"Activation\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            all_figs.append(fig)\n",
    "    \n",
    "    return all_figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_neuron_activations(\n",
    "    model=model, \n",
    "    neuron_info_list=validation_results['overlap_details'], \n",
    "    examples=[\"Problem: Solve for x in 2x + 4 = 8 Solution: \\n<think>\\n Okay, I'll subtract 4 from both sides to isolate x. \\n 2x + 4 - 4 = 8 - 4 \\n 2x = 8 \\n Now, I'll divide both sides by 2 to solve for x. \\n 2x / 2 = 8 / 2 \\n x = 4 \\n No wait, I made a mistake. I forgot to subtract 4 from the right side. \\n 2x / 2 = (8 - 4) / 2 \\n x = 2 </think> \\n The answer is \\\\boxed{2}.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, example_text, window_size=100, layer_id=None, avg_over_heads=False):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns using CircuitsVis.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        example_text: A single text example\n",
    "        window_size: Number of tokens to include in the visualization window\n",
    "        layer_id: The layer to visualize, if None, visualize all layers\n",
    "        avg_over_heads: If True, average attention over heads for each layer\n",
    "        \n",
    "    Returns:\n",
    "        CircuitsVis visualization\n",
    "    \"\"\"    \n",
    "    # Get tokens and run model with cache\n",
    "    tokens = model.to_tokens(example_text)\n",
    "    str_tokens = model.to_str_tokens(example_text)\n",
    "    \n",
    "    # Run with cache to get attention patterns\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Determine window for visualization\n",
    "    start_pos = 0\n",
    "    end_pos = min(len(str_tokens), start_pos + window_size)\n",
    "    \n",
    "    # Get attention patterns for all layers\n",
    "    attention_patterns = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        if layer_id is not None and layer != layer_id: continue\n",
    "        if avg_over_heads:\n",
    "            # Average over heads dimension\n",
    "            avg_attn = cache[\"pattern\", layer][0].mean(dim=0, keepdim=True)\n",
    "            attention_patterns.append(avg_attn)\n",
    "        else:\n",
    "            attention_patterns.append(cache[\"pattern\", layer][0])\n",
    "    \n",
    "    # Create CircuitsVis visualization\n",
    "    window_tokens = str_tokens[start_pos:end_pos]\n",
    "    window_attention = [attn[:, start_pos:end_pos, start_pos:end_pos] for attn in attention_patterns]\n",
    "    \n",
    "    # Create attention head names\n",
    "    attention_head_names = []\n",
    "    for layer in range(len(window_attention)):\n",
    "        if avg_over_heads:\n",
    "            attention_head_names.append(f\"Layer {layer} (avg)\")\n",
    "        else:\n",
    "            for head in range(window_attention[layer].shape[0]):\n",
    "                attention_head_names.append(f\"L{layer}H{head}\")\n",
    "    \n",
    "    # Flatten attention for CircuitsVis\n",
    "    flat_attention = torch.cat([attn for attn in window_attention], dim=0)\n",
    "    \n",
    "    # Create CircuitsVis visualization\n",
    "    vis = cv.attention.attention_patterns(tokens=window_tokens, attention=flat_attention)\n",
    "    \n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"backtracking_dataset_n=100.json\", \"r\") as f: examples = json.load(f)\n",
    "examples = [x for x in examples if x[\"has_backtracking\"] and x[\"is_correct\"]]\n",
    "examples.sort(key=lambda x: len(x[\"generated_cot\"]))\n",
    "example = examples[0][\"generated_cot\"]\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(example)\n",
    "str_tokens = model.to_str_tokens(example)\n",
    "neuron_activations_for_all_layers = torch.stack([cache[\"post\", layer] for layer in range(model.cfg.n_layers)], dim=1)[0, ...]\n",
    "cv.activations.text_neuron_activations(tokens=str_tokens, activations=neuron_activations_for_all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_vis = visualize_attention(model, example, window_size=1000, layer_id=None, avg_over_heads=True)\n",
    "display(attention_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_neurons_and_test(model, top_neurons, test_problems, device):\n",
    "    \"\"\"\n",
    "    Ablate (zero out) the identified neurons and test the effect on backtracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons to ablate\n",
    "        test_problems: List of test problems\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ablation results\n",
    "    \"\"\"\n",
    "    # Define a hook function to ablate specific neurons\n",
    "    def ablation_hook(activations, hook, neurons_to_ablate):\n",
    "        # neurons_to_ablate is a list of (layer, neuron) tuples\n",
    "        for layer, neuron in neurons_to_ablate:\n",
    "            if hook.name == f\"blocks.{layer}.hook_post\":\n",
    "                activations[0, :, neuron] = 0.0\n",
    "        return activations\n",
    "    \n",
    "    # Prepare neurons to ablate\n",
    "    neurons_to_ablate = [(n['layer'], n['neuron']) for n in top_neurons[:20]]  # Ablate top 20\n",
    "    \n",
    "    ablation_results = {\n",
    "        'original': [],\n",
    "        'ablated': []\n",
    "    }\n",
    "    \n",
    "    for problem in tqdm(test_problems, desc=\"Testing ablation\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate solution without ablation\n",
    "        original_prompt = f\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem_text} Solution: \\n<think>\\n\"\n",
    "        original_solution = model.generate(original_prompt, \n",
    "                                         temperature=0.4,\n",
    "                                         max_new_tokens=500,\n",
    "                                         top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in original\n",
    "        original_backtracking = identify_backtracking(original_solution)\n",
    "        \n",
    "        # Generate solution with ablation\n",
    "        ablated_solution = \"\"\n",
    "        \n",
    "        # Set up hooks for ablation\n",
    "        hooks = []\n",
    "        for layer in set(layer for layer, _ in neurons_to_ablate):\n",
    "            hook_name = f\"blocks.{layer}.hook_post\"\n",
    "            hook_fn = lambda act, hook=None, neurons=neurons_to_ablate: ablation_hook(act, hook, neurons)\n",
    "            hooks.append((hook_name, hook_fn))\n",
    "        \n",
    "        # Generate with hooks\n",
    "        with model.hooks(hooks):\n",
    "            ablated_solution = model.generate(original_prompt, \n",
    "                                            temperature=0.4,\n",
    "                                            max_new_tokens=500,\n",
    "                                            top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in ablated\n",
    "        ablated_backtracking = identify_backtracking_enhanced(ablated_solution)\n",
    "        \n",
    "        ablation_results['original'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': original_solution,\n",
    "            'backtracking_count': len(original_backtracking),\n",
    "            'backtracking_instances': original_backtracking\n",
    "        })\n",
    "        \n",
    "        ablation_results['ablated'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': ablated_solution,\n",
    "            'backtracking_count': len(ablated_backtracking),\n",
    "            'backtracking_instances': ablated_backtracking\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    original_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['original'])\n",
    "    ablated_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['ablated'])\n",
    "    \n",
    "    ablation_results['summary'] = {\n",
    "        'original_backtracking_total': original_backtracking_count,\n",
    "        'ablated_backtracking_total': ablated_backtracking_count,\n",
    "        'percent_change': ((ablated_backtracking_count - original_backtracking_count) / \n",
    "                          max(1, original_backtracking_count)) * 100\n",
    "    }\n",
    "    \n",
    "    return ablation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
