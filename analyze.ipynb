{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    # \"mps\" if torch.backends.mps.is_available() else \n",
    "    # \"cuda\" if torch.cuda.is_available() else \n",
    "    \"cpu\" # NOTE: Using CPU as GPU is occupied by main.ipynb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtracking_phrases = [\n",
    "    \"i made a mistake\", \n",
    "    \"let me recalculate\",\n",
    "    \"that's not right\",\n",
    "    \"i need to correct\",\n",
    "    \"let's try again\",\n",
    "    \"i think i went wrong\",\n",
    "    \"let's try another approach\",\n",
    "    \"actually, i should\",\n",
    "    \"wait, that's incorrect\",\n",
    "    \"let me rethink\",\n",
    "    \"on second thought\",\n",
    "    \"i need to backtrack\",\n",
    "    \"let me restart\",\n",
    "    \"i made an error\",\n",
    "    \n",
    "    # Correction indicators\n",
    "    \"hmm, that's not\", \"hmm, that doesn't\", \"hmm, this doesn't\", \n",
    "    \"wait, that's not\", \"wait, that doesn't\", \"wait, this doesn't\",\n",
    "    \"actually, that's not\", \"actually, that doesn't\", \"actually, this doesn't\",\n",
    "    \"oh, that's not\", \"oh, that doesn't\", \"oh, this doesn't\",\n",
    "    \n",
    "    # Reconsideration indicators\n",
    "    # \"let me reconsider\", \"let me think again\", \"on second thought\",\n",
    "    # \"let's reconsider\", \"let's think again\", \"thinking again\",\n",
    "    # \"reconsidering\", \"rethinking\", \"let me double-check\",\n",
    "    \n",
    "    # Mistake acknowledgment\n",
    "    \"i made a calculation error\", \"calculation error\", \"computational error\",\n",
    "    \"arithmetic error\", \"i miscalculated\",\n",
    "    \"i miscounted\", \"i misunderstood\", \"i misinterpreted\",\n",
    "    \n",
    "    # Approach change indicators\n",
    "    # \"different approach\", \"alternative approach\", \"another method\",\n",
    "    # \"different method\", \"alternative method\", \"different strategy\",\n",
    "    # \"alternative strategy\", \"different way\", \"alternative way\",\n",
    "    \n",
    "    # Doubt indicators\n",
    "    \"i'm not sure if\", \"i'm not convinced\", \"i'm skeptical\",\n",
    "    \"i'm doubtful\", \"i'm uncertain\", \"i'm not confident\",\n",
    "    \"i'm hesitant\", \"i'm not sure about\", \"i'm not certain\",\n",
    "    \n",
    "    # Specific math correction phrases\n",
    "    \"let me redo this calculation\", \"let me recalculate\",\n",
    "    \"i need to redo\", \"i should redo\", \"i'll redo\",\n",
    "    \"let me solve this again\", \"let me solve this differently\",\n",
    "    \"let me approach this differently\", \"let me try a different approach\"\n",
    "]\n",
    "\n",
    "def identify_backtracking(cot_text):\n",
    "    \"\"\"\n",
    "    Identify potential backtracking phrases in a CoT solution.\n",
    "    \n",
    "    Args:\n",
    "        cot_text: The generated CoT solution text\n",
    "    \n",
    "    Returns:\n",
    "        List of identified backtracking phrases\n",
    "    \"\"\"    \n",
    "    found_phrases = []\n",
    "    for phrase in backtracking_phrases:\n",
    "        if phrase.lower() in cot_text.lower():\n",
    "            found_phrases.append(phrase)\n",
    "    \n",
    "    return found_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cot_results(json_file_path):\n",
    "    \"\"\"\n",
    "    Analyze the Chain-of-Thought results from a saved JSON file.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(f\"Analyzing {len(results)} CoT solutions...\")\n",
    "    \n",
    "    # Initialize counters and storage\n",
    "    analysis = {\n",
    "        \"total_problems\": len(results),\n",
    "        \"correct_answers\": 0,\n",
    "        \"has_think_close_tag\": 0,\n",
    "        \"ran_out_of_tokens\": 0,\n",
    "        \"has_backtracking\": 0,\n",
    "        \"token_limit_problems\": [],\n",
    "        \"backtracking_problems\": [],\n",
    "        \"level_distribution\": Counter(),\n",
    "        \"type_distribution\": Counter(),\n",
    "        \"level_accuracy\": {},\n",
    "        \"type_accuracy\": {}\n",
    "    }\n",
    "    \n",
    "    # Track level-specific metrics\n",
    "    level_correct = Counter()\n",
    "    level_total = Counter()\n",
    "    type_correct = Counter()\n",
    "    type_total = Counter()\n",
    "    \n",
    "    # Analyze each problem\n",
    "    for result in results:\n",
    "        problem_level = result.get(\"problem_level\", \"Unknown\")\n",
    "        problem_type = result.get(\"problem_type\", \"Unknown\")\n",
    "        problem_id = result.get(\"problem_id\", \"Unknown\")\n",
    "        \n",
    "        # Update distributions\n",
    "        analysis[\"level_distribution\"][problem_level] += 1\n",
    "        analysis[\"type_distribution\"][problem_type] += 1\n",
    "        level_total[problem_level] += 1\n",
    "        type_total[problem_type] += 1\n",
    "        \n",
    "        # 1. Check for correct answers (this is a simplified check - may need refinement)\n",
    "        # Extract boxed answers from both generated and ground truth\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        ground_truth = result.get(\"ground_truth_solution\", \"\")\n",
    "        \n",
    "        # Extract boxed answers\n",
    "        def extract_boxed_answers(text):\n",
    "            boxed_pattern = r\"\\\\boxed{([^}]*)}\"\n",
    "            matches = re.findall(boxed_pattern, text)\n",
    "            return [match.strip() for match in matches]\n",
    "        \n",
    "        generated_answers = extract_boxed_answers(generated_cot)\n",
    "        ground_truth_answers = extract_boxed_answers(ground_truth)\n",
    "        \n",
    "        # Check if any generated answer matches any ground truth answer\n",
    "        is_correct = False\n",
    "        if generated_answers and ground_truth_answers:\n",
    "            # Normalize answers for comparison (remove spaces, convert to lowercase)\n",
    "            norm_generated = [re.sub(r'\\s+', '', ans.lower()) for ans in generated_answers]\n",
    "            norm_ground_truth = [re.sub(r'\\s+', '', ans.lower()) for ans in ground_truth_answers]\n",
    "            \n",
    "            # Check for any match\n",
    "            for gen_ans in norm_generated:\n",
    "                if any(gen_ans == gt_ans for gt_ans in norm_ground_truth):\n",
    "                    is_correct = True\n",
    "                    break\n",
    "        \n",
    "        if is_correct:\n",
    "            analysis[\"correct_answers\"] += 1\n",
    "            level_correct[problem_level] += 1\n",
    "            type_correct[problem_type] += 1\n",
    "        \n",
    "        # 2. Check for </think> close tags\n",
    "        if \"</think>\" in generated_cot:\n",
    "            analysis[\"has_think_close_tag\"] += 1\n",
    "        \n",
    "        # 3. Check if ran out of tokens (heuristic: no boxed answer)\n",
    "        # We determine token limit issues by checking if there's no boxed answer\n",
    "        if len(generated_answers) == 1 and generated_answers[0] == '':\n",
    "            analysis[\"ran_out_of_tokens\"] += 1\n",
    "            analysis[\"token_limit_problems\"].append({\n",
    "                \"id\": problem_id,\n",
    "                \"level\": problem_level,\n",
    "                \"type\": problem_type\n",
    "            })\n",
    "        \n",
    "        # 4. Check for backtracking\n",
    "        backtracking_phrases = identify_backtracking(generated_cot)\n",
    "        if backtracking_phrases:\n",
    "            analysis[\"has_backtracking\"] += 1\n",
    "            analysis[\"backtracking_problems\"].append({\n",
    "                \"id\": problem_id,\n",
    "                \"level\": problem_level,\n",
    "                \"type\": problem_type,\n",
    "                \"phrases\": backtracking_phrases,\n",
    "                \"correct_after_backtracking\": is_correct\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy by level and type\n",
    "    for level, count in level_total.items():\n",
    "        analysis[\"level_accuracy\"][level] = level_correct[level] / count if count > 0 else 0\n",
    "    \n",
    "    for problem_type, count in type_total.items():\n",
    "        analysis[\"type_accuracy\"][problem_type] = type_correct[problem_type] / count if count > 0 else 0\n",
    "    \n",
    "    # Calculate percentages for easier interpretation\n",
    "    analysis[\"percent_correct\"] = (analysis[\"correct_answers\"] / analysis[\"total_problems\"]) * 100 if analysis[\"total_problems\"] > 0 else 0\n",
    "    analysis[\"percent_think_close\"] = (analysis[\"has_think_close_tag\"] / analysis[\"total_problems\"]) * 100 if analysis[\"total_problems\"] > 0 else 0\n",
    "    analysis[\"percent_token_limit\"] = (analysis[\"ran_out_of_tokens\"] / analysis[\"total_problems\"]) * 100 if analysis[\"total_problems\"] > 0 else 0\n",
    "    analysis[\"percent_backtracking\"] = (analysis[\"has_backtracking\"] / analysis[\"total_problems\"]) * 100 if analysis[\"total_problems\"] > 0 else 0\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis_report(analysis):\n",
    "    \"\"\"\n",
    "    Print a formatted report of the analysis results.\n",
    "    \n",
    "    Args:\n",
    "        analysis: Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CHAIN-OF-THOUGHT ANALYSIS REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"\\nTotal problems analyzed: {analysis['total_problems']}\")\n",
    "    \n",
    "    print(\"\\n1. CORRECTNESS\")\n",
    "    print(f\"Correct answers: {analysis['correct_answers']} ({analysis['percent_correct']:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n2. THINK TAGS\")\n",
    "    print(f\"Solutions with </think> close tags: {analysis['has_think_close_tag']} ({analysis['percent_think_close']:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n3. TOKEN LIMITS\")\n",
    "    print(f\"Problems that ran out of tokens: {analysis['ran_out_of_tokens']} ({analysis['percent_token_limit']:.2f}%)\")\n",
    "    if analysis['token_limit_problems']:\n",
    "        print(\"Sample of problems that ran out of tokens:\")\n",
    "        for i, problem in enumerate(analysis['token_limit_problems']):\n",
    "            print(f\"  {i+1}. Level: {problem['level']}, Type: {problem['type']}\")\n",
    "    \n",
    "    print(\"\\n4. BACKTRACKING\")\n",
    "    print(f\"Solutions with backtracking: {analysis['has_backtracking']} ({analysis['percent_backtracking']:.2f}%)\")\n",
    "    if analysis['backtracking_problems']:\n",
    "        print(\"Sample of problems with backtracking:\")\n",
    "        for i, problem in enumerate(analysis['backtracking_problems']):  # Show first 5\n",
    "            print(f\"  {i+1}. Level: {problem['level']}, Type: {problem['type']}\")\n",
    "            print(f\"     Phrases: {', '.join(problem['phrases'])}\")\n",
    "            print(f\"     Correct after backtracking: {problem['correct_after_backtracking']}\")\n",
    "    print(\"\\n5. PERFORMANCE BY LEVEL\")\n",
    "    for level, accuracy in sorted(analysis['level_accuracy'].items()):\n",
    "        count = analysis['level_distribution'][level]\n",
    "        print(f\"  {level}: {accuracy*100:.2f}% correct ({count} problems)\")\n",
    "    \n",
    "    print(\"\\n6. PERFORMANCE BY TYPE\")\n",
    "    for problem_type, accuracy in sorted(analysis['type_accuracy'].items()):\n",
    "        count = analysis['type_distribution'][problem_type]\n",
    "        print(f\"  {problem_type}: {accuracy*100:.2f}% correct ({count} problems)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(json_file_path):\n",
    "    \"\"\"\n",
    "    Run the analysis on a JSON file and print the report.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "    \"\"\"\n",
    "    analysis = analyze_cot_results(json_file_path)\n",
    "    print_analysis_report(analysis)\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 41 CoT solutions...\n",
      "\n",
      "==================================================\n",
      "CHAIN-OF-THOUGHT ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Total problems analyzed: 41\n",
      "\n",
      "1. CORRECTNESS\n",
      "Correct answers: 13 (31.71%)\n",
      "\n",
      "2. THINK TAGS\n",
      "Solutions with </think> close tags: 20 (48.78%)\n",
      "\n",
      "3. TOKEN LIMITS\n",
      "Problems that ran out of tokens: 22 (53.66%)\n",
      "Sample of problems that ran out of tokens:\n",
      "  1. Level: Level 5, Type: Precalculus\n",
      "  2. Level: Level 5, Type: Counting & Probability\n",
      "  3. Level: Level 5, Type: Counting & Probability\n",
      "  4. Level: Level 3, Type: Intermediate Algebra\n",
      "  5. Level: Level 5, Type: Intermediate Algebra\n",
      "  6. Level: Level 4, Type: Geometry\n",
      "  7. Level: Level 2, Type: Intermediate Algebra\n",
      "  8. Level: Level 5, Type: Counting & Probability\n",
      "  9. Level: Level 5, Type: Algebra\n",
      "  10. Level: Level 5, Type: Precalculus\n",
      "  11. Level: Level 4, Type: Algebra\n",
      "  12. Level: Level 4, Type: Intermediate Algebra\n",
      "  13. Level: Level 5, Type: Counting & Probability\n",
      "  14. Level: Level 5, Type: Counting & Probability\n",
      "  15. Level: Level 3, Type: Counting & Probability\n",
      "  16. Level: Level 4, Type: Counting & Probability\n",
      "  17. Level: Level 5, Type: Geometry\n",
      "  18. Level: Level 3, Type: Precalculus\n",
      "  19. Level: Level 5, Type: Intermediate Algebra\n",
      "  20. Level: Level 3, Type: Number Theory\n",
      "  21. Level: Level 3, Type: Intermediate Algebra\n",
      "  22. Level: Level 5, Type: Precalculus\n",
      "\n",
      "4. BACKTRACKING\n",
      "Solutions with backtracking: 9 (21.95%)\n",
      "Sample of problems with backtracking:\n",
      "  1. Level: Level 5, Type: Counting & Probability\n",
      "     Phrases: let me try a different approach\n",
      "     Correct after backtracking: False\n",
      "  2. Level: Level 3, Type: Geometry\n",
      "     Phrases: i made a mistake\n",
      "     Correct after backtracking: True\n",
      "  3. Level: Level 2, Type: Intermediate Algebra\n",
      "     Phrases: that's not right\n",
      "     Correct after backtracking: False\n",
      "  4. Level: Level 5, Type: Algebra\n",
      "     Phrases: wait, that doesn't\n",
      "     Correct after backtracking: False\n",
      "  5. Level: Level 4, Type: Intermediate Algebra\n",
      "     Phrases: i made a mistake\n",
      "     Correct after backtracking: False\n",
      "  6. Level: Level 5, Type: Counting & Probability\n",
      "     Phrases: let me try a different approach\n",
      "     Correct after backtracking: False\n",
      "  7. Level: Level 2, Type: Counting & Probability\n",
      "     Phrases: wait, that doesn't\n",
      "     Correct after backtracking: True\n",
      "  8. Level: Level 2, Type: Prealgebra\n",
      "     Phrases: calculation error\n",
      "     Correct after backtracking: True\n",
      "  9. Level: Level 5, Type: Intermediate Algebra\n",
      "     Phrases: i made a mistake, wait, that doesn't\n",
      "     Correct after backtracking: False\n",
      "\n",
      "5. PERFORMANCE BY LEVEL\n",
      "  Level 1: 100.00% correct (2 problems)\n",
      "  Level 2: 42.86% correct (7 problems)\n",
      "  Level 3: 36.36% correct (11 problems)\n",
      "  Level 4: 37.50% correct (8 problems)\n",
      "  Level 5: 7.69% correct (13 problems)\n",
      "\n",
      "6. PERFORMANCE BY TYPE\n",
      "  Algebra: 50.00% correct (4 problems)\n",
      "  Counting & Probability: 10.00% correct (10 problems)\n",
      "  Geometry: 33.33% correct (3 problems)\n",
      "  Intermediate Algebra: 12.50% correct (8 problems)\n",
      "  Number Theory: 50.00% correct (2 problems)\n",
      "  Prealgebra: 57.14% correct (7 problems)\n",
      "  Precalculus: 42.86% correct (7 problems)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "analysis = run_analysis(\"math_cot_results_t=0.4_mnt=1500_tp=0.92.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 6, 'level': 'Level 3', 'type': 'Geometry', 'phrases': ['I made a mistake'], 'correct_after_backtracking': True}]\n",
      "Backtracking Problems:\n",
      "\n",
      "Problem 1:\n",
      "  Level: Level 3\n",
      "  Type: Geometry\n",
      "  Phrases: I made a mistake\n",
      "  Correct after backtracking: True\n",
      "  Problem Text: When plotted in the standard rectangular coordinate system, trapezoid $ABCD$ has vertices $A(1, -2)$, $B(1, 1)$, $C(5, 7)$ and $D(5, 1)$. What is the area of trapezoid $ABCD$?\n",
      "  Solution:\n",
      "The two bases of the trapezoids are the segments $AB$ and $CD$, and the height is the perpendicular distance between the bases, which in this case is the difference of the $x$-coordinates: $5 - 1 = 4$. Similarly, the lengths of the bases are the differences of the $y$-coordinates of their two endpoints. Using the formula $A = \\frac{1}{2}(b_1+ b_2)h$, the area is $\\frac{1}{2}(3+6)(4) = \\boxed{18}$ square units.\n",
      "  Generated Solution:\n",
      "Solve this math problem step by step. Put your final answer in \\boxed{}. Problem: When plotted in the standard rectangular coordinate system, trapezoid $ABCD$ has vertices $A(1, -2)$, $B(1, 1)$, $C(5, 7)$ and $D(5, 1)$. What is the area of trapezoid $ABCD$? Solution: \n",
      "<think>\n",
      "Okay, so I need to find the area of trapezoid ABCD. The vertices are given as A(1, -2), B(1, 1), C(5, 7), and D(5, 1). Hmm, trapezoid means it has one pair of sides that are parallel. I remember that in a trapezoid, the two parallel sides are called the bases, and the distance between them is the height. The area is then calculated as the average of the two bases multiplied by the height.\n",
      "\n",
      "First, I should plot these points or at least visualize them to understand the shape. Let me note down the coordinates:\n",
      "\n",
      "- A is at (1, -2)\n",
      "- B is at (1, 1)\n",
      "- C is at (5, 7)\n",
      "- D is at (5, 1)\n",
      "\n",
      "Looking at the x-coordinates, A and B share the same x-value of 1, while C and D share the same x-value of 5. Similarly, the y-coordinates for A and B are -2 and 1, while for C and D, they are 7 and 1. So, it seems that sides AB and CD are vertical because they share the same x-values. Wait, but AB is from (1, -2) to (1, 1), which is a vertical line. Similarly, CD is from (5, 1) to (5, 7), which is also a vertical line. So, AB and CD are both vertical, meaning they are parallel to each other. Therefore, the trapezoid has two sides that are vertical, which are the bases, and the other two sides are the legs, which are not vertical.\n",
      "\n",
      "So, the bases are AB and CD. Now, I need to find their lengths. AB goes from (1, -2) to (1, 1). Since the x-coordinate doesn't change, this is a vertical line. The length is the difference in the y-coordinates. So, from -2 to 1 is a change of 3 units. So, AB is 3 units long.\n",
      "\n",
      "Similarly, CD goes from (5, 1) to (5, 7). Again, the x-coordinate doesn't change, so it's vertical. The length is the difference in y-coordinates, which is 7 - 1 = 6 units. So, CD is 6 units long.\n",
      "\n",
      "Now, the height of the trapezoid is the distance between the two parallel sides AB and CD. Since AB and CD are both vertical, the distance between them is the horizontal distance between x=1 and x=5. So, that's 5 - 1 = 4 units. Therefore, the height is 4 units.\n",
      "\n",
      "Now, the area of a trapezoid is given by the formula: (base1 + base2)/2 * height. Plugging in the values, that's (3 + 6)/2 * 4.\n",
      "\n",
      "Let me compute that step by step. First, 3 + 6 is 9. Then, 9 divided by 2 is 4.5. Then, 4.5 multiplied by 4 is 18. So, the area should be 18 square units.\n",
      "\n",
      "Wait, let me double-check. Maybe I made a mistake in calculating the bases or the height. AB is vertical from (1, -2) to (1, 1), so length is 3. CD is vertical from (5, 1) to (5, 7), so length is 6. The distance between AB and CD is the horizontal distance between x=1 and x=5, which is 4. So, plugging into the formula: (3 + 6)/2 * 4 = (9/2)*4 = 4.5*4 = 18. Yes, that seems correct.\n",
      "\n",
      "Alternatively, I can think of the trapezoid as a rectangle minus some parts. But in this case, since it's a trapezoid with vertical sides, maybe that's not the easiest way. But the formula seems solid.\n",
      "\n",
      "Wait another thought: sometimes, the height is not the horizontal distance if the sides are not vertical. But in this case, since both AB and CD are vertical, the height is indeed the horizontal distance between their x-values. So, that's correct.\n",
      "\n",
      "Therefore, I'm confident that the area is 18 square units.\n",
      "</think>\n",
      "\n",
      "The area of trapezoid ABCD is calculated by finding the lengths of the two parallel sides (bases) and the height, then applying the formula for the area of a trapezoid.\n",
      "\n",
      "1. **Identify the bases and height:**\n",
      "   - AB and CD are vertical sides, so they are the bases.\n",
      "   - The length of AB is the difference in y-coordinates: 1 - (-2) = 3 units.\n",
      "   - The length of CD is the difference in y-coordinates: 7 - 1 = 6 units.\n",
      "   - The height is the horizontal distance between x=1 and x=5: 5 - 1 = 4 units.\n",
      "\n",
      "2. **Apply the formula:**\n",
      "   \\[\n",
      "   \\text{Area} = \\frac{\\text{Base1} + \\text{Base2}}{2} \\times \\text{Height} = \\frac{3 + 6}{2} \\times 4 = \\frac{9}{2} \\times 4 = 4.5 \\times 4 = 18\n",
      "   \\]\n",
      "\n",
      "**Answer:** \\boxed{18}\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "with open(\"math_cot_results_t=0.4_mnt=1500_tp=0.92.json\", 'r') as f:\n",
    "    results = json.load(f)\n",
    "        \n",
    "# Get the backtracking problems\n",
    "backtracking_problems = analysis['backtracking_problems']\n",
    "print(backtracking_problems)\n",
    "\n",
    "# Print the answers\n",
    "print(\"Backtracking Problems:\")\n",
    "for i, problem in enumerate(backtracking_problems):\n",
    "    print(f\"\\nProblem {i+1}:\")\n",
    "    \n",
    "    # Get the full problem details from results using the problem id\n",
    "    problem_id = problem['id']\n",
    "    full_problem = next((p for p in results if p['problem_id'] == problem_id), None)\n",
    "    \n",
    "    print(f\"  Level: {problem['level']}\")\n",
    "    print(f\"  Type: {problem['type']}\")\n",
    "    print(f\"  Phrases: {', '.join(problem['phrases'])}\")\n",
    "    print(f\"  Correct after backtracking: {problem['correct_after_backtracking']}\")\n",
    "    \n",
    "    if full_problem:\n",
    "        print(f\"  Problem Text: {full_problem.get('problem_text', 'N/A')}\")\n",
    "        print(f\"  Solution:\")\n",
    "        print(f\"{full_problem.get('ground_truth_solution', 'N/A')}\")\n",
    "        print(f\"  Generated Solution:\")\n",
    "        print(f\"{full_problem.get('generated_cot', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"  Problem details not found for id: {problem_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_backtracking_neurons_improved(model, json_file_path, device, top_k=50):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events by processing entire CoT solutions\n",
    "    and tracking activations at specific backtracking points.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        device: The device to run inference on\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"\n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    non_backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(100, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    for result in tqdm(sampled_results):\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        \n",
    "        # Skip if the generated CoT is empty\n",
    "        if not generated_cot:\n",
    "            continue\n",
    "        \n",
    "        # Process the entire CoT solution\n",
    "        tokens = model.to_tokens(generated_cot)\n",
    "        str_tokens = model.to_str_tokens(generated_cot)\n",
    "        \n",
    "        # Run the model with cache to get all activations\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Find positions of backtracking phrases in the token sequence\n",
    "        backtracking_positions = []\n",
    "        for phrase in backtracking_phrases:\n",
    "            phrase_tokens = model.to_str_tokens(phrase)\n",
    "            \n",
    "            # Look for this phrase in the token sequence\n",
    "            for i in range(len(str_tokens[0]) - len(phrase_tokens) + 1):\n",
    "                # Check if this position contains the phrase\n",
    "                match = True\n",
    "                for j, token in enumerate(phrase_tokens):\n",
    "                    if i+j >= len(str_tokens[0]) or str_tokens[0][i+j].lower() != token.lower():\n",
    "                        match = False\n",
    "                        break\n",
    "                \n",
    "                if match:\n",
    "                    # Found a match, add the position range\n",
    "                    backtracking_positions.append((i, i + len(phrase_tokens)))\n",
    "        \n",
    "        # If no backtracking phrases found, sample random positions as non-backtracking\n",
    "        if not backtracking_positions:\n",
    "            # Sample random positions (avoiding the beginning and end)\n",
    "            if len(tokens[0]) > 20:\n",
    "                num_samples = min(5, len(tokens[0]) - 10)\n",
    "                for _ in range(num_samples):\n",
    "                    pos = random.randint(5, len(tokens[0]) - 5)\n",
    "                    # Extract activations for this position from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                        non_backtracking_activations.append((layer, layer_activations))\n",
    "        else:\n",
    "            # For each backtracking position, extract activations\n",
    "            for start_pos, end_pos in backtracking_positions:\n",
    "                # Get the position where backtracking starts\n",
    "                trigger_pos = start_pos\n",
    "                \n",
    "                # Extract activations at the trigger position from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer][0, trigger_pos].detach().cpu().numpy()\n",
    "                    backtracking_activations.append((layer, layer_activations))\n",
    "                \n",
    "                # Also sample non-backtracking positions from the same solution\n",
    "                # (avoiding positions close to backtracking phrases)\n",
    "                safe_positions = []\n",
    "                for pos in range(5, len(tokens[0]) - 5):\n",
    "                    # Check if this position is far from any backtracking phrase\n",
    "                    is_safe = True\n",
    "                    for bt_start, bt_end in backtracking_positions:\n",
    "                        if pos >= bt_start - 10 and pos <= bt_end + 10:\n",
    "                            is_safe = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_safe:\n",
    "                        safe_positions.append(pos)\n",
    "                \n",
    "                # Sample from safe positions\n",
    "                if safe_positions:\n",
    "                    num_samples = min(len(backtracking_positions), len(safe_positions))\n",
    "                    for pos in random.sample(safe_positions, num_samples):\n",
    "                        # Extract activations for this position from all layers\n",
    "                        for layer in range(model.cfg.n_layers):\n",
    "                            layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                            non_backtracking_activations.append((layer, layer_activations))\n",
    "    \n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, analyze neuron activations\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Collect activations for this layer\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(layer_backtracking.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            bt_activations = layer_backtracking[:, neuron_idx]\n",
    "            non_bt_activations = layer_non_backtracking[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(bt_activations)\n",
    "            mean_non_backtracking = np.mean(non_bt_activations)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(bt_activations) + np.var(non_bt_activations)) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Create dataset for AUC calculation\n",
    "            X = np.concatenate([bt_activations, non_bt_activations])\n",
    "            y = np.concatenate([\n",
    "                np.ones(len(bt_activations)),\n",
    "                np.zeros(len(non_bt_activations))\n",
    "            ])\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try:\n",
    "                auc = roc_auc_score(y, X)\n",
    "            except:\n",
    "                auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], \n",
    "                                     key=lambda x: abs(x['effect_size']), \n",
    "                                     reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for neuron in neurons[:top_k]:\n",
    "            all_neurons.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron['neuron'],\n",
    "                'effect_size': neuron['effect_size'],\n",
    "                'auc': neuron['auc']\n",
    "            })\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'top_neurons': all_neurons[:top_k],\n",
    "        'layer_scores': neuron_scores\n",
    "    }\n",
    "    \n",
    "def identify_backtracking_neurons(model, json_file_path, device, top_k=50):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        device: The device to run inference on\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"\n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []\n",
    "    non_backtracking_activations = []\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(100, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    for result in tqdm(sampled_results):\n",
    "        problem_text = result.get(\"problem_text\", \"\")\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        \n",
    "        # Skip if the generated CoT is empty\n",
    "        if not generated_cot:\n",
    "            continue\n",
    "        \n",
    "        # Identify backtracking phrases with context\n",
    "        backtracking_instances = identify_backtracking(generated_cot)\n",
    "        \n",
    "        # If no backtracking, use this as a control example\n",
    "        if not backtracking_instances or len(backtracking_instances) == 0:\n",
    "            # Get a random segment from the CoT\n",
    "            tokens = model.to_tokens(generated_cot)\n",
    "            if len(tokens[0]) > 20:  # Ensure we have enough tokens\n",
    "                # Take a random segment with context window similar to backtracking examples\n",
    "                context_window = 50  # characters before and after, matching the backtracking case\n",
    "                \n",
    "                # Convert to text indices for consistency with backtracking case\n",
    "                text_length = len(generated_cot)\n",
    "                if text_length > context_window * 2:\n",
    "                    # Pick a random center point\n",
    "                    center_idx = random.randint(context_window, text_length - context_window)\n",
    "                    context_start = center_idx - context_window\n",
    "                    context_end = center_idx + context_window\n",
    "                    context = generated_cot[context_start:context_end]\n",
    "                else:\n",
    "                    # If text is too short, use the whole text\n",
    "                    context = generated_cot\n",
    "                \n",
    "                # Get activations for this context\n",
    "                tokens = model.to_tokens(context)\n",
    "                _, cache = model.run_with_cache(tokens)\n",
    "                \n",
    "                # Extract activations from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "                    # Flatten across sequence positions\n",
    "                    flat_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
    "                    non_backtracking_activations.append((layer, flat_activations))\n",
    "        else:\n",
    "            # For each backtracking instance, get the surrounding context\n",
    "            for phrase in backtracking_instances:\n",
    "                # Locate the phrase in the generated CoT (case insensitive)\n",
    "                phrase_lower = phrase.lower()\n",
    "                generated_cot_lower = generated_cot.lower()\n",
    "                start_idx = generated_cot_lower.find(phrase_lower)\n",
    "                \n",
    "                if start_idx != -1:\n",
    "                    end_idx = start_idx + len(phrase)\n",
    "                    \n",
    "                    # Extract context around the backtracking phrase\n",
    "                    context_window = 50  # characters before and after\n",
    "                    context_start = max(0, start_idx - context_window)\n",
    "                    context_end = min(len(generated_cot), end_idx + context_window)\n",
    "                    context = generated_cot[context_start:context_end]\n",
    "                    \n",
    "                    # Convert context to tokens\n",
    "                    tokens = model.to_tokens(context)\n",
    "                    \n",
    "                    # Get activations for this context\n",
    "                    _, cache = model.run_with_cache(tokens)\n",
    "                    \n",
    "                    # Extract activations from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "                        # Flatten across sequence positions\n",
    "                        flat_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
    "                        backtracking_activations.append((layer, flat_activations))\n",
    "    \n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, train a classifier to distinguish backtracking from non-backtracking\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Collect activations for this layer\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create dataset\n",
    "        X = np.vstack([layer_backtracking, layer_non_backtracking])\n",
    "        y = np.concatenate([\n",
    "            np.ones(len(layer_backtracking)),\n",
    "            np.zeros(len(layer_non_backtracking))\n",
    "        ])\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(X.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            neuron_activations = X[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(neuron_activations[:len(layer_backtracking)])\n",
    "            mean_non_backtracking = np.mean(neuron_activations[len(layer_backtracking):])\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(neuron_activations[:len(layer_backtracking)]) + \n",
    "                                 np.var(neuron_activations[len(layer_backtracking):])) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try:\n",
    "                auc = roc_auc_score(y, neuron_activations)\n",
    "            except:\n",
    "                auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], \n",
    "                                     key=lambda x: abs(x['effect_size']), \n",
    "                                     reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for neuron in neurons[:top_k]:\n",
    "            all_neurons.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron['neuron'],\n",
    "                'effect_size': neuron['effect_size'],\n",
    "                'auc': neuron['auc']\n",
    "            })\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'top_neurons': all_neurons[:top_k],\n",
    "        'layer_scores': neuron_scores\n",
    "    }\n",
    "\n",
    "def validate_backtracking_neurons(model, top_neurons, device, num_examples=10):\n",
    "    \"\"\"\n",
    "    Validate the identified backtracking neurons by testing them on new examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons identified\n",
    "        device: The device to run inference on\n",
    "        num_examples: Number of examples to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Function to sample problems from the dataset\n",
    "    def sample_math_problems(dataset, n=5, level=None, problem_type=None):\n",
    "        \"\"\"\n",
    "        Sample n problems from the dataset, optionally filtering by level or type.\n",
    "        \n",
    "        Args:\n",
    "            dataset: The MATH dataset\n",
    "            n: Number of problems to sample\n",
    "            level: Optional filter for problem difficulty (e.g., \"Level 1\")\n",
    "            problem_type: Optional filter for problem type (e.g., \"Algebra\")\n",
    "        \n",
    "        Returns:\n",
    "            List of sampled problems\n",
    "        \"\"\"\n",
    "        filtered_dataset = dataset['train']\n",
    "        \n",
    "        if level:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['level'] == level]\n",
    "        \n",
    "        if problem_type:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['type'] == problem_type]\n",
    "        \n",
    "        filtered_dataset = list(filtered_dataset)  # Convert to list to ensure it's a sequence\n",
    "        return random.sample(filtered_dataset, min(n, len(filtered_dataset)))\n",
    "\n",
    "    # Function to generate CoT using the model\n",
    "    def generate_cot_for_problem(\n",
    "        model: HookedTransformer, \n",
    "        problem: str, \n",
    "        temperature: float = 0.4, \n",
    "        max_new_tokens: int = 1500, \n",
    "        top_p: float = 0.92\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a chain-of-thought solution for a given math problem.\n",
    "        \n",
    "        Args:\n",
    "            model: The HookedTransformer model\n",
    "            problem: The math problem text\n",
    "            temperature: The temperature for the model\n",
    "            max_new_tokens: The maximum number of tokens to generate\n",
    "            top_p: The top-p value for the model\n",
    "        Returns:\n",
    "            The generated chain-of-thought solution\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem} Solution: \\n<think>\\n\"\"\"\n",
    "        result = model.generate(prompt, \n",
    "                                temperature=temperature,\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                top_p=top_p)\n",
    "        return result\n",
    "\n",
    "    # Load the MATH dataset for validation\n",
    "    math_dataset = load_dataset(\"fdyrd/math\")\n",
    "    validation_problems = sample_math_problems(math_dataset, n=num_examples)\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    for problem in tqdm(validation_problems, desc=\"Validating neurons\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate a solution with backtracking\n",
    "        solution = generate_cot_for_problem(model, problem_text)\n",
    "        \n",
    "        # Identify backtracking instances\n",
    "        backtracking_instances = identify_backtracking(solution)\n",
    "        \n",
    "        if not backtracking_instances or len(backtracking_instances) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For the first backtracking instance, analyze neuron activations\n",
    "        phrase = backtracking_instances[0]\n",
    "        phrase_lower = phrase.lower()\n",
    "        solution_lower = solution.lower()\n",
    "        start_idx = solution_lower.find(phrase_lower)\n",
    "                \n",
    "        if start_idx == -1:\n",
    "            continue\n",
    "        \n",
    "        end_idx = start_idx + len(phrase)\n",
    "        \n",
    "        # Extract context around the backtracking phrase\n",
    "        context_window = 50  # characters before and after\n",
    "        context_start = max(0, start_idx - context_window)\n",
    "        context_end = min(len(solution), end_idx + context_window)\n",
    "        context = solution[context_start:context_end]\n",
    "        \n",
    "        # Convert context to tokens\n",
    "        tokens = model.to_tokens(context)\n",
    "        \n",
    "        # Run with cache to get activations\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Check activation of top neurons\n",
    "        neuron_activations = []\n",
    "        \n",
    "        for neuron_info in top_neurons[:10]:  # Check top 10 neurons\n",
    "            layer = neuron_info['layer']\n",
    "            neuron = neuron_info['neuron']\n",
    "            \n",
    "            # Get activations for this layer\n",
    "            layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "            \n",
    "            # Get mean activation for this neuron across sequence positions\n",
    "            mean_activation = np.mean(layer_activations[0, :, neuron])\n",
    "            \n",
    "            neuron_activations.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron,\n",
    "                'activation': float(mean_activation),\n",
    "                'context': context,\n",
    "                'backtracking_phrase': phrase\n",
    "            })\n",
    "        \n",
    "        validation_results.append({\n",
    "            'problem': problem_text,\n",
    "            'solution': solution,\n",
    "            'backtracking_phrase': phrase,\n",
    "            'neuron_activations': neuron_activations\n",
    "        })\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def visualize_neuron_activations(model, neuron_info, examples, device):\n",
    "    \"\"\"\n",
    "    Visualize the activations of a specific neuron across different examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        neuron_info: Dictionary with neuron information (layer, index)\n",
    "        examples: List of text examples to analyze\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib figure with visualization\n",
    "    \"\"\"\n",
    "    layer = neuron_info['layer']\n",
    "    neuron = neuron_info['neuron']\n",
    "    \n",
    "    activations_by_example = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Get tokens\n",
    "        tokens = model.to_tokens(example)\n",
    "        str_tokens = model.to_str_tokens(example)\n",
    "        \n",
    "        # Run with cache\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Get activations for this layer and neuron\n",
    "        layer_activations = cache[\"post\", layer][0, :, neuron].detach().cpu().numpy()\n",
    "        \n",
    "        activations_by_example.append((str_tokens, layer_activations))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(len(examples), 1, figsize=(15, 4 * len(examples)))\n",
    "    if len(examples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (tokens, activations) in enumerate(activations_by_example):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot activations\n",
    "        ax.bar(range(len(activations)), activations)\n",
    "        \n",
    "        # Add token labels\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "        \n",
    "        # Highlight tokens with high activation\n",
    "        threshold = np.mean(activations) + np.std(activations)\n",
    "        for j, act in enumerate(activations):\n",
    "            if act > threshold:\n",
    "                ax.get_xticklabels()[j].set_color('red')\n",
    "                ax.get_xticklabels()[j].set_weight('bold')\n",
    "        \n",
    "        ax.set_title(f\"Example {i+1}: Neuron {neuron} in Layer {layer}\")\n",
    "        ax.set_ylabel(\"Activation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def ablate_neurons_and_test(model, top_neurons, test_problems, device):\n",
    "    \"\"\"\n",
    "    Ablate (zero out) the identified neurons and test the effect on backtracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons to ablate\n",
    "        test_problems: List of test problems\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ablation results\n",
    "    \"\"\"\n",
    "    # Define a hook function to ablate specific neurons\n",
    "    def ablation_hook(activations, hook, neurons_to_ablate):\n",
    "        # neurons_to_ablate is a list of (layer, neuron) tuples\n",
    "        for layer, neuron in neurons_to_ablate:\n",
    "            if hook.name == f\"blocks.{layer}.hook_post\":\n",
    "                activations[0, :, neuron] = 0.0\n",
    "        return activations\n",
    "    \n",
    "    # Prepare neurons to ablate\n",
    "    neurons_to_ablate = [(n['layer'], n['neuron']) for n in top_neurons[:20]]  # Ablate top 20\n",
    "    \n",
    "    ablation_results = {\n",
    "        'original': [],\n",
    "        'ablated': []\n",
    "    }\n",
    "    \n",
    "    for problem in tqdm(test_problems, desc=\"Testing ablation\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate solution without ablation\n",
    "        original_prompt = f\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem_text} Solution: \\n<think>\\n\"\n",
    "        original_solution = model.generate(original_prompt, \n",
    "                                         temperature=0.4,\n",
    "                                         max_new_tokens=500,\n",
    "                                         top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in original\n",
    "        original_backtracking = identify_backtracking_enhanced(original_solution)\n",
    "        \n",
    "        # Generate solution with ablation\n",
    "        ablated_solution = \"\"\n",
    "        \n",
    "        # Set up hooks for ablation\n",
    "        hooks = []\n",
    "        for layer in set(layer for layer, _ in neurons_to_ablate):\n",
    "            hook_name = f\"blocks.{layer}.hook_post\"\n",
    "            hook_fn = lambda act, hook=None, neurons=neurons_to_ablate: ablation_hook(act, hook, neurons)\n",
    "            hooks.append((hook_name, hook_fn))\n",
    "        \n",
    "        # Generate with hooks\n",
    "        with model.hooks(hooks):\n",
    "            ablated_solution = model.generate(original_prompt, \n",
    "                                            temperature=0.4,\n",
    "                                            max_new_tokens=500,\n",
    "                                            top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in ablated\n",
    "        ablated_backtracking = identify_backtracking_enhanced(ablated_solution)\n",
    "        \n",
    "        ablation_results['original'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': original_solution,\n",
    "            'backtracking_count': len(original_backtracking),\n",
    "            'backtracking_instances': original_backtracking\n",
    "        })\n",
    "        \n",
    "        ablation_results['ablated'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': ablated_solution,\n",
    "            'backtracking_count': len(ablated_backtracking),\n",
    "            'backtracking_instances': ablated_backtracking\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    original_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['original'])\n",
    "    ablated_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['ablated'])\n",
    "    \n",
    "    ablation_results['summary'] = {\n",
    "        'original_backtracking_total': original_backtracking_count,\n",
    "        'ablated_backtracking_total': ablated_backtracking_count,\n",
    "        'percent_change': ((ablated_backtracking_count - original_backtracking_count) / \n",
    "                          max(1, original_backtracking_count)) * 100\n",
    "    }\n",
    "    \n",
    "    return ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 41 examples to identify backtracking neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 2/41 [00:45<14:00, 21.54s/it]"
     ]
    }
   ],
   "source": [
    "# Identify neurons associated with backtracking\n",
    "neuron_analysis = identify_backtracking_neurons_improved(\n",
    "    model=model,\n",
    "    json_file_path=\"math_cot_results_t=0.4_mnt=1500_tp=0.92.json\",\n",
    "    device=device,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "# Print top neurons\n",
    "print(\"Top 10 neurons associated with backtracking:\")\n",
    "for i, neuron in enumerate(neuron_analysis['top_neurons'][:10]):\n",
    "    print(f\"{i+1}. Layer {neuron['layer']}, Neuron {neuron['neuron']}: Effect size = {neuron['effect_size']:.4f}, AUC = {neuron['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating neurons:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89685c09f6f44e0beee084270c72e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating neurons:   0%|          | 0/10 [02:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Validate the identified neurons on new examples\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m validation_results \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_backtracking_neurons\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_neurons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneuron_analysis\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtop_neurons\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print validation results\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidation results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 246\u001b[0m, in \u001b[0;36mvalidate_backtracking_neurons\u001b[0;34m(model, top_neurons, device, num_examples)\u001b[0m\n\u001b[1;32m    243\u001b[0m problem_text \u001b[38;5;241m=\u001b[39m problem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Generate a solution with backtracking\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m solution \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_cot_for_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Identify backtracking instances\u001b[39;00m\n\u001b[1;32m    249\u001b[0m backtracking_instances \u001b[38;5;241m=\u001b[39m identify_backtracking(solution)\n",
      "Cell \u001b[0;32mIn[67], line 230\u001b[0m, in \u001b[0;36mvalidate_backtracking_neurons.<locals>.generate_cot_for_problem\u001b[0;34m(model, problem, temperature, max_new_tokens, top_p)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03mGenerate a chain-of-thought solution for a given math problem.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    The generated chain-of-thought solution\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mSolve this math problem step by step. Put your final answer in \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mboxed\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m. Problem: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Solution: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<think>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 230\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformer_lens/HookedTransformer.py:2287\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_type \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   2284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2286\u001b[0m     ]:  \u001b[38;5;66;03m# Those types of inputs support frequency penalty\u001b[39;00m\n\u001b[0;32m-> 2287\u001b[0m         sampled_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfreq_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m                \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_tokens_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampled_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n\u001b[1;32m   2299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2300\u001b[0m         sampled_tokens \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39msample_logits(\n\u001b[1;32m   2301\u001b[0m             final_logits, top_k\u001b[38;5;241m=\u001b[39mtop_k, top_p\u001b[38;5;241m=\u001b[39mtop_p, temperature\u001b[38;5;241m=\u001b[39mtemperature\n\u001b[1;32m   2302\u001b[0m         )\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformer_lens/utils.py:437\u001b[0m, in \u001b[0;36msample_logits\u001b[0;34m(final_logits, top_k, top_p, temperature, freq_penalty, tokens)\u001b[0m\n\u001b[1;32m    434\u001b[0m     final_logits \u001b[38;5;241m=\u001b[39m final_logits\u001b[38;5;241m.\u001b[39mmasked_fill(indices_to_remove, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    436\u001b[0m final_logits \u001b[38;5;241m=\u001b[39m final_logits\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_logits\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/distributions/categorical.py:133\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    131\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    132\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 133\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Validate the identified neurons on new examples\n",
    "validation_results = validate_backtracking_neurons(\n",
    "    model=model,\n",
    "    top_neurons=neuron_analysis['top_neurons'],\n",
    "    device=device,\n",
    "    num_examples=10\n",
    ")\n",
    "\n",
    "# Print validation results\n",
    "print(\"\\nValidation results:\")\n",
    "for result in validation_results:\n",
    "    print(f\"Problem: {result['problem'][:100]}...\")\n",
    "    print(f\"Backtracking phrase: {result['backtracking_phrase']}\")\n",
    "    print(\"Top neuron activations:\")\n",
    "    for act in result['neuron_activations'][:3]:\n",
    "        print(f\"  Layer {act['layer']}, Neuron {act['neuron']}: {act['activation']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
