{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-02-27 14:36:02.829449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740666962.852245  678151 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740666962.859096  678151 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Got device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)\n",
    "model.cfg.n_ctx = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 Model context length: 2048\n",
      "🧠 Model layers: 28\n",
      "🔤 Vocabulary size: 151936\n",
      "📊 Hidden dimension: 1536\n",
      "🧩 Attention heads: 12\n",
      "🏷️ Model name: DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"📏 Model context length: {model.cfg.n_ctx}\")\n",
    "print(f\"🧠 Model layers: {model.cfg.n_layers}\")\n",
    "print(f\"🔤 Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"📊 Hidden dimension: {model.cfg.d_model}\")\n",
    "print(f\"🧩 Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"🏷️ Model name: {model.cfg.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 958 CoT solutions...\n",
      "\n",
      "==================================================\n",
      "CHAIN-OF-THOUGHT ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Total problems analyzed: 958\n",
      "\n",
      "1. CORRECTNESS\n",
      "Correct answers: 393 (41.02%)\n",
      "\n",
      "2. THINK TAGS\n",
      "Solutions with </think> close tags: 605 (63.15%)\n",
      "\n",
      "3. TOKEN LIMITS\n",
      "Problems that ran out of tokens: 429 (44.78%)\n",
      "\n",
      "4. BACKTRACKING\n",
      "Solutions with backtracking: 248 (25.89%)\n",
      "Sample of problems with backtracking:\n",
      "     Level: Level 5, Type: Precalculus, Phrases: let me think again\n",
      "     Level: Level 4, Type: Prealgebra, Phrases: let me think again\n",
      "     Level: Level 2, Type: Precalculus, Phrases: i made a mistake\n",
      "     Level: Level 2, Type: Algebra, Phrases: let me recalculate, let me recalculate\n",
      "     Level: Level 3, Type: Prealgebra, Phrases: i made a mistake, wait, that doesn't\n",
      "     Level: Level 4, Type: Algebra, Phrases: let me think again\n",
      "     Level: Level 5, Type: Number Theory, Phrases: let me think again\n",
      "     Level: Level 2, Type: Algebra, Phrases: wait, that doesn't\n",
      "\n",
      "5. PERFORMANCE BY LEVEL\n",
      "  Level 1: 78.21% correct (78 problems)\n",
      "  Level 2: 57.92% correct (183 problems)\n",
      "  Level 3: 46.31% correct (203 problems)\n",
      "  Level 4: 35.27% correct (224 problems)\n",
      "  Level 5: 19.63% correct (270 problems)\n",
      "\n",
      "6. PERFORMANCE BY TYPE\n",
      "  Algebra: 59.19% correct (223 problems)\n",
      "  Counting & Probability: 40.00% correct (110 problems)\n",
      "  Geometry: 24.42% correct (86 problems)\n",
      "  Intermediate Algebra: 20.37% correct (162 problems)\n",
      "  Number Theory: 40.34% correct (119 problems)\n",
      "  Prealgebra: 56.21% correct (153 problems)\n",
      "  Precalculus: 27.62% correct (105 problems)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# analysis = run_analysis(\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\")\n",
    "# analysis = run_analysis(\"math_cot_results_t=0.8_mnt=3600_tp=0.92.json\")\n",
    "analysis = run_analysis(\"math_cot_results_t=0.7_mnt=1800_tp=0.92.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_backtracking_neurons(model, json_file_path, min_sample_size=1000, top_k=50):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events by processing entire CoT solutions\n",
    "    and tracking activations at specific backtracking points.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        device: The device to run inference on\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"    \n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    non_backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(min_sample_size, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    for result in tqdm(sampled_results):\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        if not generated_cot: continue\n",
    "        \n",
    "        tokens = model.to_tokens(generated_cot)\n",
    "        str_tokens = model.to_str_tokens(generated_cot)\n",
    "        print(tokens)\n",
    "        print(str_tokens)\n",
    "        return None\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Find positions of backtracking phrases in the token sequence\n",
    "        backtracking_positions = []\n",
    "        for phrase in backtracking_phrases:\n",
    "            phrase_tokens = model.to_str_tokens(phrase)\n",
    "            \n",
    "            # Look for this phrase in the token sequence\n",
    "            for i in range(len(str_tokens[0]) - len(phrase_tokens) + 1):\n",
    "                # Check if this position contains the phrase\n",
    "                match = True\n",
    "                for j, token in enumerate(phrase_tokens):\n",
    "                    if i+j >= len(str_tokens[0]) or str_tokens[0][i+j].lower() != token.lower():\n",
    "                        match = False\n",
    "                        break\n",
    "                \n",
    "                if match:\n",
    "                    # Found a match, add the position range\n",
    "                    backtracking_positions.append((i, i + len(phrase_tokens)))\n",
    "        \n",
    "        # If no backtracking phrases found, sample random positions as non-backtracking\n",
    "        if not backtracking_positions:\n",
    "            # Sample random positions (avoiding the beginning and end)\n",
    "            if len(tokens[0]) > 20:\n",
    "                num_samples = min(5, len(tokens[0]) - 10)\n",
    "                for _ in range(num_samples):\n",
    "                    pos = random.randint(5, len(tokens[0]) - 5)\n",
    "                    # Extract activations for this position from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                        non_backtracking_activations.append((layer, layer_activations))\n",
    "        else:\n",
    "            # For each backtracking position, extract activations\n",
    "            for start_pos, end_pos in backtracking_positions:\n",
    "                # Get the position where backtracking starts\n",
    "                trigger_pos = start_pos\n",
    "                \n",
    "                # Extract activations at the trigger position from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer][0, trigger_pos].detach().cpu().numpy()\n",
    "                    backtracking_activations.append((layer, layer_activations))\n",
    "                \n",
    "                # Also sample non-backtracking positions from the same solution\n",
    "                # (avoiding positions close to backtracking phrases)\n",
    "                safe_positions = []\n",
    "                for pos in range(5, len(tokens[0]) - 5):\n",
    "                    # Check if this position is far from any backtracking phrase\n",
    "                    is_safe = True\n",
    "                    for bt_start, bt_end in backtracking_positions:\n",
    "                        if pos >= bt_start - 10 and pos <= bt_end + 10:\n",
    "                            is_safe = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_safe:\n",
    "                        safe_positions.append(pos)\n",
    "                \n",
    "                # Sample from safe positions\n",
    "                if safe_positions:\n",
    "                    num_samples = min(len(backtracking_positions), len(safe_positions))\n",
    "                    for pos in random.sample(safe_positions, num_samples):\n",
    "                        # Extract activations for this position from all layers\n",
    "                        for layer in range(model.cfg.n_layers):\n",
    "                            layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                            non_backtracking_activations.append((layer, layer_activations))\n",
    "    \n",
    "    print(f\"Found {len(backtracking_activations)} backtracking activations and {len(non_backtracking_activations)} non-backtracking activations\")\n",
    "    return None\n",
    "\n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, analyze neuron activations\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Collect activations for this layer\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(layer_backtracking.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            bt_activations = layer_backtracking[:, neuron_idx]\n",
    "            non_bt_activations = layer_non_backtracking[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(bt_activations)\n",
    "            mean_non_backtracking = np.mean(non_bt_activations)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(bt_activations) + np.var(non_bt_activations)) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Create dataset for AUC calculation\n",
    "            X = np.concatenate([bt_activations, non_bt_activations])\n",
    "            y = np.concatenate([np.ones(len(bt_activations)), np.zeros(len(non_bt_activations))])\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try: auc = roc_auc_score(y, X)\n",
    "            except: auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for n in neurons[:top_k]:\n",
    "            all_neurons.append({'layer': layer, 'neuron': n['neuron'], 'effect_size': n['effect_size'], 'auc': n['auc']})\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {'top_neurons': all_neurons[:top_k], 'layer_scores': neuron_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 996 examples to identify backtracking neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/996 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  50, 3948,  419,  ...,  532,   59,   60]], device='cuda:0')\n",
      "['S', 'olve', ' this', ' math', ' problem', ' step', ' by', ' step', '.', ' Put', ' your', ' final', ' answer', ' in', ' \\\\', 'boxed', '{}.', ' Problem', ':', ' Let', ' \\\\', '[', 'f', '(x', ')', ' =', ' \\\\', 'left', '\\\\', '{\\n', '\\\\', 'begin', '{', 'array', '}{', 'cl', '}\\n', 'x', '^', '2', '+', '1', ' &', '\\\\', 'text', '{', ' if', ' }', 'x', '>', '5', ',', ' \\\\\\\\\\n', '2', 'x', '-', '3', ' &', '\\\\', 'text', '{', ' if', ' }', ' -', '5', ' \\\\', 'le', ' x', ' \\\\', 'le', ' ', '5', ',', ' \\\\\\\\\\n', '3', ' &', '\\\\', 'text', '{', ' if', ' }', ' x', ' <-', '5', '.\\n', '\\\\', 'end', '{', 'array', '}\\n', '\\\\', 'right', '.\\\\', ']', 'Find', ' $', 'f', '(-', '7', ')+', 'f', '(', '0', ')+', 'f', '(', '7', ')', '$.', ' Solution', ':', ' \\n', '<think>', '\\n', 'Okay', ',', ' so', ' I', ' need', ' to', ' solve', ' this', ' math', ' problem', ' where', ' I', ' have', ' a', ' function', ' f', '(x', ')', ' defined', ' in', ' three', ' different', ' ways', ' depending', ' on', ' the', ' value', ' of', ' x', '.', ' The', ' problem', ' is', ' asking', ' me', ' to', ' find', ' the', ' sum', ' of', ' f', '(-', '7', '),', ' f', '(', '0', '),', ' and', ' f', '(', '7', ').', ' Let', ' me', ' write', ' down', ' the', ' function', ' again', ' to', ' make', ' sure', ' I', ' have', ' it', ' right', '.\\n\\n', 'So', ',', ' the', ' function', ' f', '(x', ')', ' is', ' defined', ' as', ':\\n', '-', ' If', ' x', ' is', ' greater', ' than', ' ', '5', ',', ' then', ' f', '(x', ')', ' is', ' x', ' squared', ' plus', ' ', '1', '.\\n', '-', ' If', ' x', ' is', ' between', ' -', '5', ' and', ' ', '5', ',', ' inclusive', ',', ' then', ' f', '(x', ')', ' is', ' ', '2', 'x', ' minus', ' ', '3', '.\\n', '-', ' If', ' x', ' is', ' less', ' than', ' -', '5', ',', ' then', ' f', '(x', ')', ' is', ' just', ' ', '3', '.\\n\\n', 'Alright', ',', ' so', ' I', ' need', ' to', ' compute', ' f', '(-', '7', '),', ' f', '(', '0', '),', ' and', ' f', '(', '7', ').', ' Let', ' me', ' tackle', ' each', ' one', ' step', ' by', ' step', '.\\n\\n', 'Starting', ' with', ' f', '(-', '7', ').', ' Since', ' -', '7', ' is', ' less', ' than', ' -', '5', ',', ' according', ' to', ' the', ' function', ' definition', ',', ' f', '(x', ')', ' in', ' this', ' case', ' is', ' just', ' ', '3', '.', ' So', ',', ' f', '(-', '7', ')', ' should', ' be', ' ', '3', '.', ' That', ' was', ' straightforward', '.\\n\\n', 'Next', ',', ' f', '(', '0', ').', ' Now', ',', ' ', '0', ' is', ' between', ' -', '5', ' and', ' ', '5', ',', ' inclusive', '.', ' So', ',', ' according', ' to', ' the', ' function', ',', ' we', ' use', ' the', ' middle', ' case', ',', ' which', ' is', ' ', '2', 'x', ' minus', ' ', '3', '.', ' Pl', 'ugging', ' in', ' ', '0', ',', ' that', ' becomes', ' ', '2', '*', '0', ' -', ' ', '3', ',', ' which', ' is', ' ', '0', ' -', ' ', '3', ',', ' so', ' that', \"'s\", ' -', '3', '.', ' So', ',', ' f', '(', '0', ')', ' is', ' -', '3', '.\\n\\n', 'Finally', ',', ' f', '(', '7', ').', ' ', '7', ' is', ' greater', ' than', ' ', '5', ',', ' so', ' we', ' use', ' the', ' first', ' case', ',', ' which', ' is', ' x', ' squared', ' plus', ' ', '1', '.', ' Pl', 'ugging', ' in', ' ', '7', ',', ' that', \"'s\", ' ', '7', ' squared', ' plus', ' ', '1', '.', ' ', '7', ' squared', ' is', ' ', '4', '9', ',', ' so', ' ', '4', '9', ' plus', ' ', '1', ' is', ' ', '5', '0', '.', ' Therefore', ',', ' f', '(', '7', ')', ' is', ' ', '5', '0', '.\\n\\n', 'Now', ',', ' I', ' need', ' to', ' add', ' up', ' these', ' three', ' results', ':', ' f', '(-', '7', ')', ' +', ' f', '(', '0', ')', ' +', ' f', '(', '7', ').', ' So', ',', ' that', \"'s\", ' ', '3', ' +', ' (-', '3', ')', ' +', ' ', '5', '0', '.', ' Let', \"'s\", ' compute', ' that', ' step', ' by', ' step', '.\\n\\n', 'First', ',', ' ', '3', ' plus', ' (-', '3', ')', ' is', ' ', '0', '.', ' Then', ',', ' ', '0', ' plus', ' ', '5', '0', ' is', ' ', '5', '0', '.', ' So', ',', ' the', ' total', ' sum', ' is', ' ', '5', '0', '.\\n\\n', 'Wait', ' a', ' second', ',', ' let', ' me', ' double', '-check', ' my', ' calculations', ' to', ' make', ' sure', ' I', ' didn', \"'t\", ' make', ' any', ' mistakes', '.', ' It', \"'s\", ' easy', ' to', ' mix', ' up', ' signs', ' or', ' m', 'iscal', 'culate', ' when', ' dealing', ' with', ' multiple', ' operations', '.\\n\\n', 'Starting', ' with', ' f', '(-', '7', '):', ' x', ' is', ' -', '7', ',', ' which', ' is', ' less', ' than', ' -', '5', ',', ' so', ' f', '(x', ')', ' is', ' ', '3', '.', ' That', ' seems', ' correct', '.\\n\\n', 'f', '(', '0', '):', ' x', ' is', ' ', '0', ',', ' which', ' is', ' between', ' -', '5', ' and', ' ', '5', ',', ' so', ' f', '(x', ')', ' is', ' ', '2', '*', '0', ' -', ' ', '3', ' =', ' -', '3', '.', ' Correct', '.\\n\\n', 'f', '(', '7', '):', ' x', ' is', ' ', '7', ',', ' which', ' is', ' greater', ' than', ' ', '5', ',', ' so', ' f', '(x', ')', ' is', ' ', '7', ' squared', ' plus', ' ', '1', ' =', ' ', '4', '9', ' +', ' ', '1', ' =', ' ', '5', '0', '.', ' Correct', '.\\n\\n', 'Adding', ' them', ' up', ':', ' ', '3', ' +', ' (-', '3', ')', ' +', ' ', '5', '0', '.', ' So', ',', ' ', '3', ' -', ' ', '3', ' is', ' ', '0', ',', ' then', ' ', '0', ' +', ' ', '5', '0', ' is', ' ', '5', '0', '.', ' That', ' seems', ' right', '.\\n\\n', 'Hmm', ',', ' just', ' to', ' make', ' sure', ',', ' maybe', ' I', ' should', ' write', ' out', ' each', ' step', ' more', ' formally', ' to', ' ensure', ' that', ' I', ' didn', \"'t\", ' overlook', ' anything', '.\\n\\n', 'For', ' f', '(-', '7', '):\\n', 'Since', ' -', '7', ' <', ' -', '5', ',', ' f', '(-', '7', ')', ' =', ' ', '3', '.\\n\\n', 'For', ' f', '(', '0', '):\\n', 'Since', ' -', '5', ' ≤', ' ', '0', ' ≤', ' ', '5', ',', ' f', '(', '0', ')', ' =', ' ', '2', '*', '0', ' -', ' ', '3', ' =', ' -', '3', '.\\n\\n', 'For', ' f', '(', '7', '):\\n', 'Since', ' ', '7', ' >', ' ', '5', ',', ' f', '(', '7', ')', ' =', ' ', '7', '²', ' +', ' ', '1', ' =', ' ', '4', '9', ' +', ' ', '1', ' =', ' ', '5', '0', '.\\n\\n', 'Now', ',', ' adding', ' them', ':\\n', '3', ' +', ' (-', '3', ')', ' +', ' ', '5', '0', ' =', ' (', '3', ' -', ' ', '3', ')', ' +', ' ', '5', '0', ' =', ' ', '0', ' +', ' ', '5', '0', ' =', ' ', '5', '0', '.\\n\\n', 'Yes', ',', ' that', ' seems', ' consistent', '.', ' I', ' don', \"'t\", ' think', ' I', ' made', ' any', ' errors', ' here', '.', ' It', \"'s\", ' always', ' good', ' to', ' verify', ' each', ' step', ',', ' especially', ' when', ' dealing', ' with', ' multiple', ' operations', ' and', ' different', ' cases', '.\\n\\n', 'So', ',', ' the', ' final', ' answer', ' should', ' be', ' ', '5', '0', ',', ' which', ' is', ' f', '(-', '7', ')', ' +', ' f', '(', '0', ')', ' +', ' f', '(', '7', ').\\n\\n', '**', 'Final', ' Answer', '**\\n', '\\\\', 'boxed', '{', '5', '0', '}\\n', '</think>', '\\n\\n', 'To', ' solve', ' the', ' problem', ',', ' we', ' need', ' to', ' evaluate', ' the', ' function', ' \\\\(', ' f', '(x', ')', ' \\\\', ')', ' at', ' \\\\(', ' x', ' =', ' -', '7', ' \\\\', '),', ' \\\\(', ' x', ' =', ' ', '0', ' \\\\', '),', ' and', ' \\\\(', ' x', ' =', ' ', '7', ' \\\\', '),', ' and', ' then', ' sum', ' these', ' values', '.\\n\\n', 'The', ' function', ' \\\\(', ' f', '(x', ')', ' \\\\', ')', ' is', ' defined', ' as', ':\\n', '\\\\', '[\\n', 'f', '(x', ')', ' =', ' \\\\', 'left', '\\\\', '{\\n', '\\\\', 'begin', '{', 'array', '}{', 'cl', '}\\n', 'x', '^', '2', ' +', ' ', '1', ' &', ' \\\\', 'text', '{', 'if', ' }', ' x', ' >', ' ', '5', ',', ' \\\\\\\\\\n', '2', 'x', ' -', ' ', '3', ' &', ' \\\\', 'text', '{', 'if', ' }', ' -', '5', ' \\\\', 'le', 'q', ' x', ' \\\\', 'le', 'q', ' ', '5', ',', ' \\\\\\\\\\n', '3', ' &', ' \\\\', 'text', '{', 'if', ' }', ' x', ' <', ' -', '5', '.\\n', '\\\\', 'end', '{', 'array', '}\\n', '\\\\', 'right', '.\\n', '\\\\', ']\\n\\n', '1', '.', ' Evalu', 'ating', ' \\\\(', ' f', '(-', '7', ')', ' \\\\', '):\\n', '  ', ' -', ' Since', ' \\\\(', ' -', '7', ' <', ' -', '5', ' \\\\', '),', ' we', ' use', ' the', ' third', ' case', '.\\n', '  ', ' -', ' \\\\(', ' f', '(-', '7', ')', ' =', ' ', '3', ' \\\\', ').\\n\\n', '2', '.', ' Evalu', 'ating', ' \\\\(', ' f', '(', '0', ')', ' \\\\', '):\\n', '  ', ' -', ' Since', ' \\\\(', ' -', '5', ' \\\\', 'le', 'q', ' ', '0', ' \\\\', 'le', 'q', ' ', '5', ' \\\\', '),', ' we', ' use', ' the', ' second', ' case', '.\\n', '  ', ' -', ' \\\\(', ' f', '(', '0', ')', ' =', ' ', '2', '(', '0', ')', ' -', ' ', '3', ' =', ' -', '3', ' \\\\', ').\\n\\n', '3', '.', ' Evalu', 'ating', ' \\\\(', ' f', '(', '7', ')', ' \\\\', '):\\n', '  ', ' -', ' Since', ' \\\\(', ' ', '7', ' >', ' ', '5', ' \\\\', '),', ' we', ' use', ' the', ' first', ' case', '.\\n', '  ', ' -', ' \\\\(', ' f', '(', '7', ')', ' =', ' ', '7', '^', '2', ' +', ' ', '1', ' =', ' ', '4', '9', ' +', ' ', '1', ' =', ' ', '5', '0', ' \\\\', ').\\n\\n', 'Next', ',', ' we', ' sum', ' these', ' values', ':\\n', '\\\\', '[\\n', 'f', '(-', '7', ')', ' +', ' f', '(', '0', ')', ' +', ' f', '(', '7', ')', ' =', ' ', '3', ' +', ' (-', '3', ')', ' +', ' ', '5', '0', ' =', ' ', '0', ' +', ' ', '5', '0', ' =', ' ', '5', '0', '.\\n', '\\\\', ']\\n\\n', 'Thus', ',', ' the', ' final', ' answer', ' is', ':\\n', '\\\\', '[\\n', '\\\\', 'boxed', '{', '5', '0', '}\\n', '\\\\', ']']\n",
      "Top 10 neurons associated with backtracking:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_675928/1747419595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print top neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top 10 neurons associated with backtracking:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron_analysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top_neurons'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i+1}. Layer {neuron['layer']}, Neuron {neuron['neuron']}: Effect size = {neuron['effect_size']:.4f}, AUC = {neuron['auc']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "neuron_analysis = identify_backtracking_neurons(\n",
    "    model=model,\n",
    "    json_file_path=\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\",\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "# Print top neurons\n",
    "print(\"Top 10 neurons associated with backtracking:\")\n",
    "for i, neuron in enumerate(neuron_analysis['top_neurons'][:10]):\n",
    "    print(f\"{i+1}. Layer {neuron['layer']}, Neuron {neuron['neuron']}: Effect size = {neuron['effect_size']:.4f}, AUC = {neuron['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_backtracking_neurons(model, json_file_path, device, top_k=50):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        device: The device to run inference on\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"\n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []\n",
    "    non_backtracking_activations = []\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(100, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    for result in tqdm(sampled_results):\n",
    "        problem_text = result.get(\"problem_text\", \"\")\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        \n",
    "        # Skip if the generated CoT is empty\n",
    "        if not generated_cot:\n",
    "            continue\n",
    "        \n",
    "        # Identify backtracking phrases with context\n",
    "        backtracking_instances = identify_backtracking(generated_cot)\n",
    "        \n",
    "        # If no backtracking, use this as a control example\n",
    "        if not backtracking_instances or len(backtracking_instances) == 0:\n",
    "            # Get a random segment from the CoT\n",
    "            tokens = model.to_tokens(generated_cot)\n",
    "            if len(tokens[0]) > 20:  # Ensure we have enough tokens\n",
    "                # Take a random segment with context window similar to backtracking examples\n",
    "                context_window = 50  # characters before and after, matching the backtracking case\n",
    "                \n",
    "                # Convert to text indices for consistency with backtracking case\n",
    "                text_length = len(generated_cot)\n",
    "                if text_length > context_window * 2:\n",
    "                    # Pick a random center point\n",
    "                    center_idx = random.randint(context_window, text_length - context_window)\n",
    "                    context_start = center_idx - context_window\n",
    "                    context_end = center_idx + context_window\n",
    "                    context = generated_cot[context_start:context_end]\n",
    "                else:\n",
    "                    # If text is too short, use the whole text\n",
    "                    context = generated_cot\n",
    "                \n",
    "                # Get activations for this context\n",
    "                tokens = model.to_tokens(context)\n",
    "                _, cache = model.run_with_cache(tokens)\n",
    "                \n",
    "                # Extract activations from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "                    # Flatten across sequence positions\n",
    "                    flat_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
    "                    non_backtracking_activations.append((layer, flat_activations))\n",
    "        else:\n",
    "            # For each backtracking instance, get the surrounding context\n",
    "            for phrase in backtracking_instances:\n",
    "                # Locate the phrase in the generated CoT (case insensitive)\n",
    "                phrase_lower = phrase.lower()\n",
    "                generated_cot_lower = generated_cot.lower()\n",
    "                start_idx = generated_cot_lower.find(phrase_lower)\n",
    "                \n",
    "                if start_idx != -1:\n",
    "                    end_idx = start_idx + len(phrase)\n",
    "                    \n",
    "                    # Extract context around the backtracking phrase\n",
    "                    context_window = 50  # characters before and after\n",
    "                    context_start = max(0, start_idx - context_window)\n",
    "                    context_end = min(len(generated_cot), end_idx + context_window)\n",
    "                    context = generated_cot[context_start:context_end]\n",
    "                    \n",
    "                    # Convert context to tokens\n",
    "                    tokens = model.to_tokens(context)\n",
    "                    \n",
    "                    # Get activations for this context\n",
    "                    _, cache = model.run_with_cache(tokens)\n",
    "                    \n",
    "                    # Extract activations from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "                        # Flatten across sequence positions\n",
    "                        flat_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
    "                        backtracking_activations.append((layer, flat_activations))\n",
    "    \n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, train a classifier to distinguish backtracking from non-backtracking\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Collect activations for this layer\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create dataset\n",
    "        X = np.vstack([layer_backtracking, layer_non_backtracking])\n",
    "        y = np.concatenate([\n",
    "            np.ones(len(layer_backtracking)),\n",
    "            np.zeros(len(layer_non_backtracking))\n",
    "        ])\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(X.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            neuron_activations = X[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(neuron_activations[:len(layer_backtracking)])\n",
    "            mean_non_backtracking = np.mean(neuron_activations[len(layer_backtracking):])\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(neuron_activations[:len(layer_backtracking)]) + \n",
    "                                 np.var(neuron_activations[len(layer_backtracking):])) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try:\n",
    "                auc = roc_auc_score(y, neuron_activations)\n",
    "            except:\n",
    "                auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], \n",
    "                                     key=lambda x: abs(x['effect_size']), \n",
    "                                     reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for neuron in neurons[:top_k]:\n",
    "            all_neurons.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron['neuron'],\n",
    "                'effect_size': neuron['effect_size'],\n",
    "                'auc': neuron['auc']\n",
    "            })\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'top_neurons': all_neurons[:top_k],\n",
    "        'layer_scores': neuron_scores\n",
    "    }\n",
    "\n",
    "def validate_backtracking_neurons(model, top_neurons, device, num_examples=10):\n",
    "    \"\"\"\n",
    "    Validate the identified backtracking neurons by testing them on new examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons identified\n",
    "        device: The device to run inference on\n",
    "        num_examples: Number of examples to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Function to sample problems from the dataset\n",
    "    def sample_math_problems(dataset, n=5, level=None, problem_type=None):\n",
    "        \"\"\"\n",
    "        Sample n problems from the dataset, optionally filtering by level or type.\n",
    "        \n",
    "        Args:\n",
    "            dataset: The MATH dataset\n",
    "            n: Number of problems to sample\n",
    "            level: Optional filter for problem difficulty (e.g., \"Level 1\")\n",
    "            problem_type: Optional filter for problem type (e.g., \"Algebra\")\n",
    "        \n",
    "        Returns:\n",
    "            List of sampled problems\n",
    "        \"\"\"\n",
    "        filtered_dataset = dataset['train']\n",
    "        \n",
    "        if level:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['level'] == level]\n",
    "        \n",
    "        if problem_type:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['type'] == problem_type]\n",
    "        \n",
    "        filtered_dataset = list(filtered_dataset)  # Convert to list to ensure it's a sequence\n",
    "        return random.sample(filtered_dataset, min(n, len(filtered_dataset)))\n",
    "\n",
    "    # Function to generate CoT using the model\n",
    "    def generate_cot_for_problem(\n",
    "        model: HookedTransformer, \n",
    "        problem: str, \n",
    "        temperature: float = 0.4, \n",
    "        max_new_tokens: int = 1500, \n",
    "        top_p: float = 0.92\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a chain-of-thought solution for a given math problem.\n",
    "        \n",
    "        Args:\n",
    "            model: The HookedTransformer model\n",
    "            problem: The math problem text\n",
    "            temperature: The temperature for the model\n",
    "            max_new_tokens: The maximum number of tokens to generate\n",
    "            top_p: The top-p value for the model\n",
    "        Returns:\n",
    "            The generated chain-of-thought solution\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem} Solution: \\n<think>\\n\"\"\"\n",
    "        result = model.generate(prompt, \n",
    "                                temperature=temperature,\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                top_p=top_p)\n",
    "        return result\n",
    "\n",
    "    # Load the MATH dataset for validation\n",
    "    math_dataset = load_dataset(\"fdyrd/math\")\n",
    "    validation_problems = sample_math_problems(math_dataset, n=num_examples)\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    for problem in tqdm(validation_problems, desc=\"Validating neurons\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate a solution with backtracking\n",
    "        solution = generate_cot_for_problem(model, problem_text)\n",
    "        \n",
    "        # Identify backtracking instances\n",
    "        backtracking_instances = identify_backtracking(solution)\n",
    "        \n",
    "        if not backtracking_instances or len(backtracking_instances) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For the first backtracking instance, analyze neuron activations\n",
    "        phrase = backtracking_instances[0]\n",
    "        phrase_lower = phrase.lower()\n",
    "        solution_lower = solution.lower()\n",
    "        start_idx = solution_lower.find(phrase_lower)\n",
    "                \n",
    "        if start_idx == -1:\n",
    "            continue\n",
    "        \n",
    "        end_idx = start_idx + len(phrase)\n",
    "        \n",
    "        # Extract context around the backtracking phrase\n",
    "        context_window = 50  # characters before and after\n",
    "        context_start = max(0, start_idx - context_window)\n",
    "        context_end = min(len(solution), end_idx + context_window)\n",
    "        context = solution[context_start:context_end]\n",
    "        \n",
    "        # Convert context to tokens\n",
    "        tokens = model.to_tokens(context)\n",
    "        \n",
    "        # Run with cache to get activations\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Check activation of top neurons\n",
    "        neuron_activations = []\n",
    "        \n",
    "        for neuron_info in top_neurons[:10]:  # Check top 10 neurons\n",
    "            layer = neuron_info['layer']\n",
    "            neuron = neuron_info['neuron']\n",
    "            \n",
    "            # Get activations for this layer\n",
    "            layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "            \n",
    "            # Get mean activation for this neuron across sequence positions\n",
    "            mean_activation = np.mean(layer_activations[0, :, neuron])\n",
    "            \n",
    "            neuron_activations.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron,\n",
    "                'activation': float(mean_activation),\n",
    "                'context': context,\n",
    "                'backtracking_phrase': phrase\n",
    "            })\n",
    "        \n",
    "        validation_results.append({\n",
    "            'problem': problem_text,\n",
    "            'solution': solution,\n",
    "            'backtracking_phrase': phrase,\n",
    "            'neuron_activations': neuron_activations\n",
    "        })\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def visualize_neuron_activations(model, neuron_info, examples, device):\n",
    "    \"\"\"\n",
    "    Visualize the activations of a specific neuron across different examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        neuron_info: Dictionary with neuron information (layer, index)\n",
    "        examples: List of text examples to analyze\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib figure with visualization\n",
    "    \"\"\"\n",
    "    layer = neuron_info['layer']\n",
    "    neuron = neuron_info['neuron']\n",
    "    \n",
    "    activations_by_example = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Get tokens\n",
    "        tokens = model.to_tokens(example)\n",
    "        str_tokens = model.to_str_tokens(example)\n",
    "        \n",
    "        # Run with cache\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Get activations for this layer and neuron\n",
    "        layer_activations = cache[\"post\", layer][0, :, neuron].detach().cpu().numpy()\n",
    "        \n",
    "        activations_by_example.append((str_tokens, layer_activations))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(len(examples), 1, figsize=(15, 4 * len(examples)))\n",
    "    if len(examples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (tokens, activations) in enumerate(activations_by_example):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot activations\n",
    "        ax.bar(range(len(activations)), activations)\n",
    "        \n",
    "        # Add token labels\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "        \n",
    "        # Highlight tokens with high activation\n",
    "        threshold = np.mean(activations) + np.std(activations)\n",
    "        for j, act in enumerate(activations):\n",
    "            if act > threshold:\n",
    "                ax.get_xticklabels()[j].set_color('red')\n",
    "                ax.get_xticklabels()[j].set_weight('bold')\n",
    "        \n",
    "        ax.set_title(f\"Example {i+1}: Neuron {neuron} in Layer {layer}\")\n",
    "        ax.set_ylabel(\"Activation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def ablate_neurons_and_test(model, top_neurons, test_problems, device):\n",
    "    \"\"\"\n",
    "    Ablate (zero out) the identified neurons and test the effect on backtracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons to ablate\n",
    "        test_problems: List of test problems\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ablation results\n",
    "    \"\"\"\n",
    "    # Define a hook function to ablate specific neurons\n",
    "    def ablation_hook(activations, hook, neurons_to_ablate):\n",
    "        # neurons_to_ablate is a list of (layer, neuron) tuples\n",
    "        for layer, neuron in neurons_to_ablate:\n",
    "            if hook.name == f\"blocks.{layer}.hook_post\":\n",
    "                activations[0, :, neuron] = 0.0\n",
    "        return activations\n",
    "    \n",
    "    # Prepare neurons to ablate\n",
    "    neurons_to_ablate = [(n['layer'], n['neuron']) for n in top_neurons[:20]]  # Ablate top 20\n",
    "    \n",
    "    ablation_results = {\n",
    "        'original': [],\n",
    "        'ablated': []\n",
    "    }\n",
    "    \n",
    "    for problem in tqdm(test_problems, desc=\"Testing ablation\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate solution without ablation\n",
    "        original_prompt = f\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem_text} Solution: \\n<think>\\n\"\n",
    "        original_solution = model.generate(original_prompt, \n",
    "                                         temperature=0.4,\n",
    "                                         max_new_tokens=500,\n",
    "                                         top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in original\n",
    "        original_backtracking = identify_backtracking_enhanced(original_solution)\n",
    "        \n",
    "        # Generate solution with ablation\n",
    "        ablated_solution = \"\"\n",
    "        \n",
    "        # Set up hooks for ablation\n",
    "        hooks = []\n",
    "        for layer in set(layer for layer, _ in neurons_to_ablate):\n",
    "            hook_name = f\"blocks.{layer}.hook_post\"\n",
    "            hook_fn = lambda act, hook=None, neurons=neurons_to_ablate: ablation_hook(act, hook, neurons)\n",
    "            hooks.append((hook_name, hook_fn))\n",
    "        \n",
    "        # Generate with hooks\n",
    "        with model.hooks(hooks):\n",
    "            ablated_solution = model.generate(original_prompt, \n",
    "                                            temperature=0.4,\n",
    "                                            max_new_tokens=500,\n",
    "                                            top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in ablated\n",
    "        ablated_backtracking = identify_backtracking_enhanced(ablated_solution)\n",
    "        \n",
    "        ablation_results['original'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': original_solution,\n",
    "            'backtracking_count': len(original_backtracking),\n",
    "            'backtracking_instances': original_backtracking\n",
    "        })\n",
    "        \n",
    "        ablation_results['ablated'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': ablated_solution,\n",
    "            'backtracking_count': len(ablated_backtracking),\n",
    "            'backtracking_instances': ablated_backtracking\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    original_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['original'])\n",
    "    ablated_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['ablated'])\n",
    "    \n",
    "    ablation_results['summary'] = {\n",
    "        'original_backtracking_total': original_backtracking_count,\n",
    "        'ablated_backtracking_total': ablated_backtracking_count,\n",
    "        'percent_change': ((ablated_backtracking_count - original_backtracking_count) / \n",
    "                          max(1, original_backtracking_count)) * 100\n",
    "    }\n",
    "    \n",
    "    return ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the identified neurons on new examples\n",
    "validation_results = validate_backtracking_neurons(\n",
    "    model=model,\n",
    "    top_neurons=neuron_analysis['top_neurons'],\n",
    "    device=device,\n",
    "    num_examples=10\n",
    ")\n",
    "\n",
    "# Print validation results\n",
    "print(\"\\nValidation results:\")\n",
    "for result in validation_results:\n",
    "    print(f\"Problem: {result['problem'][:100]}...\")\n",
    "    print(f\"Backtracking phrase: {result['backtracking_phrase']}\")\n",
    "    print(\"Top neuron activations:\")\n",
    "    for act in result['neuron_activations'][:3]:\n",
    "        print(f\"  Layer {act['layer']}, Neuron {act['neuron']}: {act['activation']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
