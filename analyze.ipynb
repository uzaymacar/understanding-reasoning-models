{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-02-27 18:15:54.150616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740680154.173659  760082 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740680154.180666  760082 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Got device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)\n",
    "model.cfg.n_ctx = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Model context length: 2048\n",
      "ðŸ§  Model layers: 28\n",
      "ðŸ”¤ Vocabulary size: 151936\n",
      "ðŸ“Š Hidden dimension: 1536\n",
      "ðŸ§© Attention heads: 12\n",
      "ðŸ·ï¸ Model name: DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ“ Model context length: {model.cfg.n_ctx}\")\n",
    "print(f\"ðŸ§  Model layers: {model.cfg.n_layers}\")\n",
    "print(f\"ðŸ”¤ Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"ðŸ“Š Hidden dimension: {model.cfg.d_model}\")\n",
    "print(f\"ðŸ§© Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"ðŸ·ï¸ Model name: {model.cfg.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 268 CoT solutions...\n",
      "\n",
      "==================================================\n",
      "CHAIN-OF-THOUGHT ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Total problems analyzed: 268\n",
      "\n",
      "1. CORRECTNESS\n",
      "Correct answers: 136 (50.75%)\n",
      "\n",
      "2. THINK TAGS\n",
      "Solutions with </think> close tags: 195 (72.76%)\n",
      "\n",
      "3. TOKEN LIMITS\n",
      "Problems that ran out of tokens: 86 (32.09%)\n",
      "\n",
      "4. BACKTRACKING\n",
      "Solutions with backtracking: 104 (38.81%)\n",
      "Sample of problems with backtracking:\n",
      "     Level: Level 3, Type: Algebra, Phrases: i made a mistake, let me think again\n",
      "     Level: Level 4, Type: Number Theory, Phrases: i made a mistake\n",
      "     Level: Level 3, Type: Intermediate Algebra, Phrases: i made a mistake\n",
      "     Level: Level 3, Type: Number Theory, Phrases: i made a mistake, that's not right, wait, that's not\n",
      "     Level: Level 1, Type: Geometry, Phrases: wait, this doesn't\n",
      "     Level: Level 5, Type: Algebra, Phrases: i made a mistake\n",
      "     Level: Level 4, Type: Number Theory, Phrases: let me think again\n",
      "     Level: Level 4, Type: Intermediate Algebra, Phrases: let me think again\n",
      "\n",
      "5. PERFORMANCE BY LEVEL\n",
      "  Level 1: 76.19% correct (21 problems)\n",
      "  Level 2: 67.39% correct (46 problems)\n",
      "  Level 3: 64.81% correct (54 problems)\n",
      "  Level 4: 51.72% correct (58 problems)\n",
      "  Level 5: 26.97% correct (89 problems)\n",
      "\n",
      "6. PERFORMANCE BY TYPE\n",
      "  Algebra: 75.81% correct (62 problems)\n",
      "  Counting & Probability: 45.45% correct (33 problems)\n",
      "  Geometry: 23.08% correct (26 problems)\n",
      "  Intermediate Algebra: 27.91% correct (43 problems)\n",
      "  Number Theory: 54.84% correct (31 problems)\n",
      "  Prealgebra: 65.96% correct (47 problems)\n",
      "  Precalculus: 30.77% correct (26 problems)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# analysis = run_analysis(\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\")\n",
    "# analysis = run_analysis(\"math_cot_results_t=0.7_mnt=1800_tp=0.92.json\")\n",
    "analysis = run_analysis(\"math_cot_results_t=0.8_mnt=3600_tp=0.92.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from math_cot_results_t=0.6_mnt=1500_tp=0.92.json...\n",
      "Loading data from math_cot_results_t=0.7_mnt=1800_tp=0.92.json...\n",
      "Loading data from math_cot_results_t=0.8_mnt=3600_tp=0.92.json...\n",
      "Loaded 2222 total problems from all files\n",
      "Found 1174 completed problems\n",
      "Found 106 problems with backtracking that were solved correctly\n",
      "Found 1019 problems without backtracking\n",
      "Saved balanced dataset with 100 samples to backtracking_dataset_n=100.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_samples': 100,\n",
       " 'backtracking_samples': 50,\n",
       " 'no_backtracking_samples': 50,\n",
       " 'unique_backtracking_ids': 50,\n",
       " 'unique_no_backtracking_ids': 50,\n",
       " 'original_backtracking_correct_count': 106,\n",
       " 'original_no_backtracking_count': 1019,\n",
       " 'total_problems_processed': 2222}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_balanced_backtracking_dataset(\n",
    "    json_file_paths=[\n",
    "        \"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\", \n",
    "        \"math_cot_results_t=0.7_mnt=1800_tp=0.92.json\", \n",
    "        \"math_cot_results_t=0.8_mnt=3600_tp=0.92.json\", \n",
    "    ],\n",
    "    output_path=\"backtracking_dataset_n=100.json\",\n",
    "    n=100,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_backtracking_neurons(model, json_file_path, min_sample_size=100, top_k=50):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events by processing entire CoT solutions\n",
    "    and tracking activations at specific backtracking points.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        device: The device to run inference on\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"    \n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    non_backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(min_sample_size, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    for result in tqdm(sampled_results, desc=\"Processing examples\"):\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        if not generated_cot: continue\n",
    "        \n",
    "        tokens = model.to_tokens(generated_cot.lower())\n",
    "        str_tokens = model.to_str_tokens(generated_cot.lower())\n",
    "        print(len(str_tokens))\n",
    "        continue\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Find positions of backtracking phrases in the token sequence\n",
    "        backtracking_positions = []\n",
    "        for phrase in backtracking_phrases:\n",
    "            phrase_tokens = model.to_str_tokens(phrase.lower())\n",
    "            \n",
    "            # Look for this phrase in the token sequence\n",
    "            for i in range(len(str_tokens) - len(phrase_tokens) + 1):\n",
    "                match = True\n",
    "                for j in range(len(phrase_tokens)):\n",
    "                    if i+j >= len(str_tokens) or str_tokens[i+j].strip().lower() != phrase_tokens[j].strip().lower():\n",
    "                        match = False\n",
    "                        break\n",
    "                \n",
    "                if match:\n",
    "                    backtracking_positions.append((i, i + len(phrase_tokens)))\n",
    "        \n",
    "        # If no backtracking phrases found, sample random positions as non-backtracking\n",
    "        if not backtracking_positions:\n",
    "            # Sample random positions (avoiding the beginning and end)\n",
    "            if len(tokens[0]) > 20:\n",
    "                num_samples = min(5, len(tokens[0]) - 10)\n",
    "                for _ in range(num_samples):\n",
    "                    pos = random.randint(5, len(tokens[0]) - 5)\n",
    "                    # Extract activations for this position from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                        non_backtracking_activations.append((layer, layer_activations))\n",
    "        else:\n",
    "            # For each backtracking position, extract activations\n",
    "            for start_pos, end_pos in backtracking_positions:\n",
    "                # Get the position where backtracking starts\n",
    "                trigger_pos = start_pos\n",
    "                \n",
    "                # Extract activations at the trigger position from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer][0, trigger_pos].detach().cpu().numpy()\n",
    "                    backtracking_activations.append((layer, layer_activations))\n",
    "                \n",
    "                # Also sample non-backtracking positions from the same solution\n",
    "                # (avoiding positions close to backtracking phrases)\n",
    "                safe_positions = []\n",
    "                for pos in range(5, len(tokens[0]) - 5):\n",
    "                    # Check if this position is far from any backtracking phrase\n",
    "                    is_safe = True\n",
    "                    for bt_start, bt_end in backtracking_positions:\n",
    "                        if pos >= bt_start - 10 and pos <= bt_end + 10:\n",
    "                            is_safe = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_safe:\n",
    "                        safe_positions.append(pos)\n",
    "                \n",
    "                # Sample from safe positions\n",
    "                if safe_positions:\n",
    "                    num_samples = min(len(backtracking_positions), len(safe_positions))\n",
    "                    for pos in random.sample(safe_positions, num_samples):\n",
    "                        # Extract activations for this position from all layers\n",
    "                        for layer in range(model.cfg.n_layers):\n",
    "                            layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                            non_backtracking_activations.append((layer, layer_activations))\n",
    "\n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, analyze neuron activations\n",
    "    for layer in tqdm(range(model.cfg.n_layers), desc=\"Analyzing layers\"):\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0: continue\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(layer_backtracking.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            bt_activations = layer_backtracking[:, neuron_idx]\n",
    "            non_bt_activations = layer_non_backtracking[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(bt_activations)\n",
    "            mean_non_backtracking = np.mean(non_bt_activations)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(bt_activations) + np.var(non_bt_activations)) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Create dataset for AUC calculation\n",
    "            X = np.concatenate([bt_activations, non_bt_activations])\n",
    "            y = np.concatenate([np.ones(len(bt_activations)), np.zeros(len(non_bt_activations))])\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try: auc = roc_auc_score(y, X)\n",
    "            except: auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for n in neurons[:top_k]:\n",
    "            all_neurons.append({'layer': layer, 'neuron': n['neuron'], 'effect_size': n['effect_size'], 'auc': n['auc']})\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {'top_neurons': all_neurons[:top_k], 'layer_scores': neuron_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20 examples to identify backtracking neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:00<00:00, 52.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n",
      "363\n",
      "262\n",
      "229\n",
      "467\n",
      "534\n",
      "469\n",
      "231\n",
      "622\n",
      "337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:00<00:00, 54.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n",
      "382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 55.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617\n",
      "587\n",
      "324\n",
      "335\n",
      "288\n",
      "291\n",
      "330\n",
      "429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing layers:   0%|          | 0/28 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_760082/270533937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m neuron_analysis = identify_backtracking_neurons(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjson_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"toy_dataset_n=20.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_760082/2058656786.py\u001b[0m in \u001b[0;36midentify_backtracking_neurons\u001b[0;34m(model, json_file_path, min_sample_size, top_k)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# For each layer, analyze neuron activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Analyzing layers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mlayer_backtracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbacktracking_activations\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mlayer_non_backtracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_backtracking_activations\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "neuron_analysis = identify_backtracking_neurons(\n",
    "    model=model,\n",
    "    json_file_path=\"toy_dataset_n=20.json\",\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "# Print top neurons\n",
    "print(\"Top 10 neurons associated with backtracking:\")\n",
    "for i, neuron in enumerate(neuron_analysis['top_neurons'][:10]):\n",
    "    print(f\"{i+1}. Layer {neuron['layer']}, Neuron {neuron['neuron']}: Effect size = {neuron['effect_size']:.4f}, AUC = {neuron['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_backtracking_neurons(model, top_neurons, num_examples=10):\n",
    "    \"\"\"\n",
    "    Validate the identified backtracking neurons by testing them on new examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons identified\n",
    "        device: The device to run inference on\n",
    "        num_examples: Number of examples to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Function to sample problems from the dataset\n",
    "    def sample_math_problems(dataset, n=5, level=None, problem_type=None):\n",
    "        \"\"\"\n",
    "        Sample n problems from the dataset, optionally filtering by level or type.\n",
    "        \n",
    "        Args:\n",
    "            dataset: The MATH dataset\n",
    "            n: Number of problems to sample\n",
    "            level: Optional filter for problem difficulty (e.g., \"Level 1\")\n",
    "            problem_type: Optional filter for problem type (e.g., \"Algebra\")\n",
    "        \n",
    "        Returns:\n",
    "            List of sampled problems\n",
    "        \"\"\"\n",
    "        filtered_dataset = dataset['train']\n",
    "        \n",
    "        if level:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['level'] == level]\n",
    "        \n",
    "        if problem_type:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['type'] == problem_type]\n",
    "        \n",
    "        filtered_dataset = list(filtered_dataset)  # Convert to list to ensure it's a sequence\n",
    "        return random.sample(filtered_dataset, min(n, len(filtered_dataset)))\n",
    "\n",
    "    # Function to generate CoT using the model\n",
    "    def generate_cot_for_problem(\n",
    "        model: HookedTransformer, \n",
    "        problem: str, \n",
    "        temperature: float = 0.4, \n",
    "        max_new_tokens: int = 1500, \n",
    "        top_p: float = 0.92\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a chain-of-thought solution for a given math problem.\n",
    "        \n",
    "        Args:\n",
    "            model: The HookedTransformer model\n",
    "            problem: The math problem text\n",
    "            temperature: The temperature for the model\n",
    "            max_new_tokens: The maximum number of tokens to generate\n",
    "            top_p: The top-p value for the model\n",
    "        Returns:\n",
    "            The generated chain-of-thought solution\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem} Solution: \\n<think>\\n\"\"\"\n",
    "        result = model.generate(prompt, \n",
    "                                temperature=temperature,\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                top_p=top_p)\n",
    "        return result\n",
    "\n",
    "    # Load the MATH dataset for validation\n",
    "    math_dataset = load_dataset(\"fdyrd/math\")\n",
    "    validation_problems = sample_math_problems(math_dataset, n=num_examples)\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    for problem in tqdm(validation_problems, desc=\"Validating neurons\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate a solution with backtracking\n",
    "        solution = generate_cot_for_problem(model, problem_text)\n",
    "        \n",
    "        # Identify backtracking instances\n",
    "        backtracking_instances = identify_backtracking(solution)\n",
    "        \n",
    "        if not backtracking_instances or len(backtracking_instances) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For the first backtracking instance, analyze neuron activations\n",
    "        phrase = backtracking_instances[0]\n",
    "        phrase_lower = phrase.lower()\n",
    "        solution_lower = solution.lower()\n",
    "        start_idx = solution_lower.find(phrase_lower)\n",
    "                \n",
    "        if start_idx == -1:\n",
    "            continue\n",
    "        \n",
    "        end_idx = start_idx + len(phrase)\n",
    "        \n",
    "        # Extract context around the backtracking phrase\n",
    "        context_window = 50  # characters before and after\n",
    "        context_start = max(0, start_idx - context_window)\n",
    "        context_end = min(len(solution), end_idx + context_window)\n",
    "        context = solution[context_start:context_end]\n",
    "        \n",
    "        # Convert context to tokens\n",
    "        tokens = model.to_tokens(context)\n",
    "        \n",
    "        # Run with cache to get activations\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Check activation of top neurons\n",
    "        neuron_activations = []\n",
    "        \n",
    "        for neuron_info in top_neurons[:10]:  # Check top 10 neurons\n",
    "            layer = neuron_info['layer']\n",
    "            neuron = neuron_info['neuron']\n",
    "            \n",
    "            # Get activations for this layer\n",
    "            layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "            \n",
    "            # Get mean activation for this neuron across sequence positions\n",
    "            mean_activation = np.mean(layer_activations[0, :, neuron])\n",
    "            \n",
    "            neuron_activations.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron,\n",
    "                'activation': float(mean_activation),\n",
    "                'context': context,\n",
    "                'backtracking_phrase': phrase\n",
    "            })\n",
    "        \n",
    "        validation_results.append({\n",
    "            'problem': problem_text,\n",
    "            'solution': solution,\n",
    "            'backtracking_phrase': phrase,\n",
    "            'neuron_activations': neuron_activations\n",
    "        })\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def visualize_neuron_activations(model, neuron_info, examples, device):\n",
    "    \"\"\"\n",
    "    Visualize the activations of a specific neuron across different examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        neuron_info: Dictionary with neuron information (layer, index)\n",
    "        examples: List of text examples to analyze\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib figure with visualization\n",
    "    \"\"\"\n",
    "    layer = neuron_info['layer']\n",
    "    neuron = neuron_info['neuron']\n",
    "    \n",
    "    activations_by_example = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Get tokens\n",
    "        tokens = model.to_tokens(example)\n",
    "        str_tokens = model.to_str_tokens(example)\n",
    "        \n",
    "        # Run with cache\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Get activations for this layer and neuron\n",
    "        layer_activations = cache[\"post\", layer][0, :, neuron].detach().cpu().numpy()\n",
    "        \n",
    "        activations_by_example.append((str_tokens, layer_activations))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(len(examples), 1, figsize=(15, 4 * len(examples)))\n",
    "    if len(examples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (tokens, activations) in enumerate(activations_by_example):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot activations\n",
    "        ax.bar(range(len(activations)), activations)\n",
    "        \n",
    "        # Add token labels\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "        \n",
    "        # Highlight tokens with high activation\n",
    "        threshold = np.mean(activations) + np.std(activations)\n",
    "        for j, act in enumerate(activations):\n",
    "            if act > threshold:\n",
    "                ax.get_xticklabels()[j].set_color('red')\n",
    "                ax.get_xticklabels()[j].set_weight('bold')\n",
    "        \n",
    "        ax.set_title(f\"Example {i+1}: Neuron {neuron} in Layer {layer}\")\n",
    "        ax.set_ylabel(\"Activation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def ablate_neurons_and_test(model, top_neurons, test_problems, device):\n",
    "    \"\"\"\n",
    "    Ablate (zero out) the identified neurons and test the effect on backtracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons to ablate\n",
    "        test_problems: List of test problems\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ablation results\n",
    "    \"\"\"\n",
    "    # Define a hook function to ablate specific neurons\n",
    "    def ablation_hook(activations, hook, neurons_to_ablate):\n",
    "        # neurons_to_ablate is a list of (layer, neuron) tuples\n",
    "        for layer, neuron in neurons_to_ablate:\n",
    "            if hook.name == f\"blocks.{layer}.hook_post\":\n",
    "                activations[0, :, neuron] = 0.0\n",
    "        return activations\n",
    "    \n",
    "    # Prepare neurons to ablate\n",
    "    neurons_to_ablate = [(n['layer'], n['neuron']) for n in top_neurons[:20]]  # Ablate top 20\n",
    "    \n",
    "    ablation_results = {\n",
    "        'original': [],\n",
    "        'ablated': []\n",
    "    }\n",
    "    \n",
    "    for problem in tqdm(test_problems, desc=\"Testing ablation\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate solution without ablation\n",
    "        original_prompt = f\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem_text} Solution: \\n<think>\\n\"\n",
    "        original_solution = model.generate(original_prompt, \n",
    "                                         temperature=0.4,\n",
    "                                         max_new_tokens=500,\n",
    "                                         top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in original\n",
    "        original_backtracking = identify_backtracking_enhanced(original_solution)\n",
    "        \n",
    "        # Generate solution with ablation\n",
    "        ablated_solution = \"\"\n",
    "        \n",
    "        # Set up hooks for ablation\n",
    "        hooks = []\n",
    "        for layer in set(layer for layer, _ in neurons_to_ablate):\n",
    "            hook_name = f\"blocks.{layer}.hook_post\"\n",
    "            hook_fn = lambda act, hook=None, neurons=neurons_to_ablate: ablation_hook(act, hook, neurons)\n",
    "            hooks.append((hook_name, hook_fn))\n",
    "        \n",
    "        # Generate with hooks\n",
    "        with model.hooks(hooks):\n",
    "            ablated_solution = model.generate(original_prompt, \n",
    "                                            temperature=0.4,\n",
    "                                            max_new_tokens=500,\n",
    "                                            top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in ablated\n",
    "        ablated_backtracking = identify_backtracking_enhanced(ablated_solution)\n",
    "        \n",
    "        ablation_results['original'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': original_solution,\n",
    "            'backtracking_count': len(original_backtracking),\n",
    "            'backtracking_instances': original_backtracking\n",
    "        })\n",
    "        \n",
    "        ablation_results['ablated'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': ablated_solution,\n",
    "            'backtracking_count': len(ablated_backtracking),\n",
    "            'backtracking_instances': ablated_backtracking\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    original_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['original'])\n",
    "    ablated_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['ablated'])\n",
    "    \n",
    "    ablation_results['summary'] = {\n",
    "        'original_backtracking_total': original_backtracking_count,\n",
    "        'ablated_backtracking_total': ablated_backtracking_count,\n",
    "        'percent_change': ((ablated_backtracking_count - original_backtracking_count) / \n",
    "                          max(1, original_backtracking_count)) * 100\n",
    "    }\n",
    "    \n",
    "    return ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the identified neurons on new examples\n",
    "validation_results = validate_backtracking_neurons(\n",
    "    model=model,\n",
    "    top_neurons=neuron_analysis['top_neurons'],\n",
    "    device=device,\n",
    "    num_examples=10\n",
    ")\n",
    "\n",
    "# Print validation results\n",
    "print(\"\\nValidation results:\")\n",
    "for result in validation_results:\n",
    "    print(f\"Problem: {result['problem'][:100]}...\")\n",
    "    print(f\"Backtracking phrase: {result['backtracking_phrase']}\")\n",
    "    print(\"Top neuron activations:\")\n",
    "    for act in result['neuron_activations'][:3]:\n",
    "        print(f\"  Layer {act['layer']}, Neuron {act['neuron']}: {act['activation']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
