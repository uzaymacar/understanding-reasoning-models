{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Got device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)\n",
    "model.cfg.n_ctx = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Model context length: 2048\n",
      "üß† Model layers: 28\n",
      "üî§ Vocabulary size: 151936\n",
      "üìä Hidden dimension: 1536\n",
      "üß© Attention heads: 12\n",
      "üè∑Ô∏è Model name: DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìè Model context length: {model.cfg.n_ctx}\")\n",
    "print(f\"üß† Model layers: {model.cfg.n_layers}\")\n",
    "print(f\"üî§ Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"üìä Hidden dimension: {model.cfg.d_model}\")\n",
    "print(f\"üß© Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"üè∑Ô∏è Model name: {model.cfg.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 958 CoT solutions...\n",
      "\n",
      "==================================================\n",
      "CHAIN-OF-THOUGHT ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Total problems analyzed: 958\n",
      "\n",
      "1. CORRECTNESS\n",
      "Correct answers: 393 (41.02%)\n",
      "\n",
      "2. THINK TAGS\n",
      "Solutions with </think> close tags: 605 (63.15%)\n",
      "\n",
      "3. TOKEN LIMITS\n",
      "Problems that ran out of tokens: 429 (44.78%)\n",
      "\n",
      "4. BACKTRACKING\n",
      "Solutions with backtracking: 248 (25.89%)\n",
      "Sample of problems with backtracking:\n",
      "     Level: Level 5, Type: Precalculus, Phrases: let me think again\n",
      "     Level: Level 4, Type: Prealgebra, Phrases: let me think again\n",
      "     Level: Level 2, Type: Precalculus, Phrases: i made a mistake\n",
      "     Level: Level 2, Type: Algebra, Phrases: let me recalculate, let me recalculate\n",
      "     Level: Level 3, Type: Prealgebra, Phrases: i made a mistake, wait, that doesn't\n",
      "     Level: Level 4, Type: Algebra, Phrases: let me think again\n",
      "     Level: Level 5, Type: Number Theory, Phrases: let me think again\n",
      "     Level: Level 2, Type: Algebra, Phrases: wait, that doesn't\n",
      "\n",
      "5. PERFORMANCE BY LEVEL\n",
      "  Level 1: 78.21% correct (78 problems)\n",
      "  Level 2: 57.92% correct (183 problems)\n",
      "  Level 3: 46.31% correct (203 problems)\n",
      "  Level 4: 35.27% correct (224 problems)\n",
      "  Level 5: 19.63% correct (270 problems)\n",
      "\n",
      "6. PERFORMANCE BY TYPE\n",
      "  Algebra: 59.19% correct (223 problems)\n",
      "  Counting & Probability: 40.00% correct (110 problems)\n",
      "  Geometry: 24.42% correct (86 problems)\n",
      "  Intermediate Algebra: 20.37% correct (162 problems)\n",
      "  Number Theory: 40.34% correct (119 problems)\n",
      "  Prealgebra: 56.21% correct (153 problems)\n",
      "  Precalculus: 27.62% correct (105 problems)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# analysis = run_analysis(\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\")\n",
    "# analysis = run_analysis(\"math_cot_results_t=0.8_mnt=3600_tp=0.92.json\")\n",
    "analysis = run_analysis(\"math_cot_results_t=0.7_mnt=1800_tp=0.92.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_backtracking_neurons(model, json_file_path, min_sample_size=1000, top_k=50):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events by processing entire CoT solutions\n",
    "    and tracking activations at specific backtracking points.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        device: The device to run inference on\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"    \n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    non_backtracking_activations = []  # Will store (layer, position, activations)\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(min_sample_size, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    sampled_results = [\n",
    "        # Backtracking examples\n",
    "        {\n",
    "            \"generated_cot\": \"2 + 4 = 5. Wait, that's not right. 2 + 4 = 6\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"3 + 11 = 8. Wait, that's not right. 3 + 11 = 14\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"7 * 6 = 36. Wait, that's not right. 7 * 6 = 42\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"15 - 8 = 5. Wait, that's not right. 15 - 8 = 7\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"24 √∑ 4 = 8. Wait, that's not right. 24 √∑ 4 = 6\"\n",
    "        },\n",
    "        # Correctly solved examples\n",
    "        {\n",
    "            \"generated_cot\": \"9 + 7 = 16. This is correct because 9 + 7 = 16.\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"12 * 3 = 36. I can verify this is correct.\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"20 - 5 = 15. This is the right answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"18 √∑ 3 = 6. This division is correct.\"\n",
    "        },\n",
    "        {\n",
    "            \"generated_cot\": \"5 * 5 = 25. This is the correct square of 5.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for result in tqdm(sampled_results):\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        if not generated_cot: continue\n",
    "        \n",
    "        tokens = model.to_tokens(generated_cot)\n",
    "        str_tokens = model.to_str_tokens(generated_cot)\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Find positions of backtracking phrases in the token sequence\n",
    "        backtracking_positions = []\n",
    "        for phrase in backtracking_phrases:\n",
    "            phrase_tokens = model.to_str_tokens(phrase)\n",
    "            \n",
    "            # Look for this phrase in the token sequence\n",
    "            for i in range(len(str_tokens[0]) - len(phrase_tokens) + 1):\n",
    "                # Check if this position contains the phrase\n",
    "                match = True\n",
    "                for j, token in enumerate(phrase_tokens):\n",
    "                    if i+j >= len(str_tokens[0]) or str_tokens[0][i+j].lower() != token.lower():\n",
    "                        match = False\n",
    "                        break\n",
    "                \n",
    "                if match:\n",
    "                    # Found a match, add the position range\n",
    "                    backtracking_positions.append((i, i + len(phrase_tokens)))\n",
    "        \n",
    "        # If no backtracking phrases found, sample random positions as non-backtracking\n",
    "        if not backtracking_positions:\n",
    "            # Sample random positions (avoiding the beginning and end)\n",
    "            if len(tokens[0]) > 20:\n",
    "                num_samples = min(5, len(tokens[0]) - 10)\n",
    "                for _ in range(num_samples):\n",
    "                    pos = random.randint(5, len(tokens[0]) - 5)\n",
    "                    # Extract activations for this position from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                        non_backtracking_activations.append((layer, layer_activations))\n",
    "        else:\n",
    "            # For each backtracking position, extract activations\n",
    "            for start_pos, end_pos in backtracking_positions:\n",
    "                # Get the position where backtracking starts\n",
    "                trigger_pos = start_pos\n",
    "                \n",
    "                # Extract activations at the trigger position from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer][0, trigger_pos].detach().cpu().numpy()\n",
    "                    backtracking_activations.append((layer, layer_activations))\n",
    "                \n",
    "                # Also sample non-backtracking positions from the same solution\n",
    "                # (avoiding positions close to backtracking phrases)\n",
    "                safe_positions = []\n",
    "                for pos in range(5, len(tokens[0]) - 5):\n",
    "                    # Check if this position is far from any backtracking phrase\n",
    "                    is_safe = True\n",
    "                    for bt_start, bt_end in backtracking_positions:\n",
    "                        if pos >= bt_start - 10 and pos <= bt_end + 10:\n",
    "                            is_safe = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_safe:\n",
    "                        safe_positions.append(pos)\n",
    "                \n",
    "                # Sample from safe positions\n",
    "                if safe_positions:\n",
    "                    num_samples = min(len(backtracking_positions), len(safe_positions))\n",
    "                    for pos in random.sample(safe_positions, num_samples):\n",
    "                        # Extract activations for this position from all layers\n",
    "                        for layer in range(model.cfg.n_layers):\n",
    "                            layer_activations = cache[\"post\", layer][0, pos].detach().cpu().numpy()\n",
    "                            non_backtracking_activations.append((layer, layer_activations))\n",
    "    \n",
    "    print(f\"Found {len(backtracking_activations)} backtracking activations and {len(non_backtracking_activations)} non-backtracking activations\")\n",
    "    return None\n",
    "\n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, analyze neuron activations\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Collect activations for this layer\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(layer_backtracking.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            bt_activations = layer_backtracking[:, neuron_idx]\n",
    "            non_bt_activations = layer_non_backtracking[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(bt_activations)\n",
    "            mean_non_backtracking = np.mean(non_bt_activations)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(bt_activations) + np.var(non_bt_activations)) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Create dataset for AUC calculation\n",
    "            X = np.concatenate([bt_activations, non_bt_activations])\n",
    "            y = np.concatenate([np.ones(len(bt_activations)), np.zeros(len(non_bt_activations))])\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try: auc = roc_auc_score(y, X)\n",
    "            except: auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for n in neurons[:top_k]:\n",
    "            all_neurons.append({'layer': layer, 'neuron': n['neuron'], 'effect_size': n['effect_size'], 'auc': n['auc']})\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {'top_neurons': all_neurons[:top_k], 'layer_scores': neuron_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 996 examples to identify backtracking neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 1/10 [01:40<15:00, 100.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m neuron_analysis \u001b[38;5;241m=\u001b[39m \u001b[43midentify_backtracking_neurons\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmath_cot_results_t=0.6_mnt=1500_tp=0.92.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print top neurons\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 neurons associated with backtracking:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 75\u001b[0m, in \u001b[0;36midentify_backtracking_neurons\u001b[0;34m(model, json_file_path, min_sample_size, top_k)\u001b[0m\n\u001b[1;32m     73\u001b[0m backtracking_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m backtracking_phrases:\n\u001b[0;32m---> 75\u001b[0m     phrase_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_str_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Look for this phrase in the token sequence\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(str_tokens[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(phrase_tokens) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Check if this position contains the phrase\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformer_lens/HookedTransformer.py:906\u001b[0m, in \u001b[0;36mHookedTransformer.to_str_tokens\u001b[0;34m(self, input, prepend_bos, padding_side)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    901\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m tokens: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_str_tokens(tokens, prepend_bos, padding_side),\n\u001b[1;32m    902\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    903\u001b[0m         )\n\u001b[1;32m    904\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 906\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    908\u001b[0m     ]\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# Gemma tokenizer expects a batch dimension\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;129;01mand\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformer_lens/HookedTransformer.py:813\u001b[0m, in \u001b[0;36mHookedTransformer.to_tokens\u001b[0;34m(self, input, prepend_bos, padding_side, move_to_device, truncate)\u001b[0m\n\u001b[1;32m    810\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_tokens_with_bos_removed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, tokens)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[0;32m--> 813\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neuron_analysis = identify_backtracking_neurons(\n",
    "    model=model,\n",
    "    json_file_path=\"math_cot_results_t=0.6_mnt=1500_tp=0.92.json\",\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "# Print top neurons\n",
    "print(\"Top 10 neurons associated with backtracking:\")\n",
    "for i, neuron in enumerate(neuron_analysis['top_neurons'][:10]):\n",
    "    print(f\"{i+1}. Layer {neuron['layer']}, Neuron {neuron['neuron']}: Effect size = {neuron['effect_size']:.4f}, AUC = {neuron['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_backtracking_neurons(model, json_file_path, device, top_k=50):\n",
    "    \"\"\"\n",
    "    Identify neurons that activate during backtracking events.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        json_file_path: Path to the JSON file with CoT results\n",
    "        device: The device to run inference on\n",
    "        top_k: Number of top neurons to identify\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with neuron analysis results\n",
    "    \"\"\"\n",
    "    # Load the results\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Initialize storage for activations\n",
    "    backtracking_activations = []\n",
    "    non_backtracking_activations = []\n",
    "    \n",
    "    # Process a subset of examples for efficiency\n",
    "    sample_size = min(100, len(results))\n",
    "    sampled_results = random.sample(results, sample_size)\n",
    "    \n",
    "    print(f\"Processing {sample_size} examples to identify backtracking neurons...\")\n",
    "    \n",
    "    for result in tqdm(sampled_results):\n",
    "        problem_text = result.get(\"problem_text\", \"\")\n",
    "        generated_cot = result.get(\"generated_cot\", \"\")\n",
    "        \n",
    "        # Skip if the generated CoT is empty\n",
    "        if not generated_cot:\n",
    "            continue\n",
    "        \n",
    "        # Identify backtracking phrases with context\n",
    "        backtracking_instances = identify_backtracking(generated_cot)\n",
    "        \n",
    "        # If no backtracking, use this as a control example\n",
    "        if not backtracking_instances or len(backtracking_instances) == 0:\n",
    "            # Get a random segment from the CoT\n",
    "            tokens = model.to_tokens(generated_cot)\n",
    "            if len(tokens[0]) > 20:  # Ensure we have enough tokens\n",
    "                # Take a random segment with context window similar to backtracking examples\n",
    "                context_window = 50  # characters before and after, matching the backtracking case\n",
    "                \n",
    "                # Convert to text indices for consistency with backtracking case\n",
    "                text_length = len(generated_cot)\n",
    "                if text_length > context_window * 2:\n",
    "                    # Pick a random center point\n",
    "                    center_idx = random.randint(context_window, text_length - context_window)\n",
    "                    context_start = center_idx - context_window\n",
    "                    context_end = center_idx + context_window\n",
    "                    context = generated_cot[context_start:context_end]\n",
    "                else:\n",
    "                    # If text is too short, use the whole text\n",
    "                    context = generated_cot\n",
    "                \n",
    "                # Get activations for this context\n",
    "                tokens = model.to_tokens(context)\n",
    "                _, cache = model.run_with_cache(tokens)\n",
    "                \n",
    "                # Extract activations from all layers\n",
    "                for layer in range(model.cfg.n_layers):\n",
    "                    layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "                    # Flatten across sequence positions\n",
    "                    flat_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
    "                    non_backtracking_activations.append((layer, flat_activations))\n",
    "        else:\n",
    "            # For each backtracking instance, get the surrounding context\n",
    "            for phrase in backtracking_instances:\n",
    "                # Locate the phrase in the generated CoT (case insensitive)\n",
    "                phrase_lower = phrase.lower()\n",
    "                generated_cot_lower = generated_cot.lower()\n",
    "                start_idx = generated_cot_lower.find(phrase_lower)\n",
    "                \n",
    "                if start_idx != -1:\n",
    "                    end_idx = start_idx + len(phrase)\n",
    "                    \n",
    "                    # Extract context around the backtracking phrase\n",
    "                    context_window = 50  # characters before and after\n",
    "                    context_start = max(0, start_idx - context_window)\n",
    "                    context_end = min(len(generated_cot), end_idx + context_window)\n",
    "                    context = generated_cot[context_start:context_end]\n",
    "                    \n",
    "                    # Convert context to tokens\n",
    "                    tokens = model.to_tokens(context)\n",
    "                    \n",
    "                    # Get activations for this context\n",
    "                    _, cache = model.run_with_cache(tokens)\n",
    "                    \n",
    "                    # Extract activations from all layers\n",
    "                    for layer in range(model.cfg.n_layers):\n",
    "                        layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "                        # Flatten across sequence positions\n",
    "                        flat_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
    "                        backtracking_activations.append((layer, flat_activations))\n",
    "    \n",
    "    # Analyze activations to find neurons that correlate with backtracking\n",
    "    neuron_scores = {}\n",
    "    \n",
    "    # For each layer, train a classifier to distinguish backtracking from non-backtracking\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Collect activations for this layer\n",
    "        layer_backtracking = np.vstack([act for l, act in backtracking_activations if l == layer])\n",
    "        layer_non_backtracking = np.vstack([act for l, act in non_backtracking_activations if l == layer])\n",
    "        \n",
    "        if len(layer_backtracking) == 0 or len(layer_non_backtracking) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create dataset\n",
    "        X = np.vstack([layer_backtracking, layer_non_backtracking])\n",
    "        y = np.concatenate([\n",
    "            np.ones(len(layer_backtracking)),\n",
    "            np.zeros(len(layer_non_backtracking))\n",
    "        ])\n",
    "        \n",
    "        # For each neuron, calculate its activation difference\n",
    "        neuron_scores[layer] = []\n",
    "        \n",
    "        for neuron_idx in range(X.shape[1]):\n",
    "            # Extract this neuron's activations\n",
    "            neuron_activations = X[:, neuron_idx]\n",
    "            \n",
    "            # Calculate mean activation for backtracking vs non-backtracking\n",
    "            mean_backtracking = np.mean(neuron_activations[:len(layer_backtracking)])\n",
    "            mean_non_backtracking = np.mean(neuron_activations[len(layer_backtracking):])\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(neuron_activations[:len(layer_backtracking)]) + \n",
    "                                 np.var(neuron_activations[len(layer_backtracking):])) / 2)\n",
    "            effect_size = (mean_backtracking - mean_non_backtracking) / (pooled_std + 1e-10)\n",
    "            \n",
    "            # Calculate AUC for this neuron\n",
    "            try:\n",
    "                auc = roc_auc_score(y, neuron_activations)\n",
    "            except:\n",
    "                auc = 0.5  # Default if calculation fails\n",
    "            \n",
    "            neuron_scores[layer].append({\n",
    "                'neuron': neuron_idx,\n",
    "                'mean_diff': mean_backtracking - mean_non_backtracking,\n",
    "                'effect_size': effect_size,\n",
    "                'auc': auc\n",
    "            })\n",
    "        \n",
    "        # Sort neurons by effect size\n",
    "        neuron_scores[layer] = sorted(neuron_scores[layer], \n",
    "                                     key=lambda x: abs(x['effect_size']), \n",
    "                                     reverse=True)\n",
    "    \n",
    "    # Identify top neurons across all layers\n",
    "    all_neurons = []\n",
    "    for layer, neurons in neuron_scores.items():\n",
    "        for neuron in neurons[:top_k]:\n",
    "            all_neurons.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron['neuron'],\n",
    "                'effect_size': neuron['effect_size'],\n",
    "                'auc': neuron['auc']\n",
    "            })\n",
    "    \n",
    "    # Sort by absolute effect size\n",
    "    all_neurons = sorted(all_neurons, key=lambda x: abs(x['effect_size']), reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'top_neurons': all_neurons[:top_k],\n",
    "        'layer_scores': neuron_scores\n",
    "    }\n",
    "\n",
    "def validate_backtracking_neurons(model, top_neurons, device, num_examples=10):\n",
    "    \"\"\"\n",
    "    Validate the identified backtracking neurons by testing them on new examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons identified\n",
    "        device: The device to run inference on\n",
    "        num_examples: Number of examples to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Function to sample problems from the dataset\n",
    "    def sample_math_problems(dataset, n=5, level=None, problem_type=None):\n",
    "        \"\"\"\n",
    "        Sample n problems from the dataset, optionally filtering by level or type.\n",
    "        \n",
    "        Args:\n",
    "            dataset: The MATH dataset\n",
    "            n: Number of problems to sample\n",
    "            level: Optional filter for problem difficulty (e.g., \"Level 1\")\n",
    "            problem_type: Optional filter for problem type (e.g., \"Algebra\")\n",
    "        \n",
    "        Returns:\n",
    "            List of sampled problems\n",
    "        \"\"\"\n",
    "        filtered_dataset = dataset['train']\n",
    "        \n",
    "        if level:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['level'] == level]\n",
    "        \n",
    "        if problem_type:\n",
    "            filtered_dataset = [ex for ex in filtered_dataset if ex['type'] == problem_type]\n",
    "        \n",
    "        filtered_dataset = list(filtered_dataset)  # Convert to list to ensure it's a sequence\n",
    "        return random.sample(filtered_dataset, min(n, len(filtered_dataset)))\n",
    "\n",
    "    # Function to generate CoT using the model\n",
    "    def generate_cot_for_problem(\n",
    "        model: HookedTransformer, \n",
    "        problem: str, \n",
    "        temperature: float = 0.4, \n",
    "        max_new_tokens: int = 1500, \n",
    "        top_p: float = 0.92\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a chain-of-thought solution for a given math problem.\n",
    "        \n",
    "        Args:\n",
    "            model: The HookedTransformer model\n",
    "            problem: The math problem text\n",
    "            temperature: The temperature for the model\n",
    "            max_new_tokens: The maximum number of tokens to generate\n",
    "            top_p: The top-p value for the model\n",
    "        Returns:\n",
    "            The generated chain-of-thought solution\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem} Solution: \\n<think>\\n\"\"\"\n",
    "        result = model.generate(prompt, \n",
    "                                temperature=temperature,\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                top_p=top_p)\n",
    "        return result\n",
    "\n",
    "    # Load the MATH dataset for validation\n",
    "    math_dataset = load_dataset(\"fdyrd/math\")\n",
    "    validation_problems = sample_math_problems(math_dataset, n=num_examples)\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    for problem in tqdm(validation_problems, desc=\"Validating neurons\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate a solution with backtracking\n",
    "        solution = generate_cot_for_problem(model, problem_text)\n",
    "        \n",
    "        # Identify backtracking instances\n",
    "        backtracking_instances = identify_backtracking(solution)\n",
    "        \n",
    "        if not backtracking_instances or len(backtracking_instances) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For the first backtracking instance, analyze neuron activations\n",
    "        phrase = backtracking_instances[0]\n",
    "        phrase_lower = phrase.lower()\n",
    "        solution_lower = solution.lower()\n",
    "        start_idx = solution_lower.find(phrase_lower)\n",
    "                \n",
    "        if start_idx == -1:\n",
    "            continue\n",
    "        \n",
    "        end_idx = start_idx + len(phrase)\n",
    "        \n",
    "        # Extract context around the backtracking phrase\n",
    "        context_window = 50  # characters before and after\n",
    "        context_start = max(0, start_idx - context_window)\n",
    "        context_end = min(len(solution), end_idx + context_window)\n",
    "        context = solution[context_start:context_end]\n",
    "        \n",
    "        # Convert context to tokens\n",
    "        tokens = model.to_tokens(context)\n",
    "        \n",
    "        # Run with cache to get activations\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Check activation of top neurons\n",
    "        neuron_activations = []\n",
    "        \n",
    "        for neuron_info in top_neurons[:10]:  # Check top 10 neurons\n",
    "            layer = neuron_info['layer']\n",
    "            neuron = neuron_info['neuron']\n",
    "            \n",
    "            # Get activations for this layer\n",
    "            layer_activations = cache[\"post\", layer].detach().cpu().numpy()\n",
    "            \n",
    "            # Get mean activation for this neuron across sequence positions\n",
    "            mean_activation = np.mean(layer_activations[0, :, neuron])\n",
    "            \n",
    "            neuron_activations.append({\n",
    "                'layer': layer,\n",
    "                'neuron': neuron,\n",
    "                'activation': float(mean_activation),\n",
    "                'context': context,\n",
    "                'backtracking_phrase': phrase\n",
    "            })\n",
    "        \n",
    "        validation_results.append({\n",
    "            'problem': problem_text,\n",
    "            'solution': solution,\n",
    "            'backtracking_phrase': phrase,\n",
    "            'neuron_activations': neuron_activations\n",
    "        })\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def visualize_neuron_activations(model, neuron_info, examples, device):\n",
    "    \"\"\"\n",
    "    Visualize the activations of a specific neuron across different examples.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        neuron_info: Dictionary with neuron information (layer, index)\n",
    "        examples: List of text examples to analyze\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib figure with visualization\n",
    "    \"\"\"\n",
    "    layer = neuron_info['layer']\n",
    "    neuron = neuron_info['neuron']\n",
    "    \n",
    "    activations_by_example = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Get tokens\n",
    "        tokens = model.to_tokens(example)\n",
    "        str_tokens = model.to_str_tokens(example)\n",
    "        \n",
    "        # Run with cache\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Get activations for this layer and neuron\n",
    "        layer_activations = cache[\"post\", layer][0, :, neuron].detach().cpu().numpy()\n",
    "        \n",
    "        activations_by_example.append((str_tokens, layer_activations))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(len(examples), 1, figsize=(15, 4 * len(examples)))\n",
    "    if len(examples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (tokens, activations) in enumerate(activations_by_example):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot activations\n",
    "        ax.bar(range(len(activations)), activations)\n",
    "        \n",
    "        # Add token labels\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "        \n",
    "        # Highlight tokens with high activation\n",
    "        threshold = np.mean(activations) + np.std(activations)\n",
    "        for j, act in enumerate(activations):\n",
    "            if act > threshold:\n",
    "                ax.get_xticklabels()[j].set_color('red')\n",
    "                ax.get_xticklabels()[j].set_weight('bold')\n",
    "        \n",
    "        ax.set_title(f\"Example {i+1}: Neuron {neuron} in Layer {layer}\")\n",
    "        ax.set_ylabel(\"Activation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def ablate_neurons_and_test(model, top_neurons, test_problems, device):\n",
    "    \"\"\"\n",
    "    Ablate (zero out) the identified neurons and test the effect on backtracking.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        top_neurons: List of top neurons to ablate\n",
    "        test_problems: List of test problems\n",
    "        device: The device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ablation results\n",
    "    \"\"\"\n",
    "    # Define a hook function to ablate specific neurons\n",
    "    def ablation_hook(activations, hook, neurons_to_ablate):\n",
    "        # neurons_to_ablate is a list of (layer, neuron) tuples\n",
    "        for layer, neuron in neurons_to_ablate:\n",
    "            if hook.name == f\"blocks.{layer}.hook_post\":\n",
    "                activations[0, :, neuron] = 0.0\n",
    "        return activations\n",
    "    \n",
    "    # Prepare neurons to ablate\n",
    "    neurons_to_ablate = [(n['layer'], n['neuron']) for n in top_neurons[:20]]  # Ablate top 20\n",
    "    \n",
    "    ablation_results = {\n",
    "        'original': [],\n",
    "        'ablated': []\n",
    "    }\n",
    "    \n",
    "    for problem in tqdm(test_problems, desc=\"Testing ablation\"):\n",
    "        problem_text = problem['problem']\n",
    "        \n",
    "        # Generate solution without ablation\n",
    "        original_prompt = f\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem_text} Solution: \\n<think>\\n\"\n",
    "        original_solution = model.generate(original_prompt, \n",
    "                                         temperature=0.4,\n",
    "                                         max_new_tokens=500,\n",
    "                                         top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in original\n",
    "        original_backtracking = identify_backtracking_enhanced(original_solution)\n",
    "        \n",
    "        # Generate solution with ablation\n",
    "        ablated_solution = \"\"\n",
    "        \n",
    "        # Set up hooks for ablation\n",
    "        hooks = []\n",
    "        for layer in set(layer for layer, _ in neurons_to_ablate):\n",
    "            hook_name = f\"blocks.{layer}.hook_post\"\n",
    "            hook_fn = lambda act, hook=None, neurons=neurons_to_ablate: ablation_hook(act, hook, neurons)\n",
    "            hooks.append((hook_name, hook_fn))\n",
    "        \n",
    "        # Generate with hooks\n",
    "        with model.hooks(hooks):\n",
    "            ablated_solution = model.generate(original_prompt, \n",
    "                                            temperature=0.4,\n",
    "                                            max_new_tokens=500,\n",
    "                                            top_p=0.92)\n",
    "        \n",
    "        # Count backtracking instances in ablated\n",
    "        ablated_backtracking = identify_backtracking_enhanced(ablated_solution)\n",
    "        \n",
    "        ablation_results['original'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': original_solution,\n",
    "            'backtracking_count': len(original_backtracking),\n",
    "            'backtracking_instances': original_backtracking\n",
    "        })\n",
    "        \n",
    "        ablation_results['ablated'].append({\n",
    "            'problem': problem_text,\n",
    "            'solution': ablated_solution,\n",
    "            'backtracking_count': len(ablated_backtracking),\n",
    "            'backtracking_instances': ablated_backtracking\n",
    "        })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    original_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['original'])\n",
    "    ablated_backtracking_count = sum(r['backtracking_count'] for r in ablation_results['ablated'])\n",
    "    \n",
    "    ablation_results['summary'] = {\n",
    "        'original_backtracking_total': original_backtracking_count,\n",
    "        'ablated_backtracking_total': ablated_backtracking_count,\n",
    "        'percent_change': ((ablated_backtracking_count - original_backtracking_count) / \n",
    "                          max(1, original_backtracking_count)) * 100\n",
    "    }\n",
    "    \n",
    "    return ablation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the identified neurons on new examples\n",
    "validation_results = validate_backtracking_neurons(\n",
    "    model=model,\n",
    "    top_neurons=neuron_analysis['top_neurons'],\n",
    "    device=device,\n",
    "    num_examples=10\n",
    ")\n",
    "\n",
    "# Print validation results\n",
    "print(\"\\nValidation results:\")\n",
    "for result in validation_results:\n",
    "    print(f\"Problem: {result['problem'][:100]}...\")\n",
    "    print(f\"Backtracking phrase: {result['backtracking_phrase']}\")\n",
    "    print(\"Top neuron activations:\")\n",
    "    for act in result['neuron_activations'][:3]:\n",
    "        print(f\"  Layer {act['layer']}, Neuron {act['neuron']}: {act['activation']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
