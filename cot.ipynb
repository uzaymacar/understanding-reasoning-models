{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic CoT Generation\n",
    "\n",
    "This notebook is used to generate synthetic CoT solutions for the MATH dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \n",
    "    \"cuda\" if torch.cuda.is_available() else \n",
    "    \"cpu\"\n",
    ")\n",
    "print('Got device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable gradient computation for all tensors to speed up inference and save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f637c91eb90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook to work, please add \n",
    "\n",
    "```python\n",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "```\n",
    "\n",
    "to the `OFFICIAL_MODEL_NAMES` list in the `loading_from_pretrained.py` file under the `TransformerLens` library after you've downloaded it locally.\n",
    "\n",
    "It seems one can also increase the `n_ctx` for architectures `\"QWenLMHeadModel\"` and `\"QWen2ForCausalLM\"` from 2048 to 4096 because the documentation mentions that they are capped due to memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = model.to(device)\n",
    "model.cfg.n_ctx = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 Model context length: 2048\n",
      "🧠 Model layers: 28\n",
      "🔤 Vocabulary size: 151936\n",
      "📊 Hidden dimension: 1536\n",
      "🧩 Attention heads: 12\n",
      "🏷️ Model name: DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    }
   ],
   "source": [
    "print(f\"📏 Model context length: {model.cfg.n_ctx}\")\n",
    "print(f\"🧠 Model layers: {model.cfg.n_layers}\")\n",
    "print(f\"🔤 Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"📊 Hidden dimension: {model.cfg.d_model}\")\n",
    "print(f\"🧩 Attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"🏷️ Model name: {model.cfg.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train', 'test']\n",
      "Available fields: ['problem', 'level', 'type', 'solution']\n",
      "Number of examples in train: 7500\n",
      "Number of examples in test: 5000\n",
      "\n",
      "Example from the dataset:\n",
      "Problem: Let \\[f(x) = \\left\\{\n",
      "\\begin{array}{cl} ax+3, &\\text{ if }x>2, \\\\\n",
      "x-5 &\\text{ if } -2 \\le x \\le 2, \\\\\n",
      "2x-b &\\text{ if } x <-2.\n",
      "\\end{array}\n",
      "\\right.\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).\n",
      "Level: Level 5\n",
      "Type: Algebra\n",
      "Solution: For the piecewise function to be continuous, the cases must \"meet\" at $2$ and $-2$. For example, $ax+3$ and $x-5$ must be equal when $x=2$. This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\Rightarrow a=-3$. Similarly, $x-5$ and $2x-b$ must be equal when $x=-2$. Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$. So $a+b=-3+3=\\boxed{0}$.\n"
     ]
    }
   ],
   "source": [
    "# Load the MATH dataset\n",
    "math_dataset = load_dataset(\"fdyrd/math\")\n",
    "\n",
    "# Examine the dataset structure\n",
    "print(f\"Available splits: {list(math_dataset.keys())}\")\n",
    "print(f\"Available fields: {list(math_dataset['train'].features.keys())}\")\n",
    "print(f\"Number of examples in train: {len(math_dataset['train'])}\")\n",
    "print(f\"Number of examples in test: {len(math_dataset['test'])}\")\n",
    "\n",
    "# Look at the first example to understand the format\n",
    "print(\"\\nExample from the dataset:\")\n",
    "example = math_dataset['train'][0]\n",
    "print(f\"Problem: {example['problem']}\")\n",
    "print(f\"Level: {example['level']}\")\n",
    "print(f\"Type: {example['type']}\")\n",
    "print(f\"Solution: {example['solution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample problems from the dataset\n",
    "def sample_math_problems(dataset, n=5, level=None, problem_type=None):\n",
    "    \"\"\"\n",
    "    Sample n problems from the dataset, optionally filtering by level or type.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The MATH dataset\n",
    "        n: Number of problems to sample\n",
    "        level: Optional filter for problem difficulty (e.g., \"Level 1\")\n",
    "        problem_type: Optional filter for problem type (e.g., \"Algebra\")\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled problems\n",
    "    \"\"\"\n",
    "    filtered_dataset = dataset['train']\n",
    "    \n",
    "    if level:\n",
    "        filtered_dataset = [ex for ex in filtered_dataset if ex['level'] == level]\n",
    "    \n",
    "    if problem_type:\n",
    "        filtered_dataset = [ex for ex in filtered_dataset if ex['type'] == problem_type]\n",
    "    \n",
    "    filtered_dataset = list(filtered_dataset)  # Convert to list to ensure it's a sequence\n",
    "    return random.sample(filtered_dataset, min(n, len(filtered_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled problems for testing:\n",
      "\n",
      "Problem 1:\n",
      "Type: Number Theory, Level: Level 3\n",
      "Problem statement: How many different positive values of $x$ will make this statement true: there are exactly $2$ positive two-digit multiples of $x$.\n",
      "\n",
      "Problem 2:\n",
      "Type: Algebra, Level: Level 3\n",
      "Problem statement: Find the value of $t$ that satisfies $\\frac{1}{t+2} + \\frac{2t}{t+2} - \\frac{3}{t+2} = 3$.\n",
      "\n",
      "Problem 3:\n",
      "Type: Prealgebra, Level: Level 3\n",
      "Problem statement: The scores on a $110$-point test were organized in the stem-and-leaf plot shown. $9 | 6$ represents $96$ points. What is the mode of the scores? \\begin{tabular}{c|lllllll}\n",
      "\\multicolumn{8}{c}{\\underline{Points on the Test}}\\\\\n",
      "5 &0 & 0 & & & & &\\\\\n",
      "6 &3 & & & & & &\\\\\n",
      "7 &7 & 8 & & & & &\\\\\n",
      "8 &2 & 6 & 7 & 9 & 9 & 9 & 9\\\\\n",
      "9 &1 & 4 & 4 & 4 & 6 & &\\\\\n",
      "10 &0 & 0 & 0 & & & &\\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "sampled_problems = sample_math_problems(math_dataset, n=3, level=\"Level 3\")\n",
    "print(\"\\nSampled problems for testing:\")\n",
    "for i, problem in enumerate(sampled_problems):\n",
    "    print(f\"\\nProblem {i+1}:\")\n",
    "    print(f\"Type: {problem['type']}, Level: {problem['level']}\")\n",
    "    print(f\"Problem statement: {problem['problem']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate CoT using the model\n",
    "def generate_cot_for_problem(\n",
    "    model: HookedTransformer, \n",
    "    problem: str, \n",
    "    temperature: float = 0.4, \n",
    "    max_new_tokens: int = 1500, \n",
    "    top_p: float = 0.92\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a chain-of-thought solution for a given math problem.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        problem: The math problem text\n",
    "        temperature: The temperature for the model\n",
    "        max_new_tokens: The maximum number of tokens to generate\n",
    "        top_p: The top-p value for the model\n",
    "    Returns:\n",
    "        The generated chain-of-thought solution\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem} Solution: \\n<think>\\n\"\"\"\n",
    "    result = model.generate(prompt, \n",
    "                            temperature=temperature,\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            top_p=top_p)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a problem\n",
    "problem_text = sampled_problems[0]['problem']\n",
    "\n",
    "# Generate CoT\n",
    "cot_solution = generate_cot_for_problem(\n",
    "    model, \n",
    "    problem_text, \n",
    "    temperature=0.6, \n",
    "    max_new_tokens=1500, \n",
    "    top_p=0.92\n",
    ")\n",
    "print(\"\\nGenerated Chain-of-Thought solution:\")\n",
    "print(cot_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to batch process multiple problems\n",
    "def batch_generate_cot(\n",
    "    model, \n",
    "    problems, \n",
    "    batch_size=4,  # Process this many problems in parallel\n",
    "    temperature=0.6, \n",
    "    max_new_tokens=1500, \n",
    "    top_p=0.92, \n",
    "    save_every=5,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate CoT solutions for multiple problems in parallel batches.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        problems: List of problem dictionaries\n",
    "        batch_size: Number of problems to process in parallel\n",
    "        temperature: The temperature for the model\n",
    "        max_new_tokens: The maximum number of tokens to generate\n",
    "        top_p: The top-p value for the model\n",
    "        save_every: How often to save intermediate results\n",
    "        save_path: Optional path to save results\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing problems and their CoT solutions\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Check if save_path exists and load existing results\n",
    "    if save_path and os.path.exists(save_path):\n",
    "        import json\n",
    "        print(f\"Loading existing results from {save_path}...\")\n",
    "        with open(save_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        # Get existing problem IDs\n",
    "        existing_ids = set(result[\"problem_id\"] for result in results)\n",
    "        print(f\"Found {len(existing_ids)} existing problems. Continuing from where we left off.\")\n",
    "    else:\n",
    "        existing_ids = set()\n",
    "    \n",
    "    # Process problems in batches\n",
    "    for i in tqdm(range(0, len(problems), batch_size), desc=\"Processing batches\"):\n",
    "        batch_problems = problems[i:i+batch_size]\n",
    "        \n",
    "        # Filter out problems that have already been processed\n",
    "        filtered_batch = []\n",
    "        filtered_indices = []\n",
    "        for j, problem in enumerate(batch_problems):\n",
    "            problem_id = i + j\n",
    "            if problem_id not in existing_ids:\n",
    "                filtered_batch.append(problem)\n",
    "                filtered_indices.append(j)\n",
    "        \n",
    "        # Skip if all problems in this batch have been processed\n",
    "        if not filtered_batch:\n",
    "            continue\n",
    "            \n",
    "        # Prepare prompts for the filtered batch\n",
    "        prompts = [\n",
    "            f\"\"\"Solve this math problem step by step. Put your final answer in \\\\boxed{{}}. Problem: {problem['problem']} Solution: \\n<think>\\n\"\"\"\n",
    "            for problem in filtered_batch\n",
    "        ]\n",
    "        \n",
    "        # Generate solutions for the batch in parallel\n",
    "        try:\n",
    "            batch_solutions = model.generate(\n",
    "                prompts,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating solutions for batch {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Process and store results\n",
    "        solutions_list = batch_solutions if len(filtered_batch) > 1 else [batch_solutions]\n",
    "        for idx, (j, problem, solution) in enumerate(zip(filtered_indices, filtered_batch, solutions_list)):\n",
    "            problem_id = i + batch_problems.index(problem)\n",
    "            results.append({\n",
    "                \"problem_id\": problem_id,\n",
    "                \"problem_text\": problem['problem'],\n",
    "                \"problem_type\": problem['type'],\n",
    "                \"problem_level\": problem['level'],\n",
    "                \"ground_truth_solution\": problem['solution'],\n",
    "                \"generated_cot\": solution\n",
    "            })\n",
    "        \n",
    "        # Save intermediate results\n",
    "        if (i // batch_size) % save_every == 0 and save_path:\n",
    "            print(f\"Saving results to {save_path}...\")\n",
    "            import json\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save final results\n",
    "    if save_path:\n",
    "        import json\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a small set of problems for testing\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "test_problems = sample_math_problems(math_dataset, n=1000)\n",
    "\n",
    "temperature = 0.8\n",
    "max_new_tokens = 3600\n",
    "top_p = 0.92\n",
    "batch_size = 1\n",
    "model.cfg.n_ctx = 4096\n",
    "\n",
    "# Generate CoT solutions for the test problems\n",
    "cot_results = batch_generate_cot(\n",
    "    model, \n",
    "    test_problems, \n",
    "    batch_size=batch_size,\n",
    "    temperature=temperature, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    top_p=top_p, \n",
    "    save_path=f\"math_cot_results_t={temperature}_mnt={max_new_tokens}_tp={top_p}.json\",\n",
    "    save_every=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
